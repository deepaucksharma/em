[
  {
    "id": "MP1",
    "name": "Velocity ↔ Quality",
    "slug": "velocity-quality",
    "metricA": {
      "id": "1.2",
      "alsoIds": [
        "1.1"
      ],
      "role": "Speed signal"
    },
    "metricB": {
      "id": "1.4",
      "alsoIds": [
        "3.3"
      ],
      "role": "Quality counterweight"
    },
    "whyPaired": "Speed without a quality check incentivizes shipping broken code fast. Quality without a speed check incentivizes paralysis.",
    "withoutA": "“We have 0% failure rate!” (because we deploy once a month after 3 weeks of manual QA).",
    "withoutB": "“We deploy 50x/day!” (but 20% of deploys break things).",
    "balancingStrategy": "Use error budgets as the arbitrator. Budget remaining → optimize for speed. Budget burning > 1x → investigate CFR root causes before pushing velocity."
  },
  {
    "id": "MP2",
    "name": "Reliability ↔ Velocity",
    "slug": "reliability-velocity",
    "metricA": {
      "id": "3.5",
      "alsoIds": [
        "3.3"
      ],
      "role": "Reliability signal"
    },
    "metricB": {
      "id": "1.2",
      "alsoIds": [
        "1.1"
      ],
      "role": "Delivery speed"
    },
    "whyPaired": "Reliability without velocity tracking leads to over-engineering. Velocity without reliability tracking leads to chaos. Error budgets exist specifically to balance these.",
    "withoutA": "“We shipped 40 features!” (but our biggest customer churned due to unreliability).",
    "withoutB": "“99.99% uptime!” (but we shipped 2 features in 6 months and lost market position).",
    "balancingStrategy": "SLO compliance > 97% → ship freely. 94-97% → reduce deploy frequency, investigate. < 94% → feature freeze until budget recovers."
  },
  {
    "id": "MP3",
    "name": "People ↔ Delivery",
    "slug": "people-delivery",
    "metricA": {
      "id": "5.1",
      "alsoIds": [
        "7.8"
      ],
      "role": "Team health signal"
    },
    "metricB": {
      "id": "2.5",
      "alsoIds": [
        "1.2",
        "1.4"
      ],
      "role": "Delivery outcome"
    },
    "whyPaired": "Happy teams that don’t ship aren’t sustainable. Productive teams that are miserable won’t last. You need both to know if performance is real and sustainable.",
    "withoutA": "“We hit every sprint goal!” (but half the team is interviewing elsewhere).",
    "withoutB": "“Team satisfaction is 4.8/5!” (but lead time is 3 weeks and commitments miss 50%).",
    "balancingStrategy": "DX Score < 3.5/5 → prioritize developer experience improvements over sprint goals for one cycle. Track whether improved DX lifts delivery within 2 sprints."
  },
  {
    "id": "MP4",
    "name": "Investment ↔ Outcome",
    "slug": "investment-outcome",
    "metricA": {
      "id": "8.4",
      "role": "Effort allocation"
    },
    "metricB": {
      "id": "8.2",
      "alsoIds": [
        "8.1"
      ],
      "role": "Strategic outcome"
    },
    "whyPaired": "Knowing where capacity goes is useless without knowing what it produced. Knowing what was produced without knowing what was invested is accidental, not strategic.",
    "withoutA": "“We drove $5M in ARR impact!” (but can’t explain why it took 80% of capacity — was that efficient?).",
    "withoutB": "“We spent 60% on features!” (but can’t say what business value resulted).",
    "balancingStrategy": "Maintain 15-20% protected allocation for non-feature work. Review attribution quarterly — if < 50% of feature work has measurable business impact, tighten the OKR-to-work-item link."
  },
  {
    "id": "MP5",
    "name": "Incidents ↔ Follow-Through",
    "slug": "incidents-follow-through",
    "metricA": {
      "id": "1.3",
      "alsoIds": [
        "3.6"
      ],
      "role": "Incident severity signal"
    },
    "metricB": {
      "id": "3.7",
      "role": "Learning accountability"
    },
    "whyPaired": "Counting incidents without tracking whether you fixed root causes means you’ll count the same incidents again next quarter.",
    "withoutA": "“We complete 95% of action items!” (but completing action items for minor issues while ignoring the critical SLO that burned through budget — no severity context to prioritize).",
    "withoutB": "“We had 12 incidents this quarter, MTTR was 20min” (same 3 root causes repeated 4 times each because nobody completed the action items).",
    "balancingStrategy": "Action item completion < 80% → block new feature work on the affected service until items complete. Repeat incidents from the same root cause → escalate to director."
  },
  {
    "id": "MP6",
    "name": "Managed Exits ↔ Regrettable Loss",
    "slug": "managed-exits-regrettable-loss",
    "metricA": {
      "id": "7.2",
      "role": "Managed exit signal"
    },
    "metricB": {
      "id": "7.1",
      "alsoIds": [
        "7.8"
      ],
      "role": "Talent loss signal"
    },
    "whyPaired": "Non-regrettable attrition reflects healthy performance management. Regrettable attrition reflects retention failures. Without both, you can’t tell if attrition is a problem or a sign of organizational health.",
    "withoutA": "“Our regrettable attrition is 2%!” (but non-regrettable is 0% — performance management has stalled and low performers are staying).",
    "withoutB": "“We managed out 5 people this quarter!” (but also lost 3 staff engineers to competitors — active management is healthy, but retention is failing).",
    "balancingStrategy": "Healthy ratio: non-regrettable > regrettable. If regrettable > 50% of total attrition, investigate retention drivers. If non-regrettable = 0%, performance management may be stalled."
  },
  {
    "id": "MP7",
    "name": "AI Throughput ↔ Verification",
    "slug": "ai-throughput-verification",
    "metricA": {
      "id": "AI-1",
      "role": "AI code volume signal"
    },
    "metricB": {
      "id": "5.4",
      "alsoIds": [
        "6.2",
        "1.4",
        "4.4"
      ],
      "role": "Verification counterweight"
    },
    "whyPaired": "AI accelerates code generation while reviews become rubber stamps on larger PRs. Without verification metrics, bug rates climb silently.",
    "withoutA": "No visibility into how much code is AI-generated — can’t diagnose quality or review bottlenecks.",
    "withoutB": "AI accelerates code generation while reviews become rubber stamps on 154% larger PRs (Faros AI industry report, 2024), and bug rates climb 9% (Faros AI industry report, 2024).",
    "balancingStrategy": "If AI PR review time > 1.5x human PR review time, reduce AI PR size limits or add automated pre-review checks. If AI CFR delta > 5%, pause AI adoption expansion."
  }
]