[
  {
    "id": "MP1",
    "name": "Velocity \u2194 Quality",
    "slug": "velocity-quality",
    "metricA": { "id": "1.2", "alsoIds": ["1.1"], "role": "Speed signal" },
    "metricB": { "id": "1.4", "alsoIds": ["3.3"], "role": "Quality counterweight" },
    "whyPaired": "Speed without a quality check incentivizes shipping broken code fast. Quality without a speed check incentivizes paralysis.",
    "withoutA": "\u201cWe have 0% failure rate!\u201d (because we deploy once a month after 3 weeks of manual QA).",
    "withoutB": "\u201cWe deploy 50x/day!\u201d (but 20% of deploys break things).",
    "balancingStrategy": "Use error budgets as the arbitrator. Budget remaining \u2192 optimize for speed. Budget burning > 1x \u2192 investigate CFR root causes before pushing velocity."
  },
  {
    "id": "MP2",
    "name": "Reliability \u2194 Velocity",
    "slug": "reliability-velocity",
    "metricA": { "id": "3.5", "alsoIds": ["3.3"], "role": "Reliability signal" },
    "metricB": { "id": "1.2", "alsoIds": ["1.1"], "role": "Delivery speed" },
    "whyPaired": "Reliability without velocity tracking leads to over-engineering. Velocity without reliability tracking leads to chaos. Error budgets exist specifically to balance these.",
    "withoutA": "\u201cWe shipped 40 features!\u201d (but our biggest customer churned due to unreliability).",
    "withoutB": "\u201c99.99% uptime!\u201d (but we shipped 2 features in 6 months and lost market position).",
    "balancingStrategy": "SLO compliance > 97% \u2192 ship freely. 94-97% \u2192 reduce deploy frequency, investigate. < 94% \u2192 feature freeze until budget recovers."
  },
  {
    "id": "MP3",
    "name": "People \u2194 Delivery",
    "slug": "people-delivery",
    "metricA": { "id": "5.1", "alsoIds": ["7.8"], "role": "Team health signal" },
    "metricB": { "id": "2.5", "alsoIds": ["1.2", "1.4"], "role": "Delivery outcome" },
    "whyPaired": "Happy teams that don\u2019t ship aren\u2019t sustainable. Productive teams that are miserable won\u2019t last. You need both to know if performance is real and sustainable.",
    "withoutA": "\u201cWe hit every sprint goal!\u201d (but half the team is interviewing elsewhere).",
    "withoutB": "\u201cTeam satisfaction is 4.8/5!\u201d (but lead time is 3 weeks and commitments miss 50%).",
    "balancingStrategy": "DX Score < 3.5/5 \u2192 prioritize developer experience improvements over sprint goals for one cycle. Track whether improved DX lifts delivery within 2 sprints."
  },
  {
    "id": "MP4",
    "name": "Investment \u2194 Outcome",
    "slug": "investment-outcome",
    "metricA": { "id": "8.4", "role": "Effort allocation" },
    "metricB": { "id": "8.2", "alsoIds": ["8.1"], "role": "Strategic outcome" },
    "whyPaired": "Knowing where capacity goes is useless without knowing what it produced. Knowing what was produced without knowing what was invested is accidental, not strategic.",
    "withoutA": "\u201cWe drove $5M in ARR impact!\u201d (but can\u2019t explain why it took 80% of capacity \u2014 was that efficient?).",
    "withoutB": "\u201cWe spent 60% on features!\u201d (but can\u2019t say what business value resulted).",
    "balancingStrategy": "Maintain 15-20% protected allocation for non-feature work. Review attribution quarterly \u2014 if < 50% of feature work has measurable business impact, tighten the OKR-to-work-item link."
  },
  {
    "id": "MP5",
    "name": "Incidents \u2194 Follow-Through",
    "slug": "incidents-follow-through",
    "metricA": { "id": "1.3", "alsoIds": ["3.6"], "role": "Incident severity signal" },
    "metricB": { "id": "3.7", "role": "Learning accountability" },
    "whyPaired": "Counting incidents without tracking whether you fixed root causes means you\u2019ll count the same incidents again next quarter.",
    "withoutA": "\u201cWe complete 95% of action items!\u201d (but completing action items for minor issues while ignoring the critical SLO that burned through budget \u2014 no severity context to prioritize).",
    "withoutB": "\u201cWe had 12 incidents this quarter, MTTR was 20min\u201d (same 3 root causes repeated 4 times each because nobody completed the action items).",
    "balancingStrategy": "Action item completion < 80% \u2192 block new feature work on the affected service until items complete. Repeat incidents from the same root cause \u2192 escalate to director."
  },
  {
    "id": "MP6",
    "name": "Managed Exits \u2194 Regrettable Loss",
    "slug": "managed-exits-regrettable-loss",
    "metricA": { "id": "7.2", "role": "Managed exit signal" },
    "metricB": { "id": "7.1", "alsoIds": ["7.8"], "role": "Talent loss signal" },
    "whyPaired": "Non-regrettable attrition reflects healthy performance management. Regrettable attrition reflects retention failures. Without both, you can\u2019t tell if attrition is a problem or a sign of organizational health.",
    "withoutA": "\u201cOur regrettable attrition is 2%!\u201d (but non-regrettable is 0% \u2014 performance management has stalled and low performers are staying).",
    "withoutB": "\u201cWe managed out 5 people this quarter!\u201d (but also lost 3 staff engineers to competitors \u2014 active management is healthy, but retention is failing).",
    "balancingStrategy": "Healthy ratio: non-regrettable > regrettable. If regrettable > 50% of total attrition, investigate retention drivers. If non-regrettable = 0%, performance management may be stalled."
  },
  {
    "id": "MP7",
    "name": "AI Throughput \u2194 Verification",
    "slug": "ai-throughput-verification",
    "metricA": { "id": "AI-1", "role": "AI code volume signal" },
    "metricB": { "id": "5.4", "alsoIds": ["6.2", "1.4", "4.4"], "role": "Verification counterweight" },
    "whyPaired": "AI accelerates code generation while reviews become rubber stamps on larger PRs. Without verification metrics, bug rates climb silently.",
    "withoutA": "No visibility into how much code is AI-generated \u2014 can\u2019t diagnose quality or review bottlenecks.",
    "withoutB": "AI accelerates code generation while reviews become rubber stamps on 154% larger PRs (Faros AI, 2024), and bug rates climb 9% (Faros AI, 2024).",
    "balancingStrategy": "If AI PR review time > 1.5x human PR review time, reduce AI PR size limits or add automated pre-review checks. If AI CFR delta > 5%, pause AI adoption expansion."
  }
]
