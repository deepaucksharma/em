[
  {
    "id": "AP-01",
    "name": "The Absent Architect",
    "slug": "the-absent-architect",
    "observableIds": [
      "C3-O1"
    ],
    "capabilityId": "C3",
    "shortDesc": "EM defers all technical direction to TL/Staff with no opinion on system evolution. Architecture drifts without vision; tech debt compounds unchecked.",
    "warningSigns": [
      "No tech strategy doc exists or current doc is >6 months stale",
      "Architecture decisions lack documented rationale (no ADRs)",
      "Design reviews are skipped or EM doesn't attend",
      "Tech debt surfaces only through incidents, never through planned review"
    ],
    "impact": "Within 2-3 quarters: team drifts architecturally; senior engineers route around EM for technical decisions; tech debt compounds until velocity drops measurably.",
    "recoveryActions": [
      "Co-author tech strategy doc with TL within first month: current state → target state → migration path.",
      "Attend every design review; contribute by bringing business constraints and asking about failure modes.",
      "Build technical opinion through asking questions, not pretending expertise.",
      "Own tech debt visibility: maintain a registry even if you don't write the remediation code."
    ],
    "sourceTopic": "Technical Strategy & System Ownership",
    "mappingNotes": "Architecture without vision → tech vision observable"
  },
  {
    "id": "AP-02",
    "name": "The Over-Involved Architect",
    "slug": "the-over-involved-architect",
    "observableIds": [
      "C3-O1",
      "C3-O2"
    ],
    "capabilityId": "C3",
    "shortDesc": "EM inserts themselves into every technical decision, bypassing TL and senior engineers. Creates a single point of failure; team cannot function autonomously.",
    "warningSigns": [
      "EM reviews all or most PRs personally",
      "Design docs require EM sign-off before proceeding",
      "Senior engineers escalate decisions they should own; PR approval requires EM sign-off",
      "Team velocity drops measurably when EM is on vacation or in back-to-back meetings"
    ],
    "impact": "Within 1-2 quarters: senior engineers leave (not empowered); team velocity drops when EM is unavailable; EM spends >50% time on IC work instead of management.",
    "recoveryActions": [
      "Create a Decision Authority Matrix within first month: list 10-15 common decision types and assign each to EM, TL, or IC with explicit escalation criteria.",
      "Remove yourself from PR review for all changes under 500 lines that don't cross service boundaries.",
      "Practice 'what do you think?' before 'here's what we should do.'",
      "Track delegation: measure how many decisions you make vs. delegate per week; target <20% EM-decided within 2 months."
    ],
    "sourceTopic": "Technical Strategy & System Ownership",
    "mappingNotes": "Over-involved architect → architecture decisions/delegation observable"
  },
  {
    "id": "AP-03",
    "name": "Metrics Theater",
    "slug": "metrics-theater",
    "observableIds": [
      "C9-O1",
      "C9-O3",
      "C9-O5"
    ],
    "capabilityId": "C9",
    "shortDesc": "Dashboards exist but nobody uses them for decisions. Metrics are collected for reporting, not for diagnosis. The team can't name a single decision that was changed by metric data.",
    "warningSigns": [
      "Beautiful dashboards nobody looks at or acts on",
      "Metrics improve but team doesn't feel faster and customers are unhappy",
      "Metrics targets become team KPIs used in performance reviews",
      "'Deploy frequency' gamed by splitting deploys",
      "Dashboards show lines of code, PRs merged, or tickets closed without connection to business outcomes",
      "No leading indicators — only lagging",
      "Team optimizes for metric movement rather than impact"
    ],
    "impact": "Within 1-2 quarters: metrics become overhead that produces no value; team resents measurement as busywork; delivery problems stay invisible; leadership distrusts engineering's self-reporting.",
    "recoveryActions": [
      "Audit every dashboard: for each metric, name the last decision it informed. Delete metrics with no decision in 90 days.",
      "Start with 3 DORA metrics and one developer satisfaction measure — nothing else until these drive decisions.",
      "Review metrics weekly in team standup: 'What does this data tell us? What should we do differently?'",
      "Track 'decisions driven by data' as a meta-metric for the first 2 quarters."
    ],
    "sourceTopic": "Engineering Excellence & Delivery",
    "mappingNotes": "Metrics theater → outcome metrics / activity metrics observables"
  },
  {
    "id": "AP-04",
    "name": "Velocity Obsession",
    "slug": "velocity-obsession",
    "observableIds": [
      "C8-O1",
      "C8-O2"
    ],
    "capabilityId": "C8",
    "shortDesc": "EM prioritizes feature velocity over reliability until incidents force attention. On-call degrades, post-mortems are skipped or incomplete, and the team enters a cycle of firefighting.",
    "warningSigns": [
      "Rising change failure rate alongside rising deployment frequency",
      "Increasing on-call pages",
      "Growing tech debt backlog",
      "Team working overtime 'to ship'",
      "Cutting testing 'to move faster'"
    ],
    "impact": "Within 2-3 quarters: incident rate increases; on-call becomes unsustainable; senior engineers leave due to burnout; stakeholder trust erodes as outages accumulate.",
    "recoveryActions": [
      "Track and publish on-call page rate, MTTR, and post-mortem action completion alongside velocity metrics.",
      "Implement error budget policy: when error budget is consumed, feature work pauses until reliability improves.",
      "Reserve minimum 20% capacity for reliability work — protect this in sprint planning.",
      "Frame reliability investment in business terms: 'Each Sev1 costs approximately X hours of engineering time and Y in customer impact.'"
    ],
    "sourceTopic": "Engineering Excellence & Delivery",
    "mappingNotes": "Velocity obsession → incident response / delivery quality observables"
  },
  {
    "id": "AP-05",
    "name": "Security As Someone Else's Problem",
    "slug": "security-as-someone-elses-problem",
    "observableIds": [
      "C13-O1",
      "C13-O3",
      "C13-O2"
    ],
    "capabilityId": "C13",
    "shortDesc": "EM treats security as the security team's job rather than embedding it in the engineering workflow. Security reviews become afterthoughts, vulnerability backlogs grow unchecked, and the team discovers threat modeling gaps only through incidents.",
    "warningSigns": [
      "Security reviews are afterthoughts or skipped",
      "No security checkpoints in CI/CD pipeline",
      "Vulnerabilities linger beyond SLA with months-old backlog",
      "Team doesn't know basic security practices",
      "'We'll fix it after launch' or 'we'll add security later' is common",
      "No threat modeling for new features touching auth, payments, or PII"
    ],
    "impact": "Security incidents that damage customer trust; compliance failures with VP-level visibility; forced emergency patching disrupting planned work; regulatory penalties",
    "recoveryActions": [
      "Embed security in the development workflow — add automated security scanning to CI/CD pipeline.",
      "Security champion rotation within team.",
      "Include security in launch checklist and require threat modeling for features touching auth, payments, or PII.",
      "Make vulnerability SLA as visible as uptime SLA with severity-based targets.",
      "Run quarterly security training and make security debt visible alongside tech debt in sprint planning.",
      "Embed security reviews into every design doc and require threat models before launch — security is a first-class gate, not an afterthought.",
      "Require automated security scanning in every CI/CD pipeline; known vulnerabilities block deployment by default."
    ],
    "sourceTopic": "Security & Operational Rigor",
    "mappingNotes": "Security as afterthought → security integration observables"
  },
  {
    "id": "AP-06",
    "name": "Hero Culture",
    "slug": "hero-culture",
    "observableIds": [
      "C8-O1",
      "C8-O3"
    ],
    "capabilityId": "C8",
    "shortDesc": "One or two engineers handle all incidents because they're fastest, creating a single point of failure. The heroes burn out, nobody else learns, and the team is paralyzed when heroes are unavailable.",
    "warningSigns": [
      "Same people always on incident calls",
      "No runbooks",
      "New team members useless during incidents",
      "MTTR depends on who's on-call",
      "Hero engineers burned out"
    ],
    "impact": "Within 1-2 quarters: heroes burn out or leave; remaining team can't handle incidents independently; MTTR spikes when heroes are unavailable; knowledge stays in one person's head.",
    "recoveryActions": [
      "Implement mandatory on-call rotation — every team member takes on-call, including senior engineers who currently dodge it.",
      "Pair junior engineers with experienced ones during on-call rotations for the first 2 cycles.",
      "Require runbooks for every alert so incident response doesn't depend on tribal knowledge.",
      "Track incident resolution by person — if one person handles >40% of incidents, redistribute."
    ],
    "sourceTopic": "Operational Risk & Incident Management",
    "mappingNotes": "Hero culture → incident command / on-call health observables"
  },
  {
    "id": "AP-07",
    "name": "Blame Post-Mortems",
    "slug": "blame-post-mortems",
    "observableIds": [
      "C8-O2"
    ],
    "capabilityId": "C8",
    "shortDesc": "Post-mortems focus on finding someone to blame rather than systemic causes. Team learns to hide incidents and near-misses; repeat failures continue because root causes go unaddressed.",
    "warningSigns": [
      "People avoid documenting what went wrong",
      "Root causes listed as 'human error'",
      "Defensive language in post-mortems",
      "Fewer incidents reported (not fewer incidents occurring)",
      "Near-misses never discussed"
    ],
    "impact": "Within 1-2 quarters: incident reporting drops (hidden incidents); same failures repeat; team psychological safety erodes; voluntary near-miss reporting stops entirely.",
    "recoveryActions": [
      "Rewrite post-mortem template to remove individual names from root cause section — focus on systems, processes, and tooling.",
      "EM explicitly models blameless language in every post-mortem they attend.",
      "Track repeat incident rate as the primary post-mortem effectiveness metric — if incidents repeat, the post-mortem failed.",
      "Celebrate near-miss reporting publicly — reward the behavior you want to see."
    ],
    "sourceTopic": "Operational Risk & Incident Management",
    "mappingNotes": "Blame post-mortems → blameless post-mortem observable"
  },
  {
    "id": "AP-08",
    "name": "Re-org Addiction",
    "slug": "re-org-addiction",
    "observableIds": [
      "C1-O3",
      "C1-O4"
    ],
    "capabilityId": "C1",
    "shortDesc": "Leadership reaches for re-orgs as the default solution to execution problems, restructuring teams every 6-9 months. Chronic instability prevents teams from gelling, destroys institutional knowledge, and masks the real process or leadership issues driving dysfunction.",
    "warningSigns": [
      "More than one re-org per year without a major strategic shift to justify it",
      "Teams never stabilize long enough to establish reliable velocity baselines",
      "Re-orgs announced without root cause analysis of the problems they're meant to solve",
      "Engineers describe their role as 'surviving the next re-org' in skip-level conversations"
    ],
    "impact": "Within 2-3 quarters: team velocity never stabilizes, institutional knowledge fragments, high performers leave for stability, remaining engineers become change-fatigued and disengage from improvement initiatives.",
    "recoveryActions": [
      "Establish a minimum 6-month stability period between structural changes.",
      "Require written root cause analysis before any re-org proposal — prove the problem is structural, not execution.",
      "Track team stability metrics (tenure in current structure, velocity trend) as a first-class org health indicator.",
      "Retrospect on past re-orgs: did they achieve their stated goals? Use evidence to break the cycle."
    ],
    "sourceTopic": "Org Design & Team Topologies",
    "mappingNotes": "Re-org addiction → re-org execution / signal recognition observables"
  },
  {
    "id": "AP-09",
    "name": "Fuzzy Ownership",
    "slug": "fuzzy-ownership",
    "observableIds": [
      "C1-O5"
    ],
    "capabilityId": "C1",
    "shortDesc": "Service ownership is ambiguous — multiple teams claim the same component, or nobody claims it. Incidents escalate slowly, accountability dissolves, and cross-team friction compounds as ownership gaps become recurring sources of dropped work.",
    "warningSigns": [
      "Incidents where 3+ teams are paged and nobody acts for >10 minutes",
      "Features that 'slip through the cracks' between teams",
      "No central ownership registry, or the registry is >3 months stale",
      "Engineers say 'I thought the other team owned that' more than once per quarter"
    ],
    "impact": "Within 1-2 quarters: incident MTTR increases as ownership disputes delay response; orphaned services accumulate tech debt; engineers lose trust in organizational structure and work around it with shadow processes.",
    "recoveryActions": [
      "Build a service-to-team ownership registry and audit it monthly.",
      "Assign a single DRI for every production service — never split ownership.",
      "Resolve ownership disputes within one week using an escalation-to-Director path.",
      "Flag orphaned services within one sprint of any team change."
    ],
    "sourceTopic": "Org Design & Team Topologies",
    "mappingNotes": "Fuzzy ownership → ownership registry observable"
  },
  {
    "id": "AP-10",
    "name": "Conflict Avoidance",
    "slug": "conflict-avoidance",
    "observableIds": [
      "C6-O2",
      "C14-O4"
    ],
    "capabilityId": "C6",
    "shortDesc": "EM avoids difficult conversations — underperformance goes unaddressed for quarters, feedback stays vague and positive, and real issues surface only in back-channels. High performers leave frustrated by inequity while team standards quietly erode.",
    "warningSigns": [
      "Underperformers coasting for quarters",
      "Team frustration about 'dead weight'",
      "EM gives everyone positive feedback",
      "Real issues discussed in back-channels",
      "Skip-level 1:1s reveal problems EM hasn't addressed"
    ],
    "impact": "High performers leave (frustrated by inequity); team standards erode; EM loses credibility with leadership and team; eventual crisis when problems can't be ignored anymore",
    "recoveryActions": [
      "Start with smallest difficult conversation and build the muscle.",
      "Use SBI framework to make feedback specific.",
      "Set personal goal: address one difficult issue per week.",
      "Get coaching from your manager on delivery.",
      "Set a personal SLA: address difficult conversations within 48 hours — treating avoidance itself as a performance gap.",
      "Track improvement via metric M-7.8 (Manager Effectiveness Score)."
    ],
    "sourceTopic": "Team Health & Execution",
    "mappingNotes": "Conflict avoidance → underperformance / feedback observables"
  },
  {
    "id": "AP-11",
    "name": "The Bubble",
    "slug": "the-bubble",
    "observableIds": [
      "C12-O1"
    ],
    "capabilityId": "C12",
    "shortDesc": "EM operates in an information bubble where only good news flows upward because the team doesn't feel safe sharing problems. Real issues surface through incidents or resignations rather than through communication, blindsiding the EM and eroding leadership trust.",
    "warningSigns": [
      "EM thinks team is happy",
      "Engagement survey results are surprising",
      "Problems discovered through incidents or attrition, not through communication",
      "1:1s are pleasant but shallow"
    ],
    "impact": "Real problems invisible until they explode; EM blindsided by resignations; incidents that could have been prevented; leadership surprised by team issues EM should have known about",
    "recoveryActions": [
      "Ask specific questions (not 'how are things?' but 'what's the most frustrating part of your work right now?').",
      "Share your own concerns first (model vulnerability).",
      "Act on small feedback to prove it's safe to share bigger feedback.",
      "Anonymous team health surveys as backup signal.",
      "Research identifies psychological safety as the #1 predictor of team effectiveness; mandatory quarterly skip-level surveys help surface problems that 1:1s miss."
    ],
    "sourceTopic": "Team Health & Execution",
    "mappingNotes": "The bubble → psychological safety observable"
  },
  {
    "id": "AP-12",
    "name": "The 24/7 Manager",
    "slug": "the-247-manager",
    "observableIds": [
      "C7-O3",
      "C4-O7"
    ],
    "capabilityId": "C7",
    "shortDesc": "EM is always available, always responsive, and burning out while modeling unsustainable pace that the team feels pressured to match. Decision quality degrades with fatigue, the team adopts unhealthy norms, and the EM becomes a single point of failure.",
    "warningSigns": [
      "EM responds to Slack at midnight and on weekends",
      "Team feels guilty taking PTO because EM never does",
      "EM's personal health and relationships are suffering",
      "Manager praises EM's 'dedication' without seeing the burnout trajectory"
    ],
    "impact": "EM burnout and potential departure; team adopts unsustainable norms; no one takes vacation; EM becomes bottleneck; quality of EM's decisions degrades with fatigue",
    "recoveryActions": [
      "Set and enforce personal boundaries visibly — the team takes cues from your behavior.",
      "Take PTO and be genuinely unreachable to prove the team can function without you.",
      "Tell your manager you're overloaded — it's not a weakness, it's a signal.",
      "Track your own working hours for a month and share the data with your manager.",
      "Microsoft's Model-Coach-Care framework explicitly treats sustainable pace as a leadership competency — leaders who model overwork are failing, not succeeding.",
      "See Playbook: 'Your On-Call Rotation Is Burning People Out' (P-C8-5) for structural approaches to preventing burnout through sustainable on-call and workload practices."
    ],
    "sourceTopic": "Team Health & Execution",
    "mappingNotes": "24/7 manager → proactive status / focus protection observables"
  },
  {
    "id": "AP-13",
    "name": "Lowering the Bar Under Pressure",
    "slug": "lowering-the-bar-under-pressure",
    "observableIds": [
      "C11-O1",
      "C11-O2"
    ],
    "capabilityId": "C11",
    "shortDesc": "EM lowers the hiring bar under pressure from open reqs and leadership urgency, accepting candidates who don't meet the standard. Poor hires drag down team velocity, consume disproportionate management attention, and often exit within 12 months — costing far more than the vacancy they filled.",
    "warningSigns": [
      "Declining interview scores accepted with 'we need someone'",
      "Onboarding problems increase",
      "Regrettable hires within 12 months",
      "Team quality diluted"
    ],
    "impact": "Poor hires drag down team velocity and morale; management overhead increases; eventual PIP/exit process (6-12 months wasted); team loses trust in hiring process",
    "recoveryActions": [
      "Hold the bar.",
      "Show leadership the cost of bad hires (in months of lost productivity, management time, and team morale).",
      "Track quality-of-hire metrics.",
      "Better to be understaffed for 3 months than to have a bad hire for 12.",
      "Give an independent interviewer (bar raiser) veto power specifically to prevent pressure-driven hiring compromises, structurally protecting quality."
    ],
    "sourceTopic": "Talent Acquisition & Team Building",
    "mappingNotes": "Hiring bar collapse → calibrated hiring / bar maintenance observables"
  },
  {
    "id": "AP-14",
    "name": "The Clone Factory",
    "slug": "the-clone-factory",
    "observableIds": [
      "C11-O1",
      "C12-O5"
    ],
    "capabilityId": "C11",
    "shortDesc": "Hiring process unconsciously filters for 'culture fit' meaning 'like us,' producing a homogeneous team that thinks alike. Groupthink emerges in design reviews, blind spots compound in product decisions, and the team becomes hostile to anyone who doesn't match the dominant profile.",
    "warningSigns": [
      "Homogeneous team",
      "Same backgrounds and perspectives",
      "Groupthink in design reviews",
      "Interview process unconsciously filters for 'culture fit' meaning 'like us'"
    ],
    "impact": "Blind spots in product decisions; lack of innovation; hostile environment for anyone different; legal and compliance risk; poor representation of user base",
    "recoveryActions": [
      "Audit: who have you hired in the last year? What perspectives are missing? Diversify interview panels.",
      "Replace 'culture fit' with 'culture add'",
      "Structured rubrics that evaluate skills, not likability.",
      "Track pipeline and conversion diversity.",
      "Use structured hiring rubrics that evaluate demonstrated competencies rather than 'culture fit,' and deliberately compose interview panels for diversity of perspective."
    ],
    "sourceTopic": "Talent Acquisition & Team Building",
    "mappingNotes": "Clone factory → hiring process / inclusive practices observables"
  },
  {
    "id": "AP-15",
    "name": "The Permanent PIP",
    "slug": "the-permanent-pip",
    "observableIds": [
      "C14-O6"
    ],
    "capabilityId": "C14",
    "shortDesc": "PIPs are designed to fail — documentation starts only after the exit decision is already made, with no genuine coaching or support. The team learns that performance management equals termination, creating a culture of fear where honest feedback and real development never happen.",
    "warningSigns": [
      "PIPs designed to fail",
      "Documentation started only when exit decision already made",
      "No real coaching or support during PIP",
      "Team perceives PIPs as 'you're being fired slowly'"
    ],
    "impact": "Legal risk from poorly documented process; team distrust of performance management; good people scared of making mistakes; HR partnership damaged",
    "recoveryActions": [
      "Genuine coaching before PIP.",
      "PIPs with real support and realistic goals.",
      "Track: what percentage of PIPs result in improvement? If zero, your PIPs are exit processes dressed up as development.",
      "Be honest with yourself about intent.",
      "Apply the Keeper Test honestly: 'Would I fight to keep this person?' If not, have a direct conversation and offer a generous, respectful exit instead of a performative PIP."
    ],
    "sourceTopic": "Performance, Calibration & Growth",
    "mappingNotes": "Permanent PIP → fair PIP process observable"
  },
  {
    "id": "AP-16",
    "name": "Promotion as Retention",
    "slug": "promotion-as-retention",
    "observableIds": [
      "C14-O5",
      "C6-O4"
    ],
    "capabilityId": "C14",
    "shortDesc": "EM promotes engineers who haven't demonstrated sustained next-level impact, using title inflation as a retention lever. Newly promoted engineers struggle at their level, calibration credibility erodes, and the leveling system loses meaning — creating inequity for those who earned their level through evidence.",
    "warningSigns": [
      "Engineers promoted without next-level evidence",
      "Newly promoted engineers struggling at new level",
      "Promo rate high but calibration pushback increasing",
      "'title inflation'"
    ],
    "impact": "Devalued levels; promoted engineers failing and demoralized; calibration credibility lost; downstream managers inherit misleveled reports; unfair to people who earned their level",
    "recoveryActions": [
      "Separate retention from promotion.",
      "Retention levers: comp adjustment, scope change, recognition, project assignment.",
      "Promotion: only when sustained next-level evidence exists.",
      "Have honest career conversation: 'you're valued at current level — here's what next level requires'",
      "Separate the promotion decision from the direct manager through a cross-team committee, preventing emotional retention-driven promotions and ensuring calibration consistency."
    ],
    "sourceTopic": "Performance, Calibration & Growth",
    "mappingNotes": "Promotion as retention → promotion evidence / high performer management observables"
  },
  {
    "id": "AP-17",
    "name": "The Lake Wobegon Effect",
    "slug": "the-lake-wobegon-effect",
    "observableIds": [
      "C14-O3",
      "C14-O2"
    ],
    "capabilityId": "C14",
    "shortDesc": "EM rates everyone 'Exceeds Expectations,' avoiding the discomfort of honest differentiation. Calibration committees override ratings, top performers don't feel recognized, underperformers never receive clear signal, and the EM's judgment credibility collapses with leadership.",
    "warningSigns": [
      "All reports rated 'Exceeds Expectations'",
      "Calibration committee overrides your ratings",
      "No one on your team has development areas in their review",
      "Skip-level feedback contradicts your assessments"
    ],
    "impact": "Credibility destroyed in calibration; your strongest people don't feel differentiated; underperformers never get clear signal; leadership doesn't trust your judgment",
    "recoveryActions": [
      "Honest self-check: is everyone on your team truly exceeding? If yes, you have an exceptional team AND you should still differentiate within 'exceeds'",
      "Calibrate with peer managers.",
      "Get comfortable with the discomfort of honest differentiation.",
      "Require managers to justify clustering at any rating level with specific evidence — making Lake Wobegon ratings structurally difficult to sustain."
    ],
    "sourceTopic": "Performance, Calibration & Growth",
    "mappingNotes": "Lake Wobegon effect → ratings distribution / calibration observables"
  },
  {
    "id": "AP-18",
    "name": "Culture by Accident",
    "slug": "culture-by-accident",
    "observableIds": [
      "C12-O2",
      "C12-O1"
    ],
    "capabilityId": "C12",
    "shortDesc": "No intentional team culture exists — norms emerge from the loudest personalities by default. Toxic behaviors go unchecked, new hires absorb dysfunction, and the team's identity becomes whatever the most aggressive person models.",
    "warningSigns": [
      "Team norms driven by most senior/vocal person",
      "New hires absorb unhealthy patterns",
      "No written team charter",
      "'that's just how we do things' is the only explanation"
    ],
    "impact": "Culture favors specific personality types; quiet contributors marginalized; bad norms calcify; culture becomes impossible to change as team grows",
    "recoveryActions": [
      "Write it down.",
      "Collaborative team charter session.",
      "Make norms explicit, discussable, and changeable.",
      "Walk every new hire through the team charter in their first week. Ask them after 30 days: 'What norm surprised you? What's missing?' Update the charter based on their feedback.",
      "Culture is a design choice, not an accident.",
      "Run quarterly team Health Checks where squads explicitly rate and discuss cultural dimensions like psychological safety and mission clarity, making culture a deliberate practice."
    ],
    "sourceTopic": "Engineering Culture & Team Identity",
    "mappingNotes": "Culture by accident → team charter / psychological safety observables"
  },
  {
    "id": "AP-19",
    "name": "The Super-EM",
    "slug": "the-super-em",
    "observableIds": [
      "C1-O9"
    ],
    "capabilityId": "C1",
    "shortDesc": "Director bypasses their EMs to manage ICs directly — giving direction that contradicts EM plans and fielding questions that should go to the EM first. EMs become figureheads, never develop independence, and the org can't scale beyond the Director's personal bandwidth.",
    "warningSigns": [
      "ICs come to Director instead of their EM for decisions",
      "Director gives direction that contradicts EM's plan without discussing with EM first",
      "EMs describe themselves as 'message passers' rather than decision-makers",
      "Director's calendar is full of IC-level meetings and code reviews"
    ],
    "impact": "Within 1-2 quarters: EMs become disempowered and either leave or disengage; Director becomes a bottleneck for all decisions; org can't scale because everything routes through one person.",
    "recoveryActions": [
      "Document an explicit delegation charter: what Director owns vs. what EMs own.",
      "Define specific intervention criteria (e.g., missed sprint commitments, attrition spike) — Director only steps in when thresholds are breached.",
      "Redirect IC questions to EMs publicly and consistently for 30 days to reset expectations.",
      "If you don't trust an EM to manage, coach them or replace them — don't work around them."
    ],
    "sourceTopic": "Managing Managers (Director Track)",
    "mappingNotes": "Super-EM Director → delegation at Director altitude observable"
  },
  {
    "id": "AP-20",
    "name": "Absent Director",
    "slug": "absent-director",
    "observableIds": [
      "C6-O6",
      "C6-O7"
    ],
    "capabilityId": "C6",
    "shortDesc": "Director delegates everything then disappears — EMs operate in silos without organizational context, coaching, or cross-team alignment. The absence creates conflicting priorities, strategic drift, and EMs who never develop because nobody is investing in their growth.",
    "warningSigns": [
      "EMs making conflicting decisions",
      "No cross-team alignment",
      "EMs don't know organizational context",
      "Director can't articulate what their teams are doing",
      "Skip-level feedback: 'I never see my Director'"
    ],
    "impact": "EMs operate in silos; cross-team problems fester; org-level strategy is absent; EMs don't develop; eventually a crisis reveals the gap",
    "recoveryActions": [
      "Regular 1:1s with each EM (non-negotiable).",
      "Monthly skip-level 1:1s.",
      "Quarterly org-level strategy review.",
      "Be present for incidents, calibration, and planning.",
      "The job is coaching and context-setting, not delegation and disappearance.",
      "Skip-level 1:1s and monthly org health reviews are non-negotiable — absence from these should be flagged in upward feedback cycles."
    ],
    "sourceTopic": "Managing Managers (Director Track)",
    "mappingNotes": "Absent Director → EM coaching / bench building observables"
  },
  {
    "id": "AP-21",
    "name": "The Feature Factory",
    "slug": "the-feature-factory",
    "observableIds": [
      "C2-O1",
      "C2-O2"
    ],
    "capabilityId": "C2",
    "shortDesc": "Team builds whatever PM asks without questioning whether it's the right investment. Engineering becomes an order-taking function, strategic technical opportunities are missed, and ROI per engineer steadily declines.",
    "warningSigns": [
      "Roadmap is just PM's feature list",
      "No engineering-initiated work",
      "Tech debt never prioritized",
      "Team can't articulate why they're building what they're building",
      "No OKRs, just a Jira backlog"
    ],
    "impact": "Engineering becomes commoditized; tech debt accumulates; best engineers leave (bored/frustrated); team has no strategic impact; engineering voice absent from product decisions",
    "recoveryActions": [
      "Negotiate engineering-owned capacity (minimum 20% for reliability, tech debt, DX).",
      "Participate in product strategy, not just execution.",
      "Push back on features without clear outcome metrics.",
      "Write OKRs that include engineering health, not just feature delivery.",
      "Require teams to own outcomes (metrics), not just outputs (features) — teams push back on feature requests that don't connect to their mission and OKRs."
    ],
    "sourceTopic": "Strategic Alignment & Roadmapping",
    "mappingNotes": "Feature factory → structured planning / trade-off framing observables"
  },
  {
    "id": "AP-22",
    "name": "The Adversarial Triad",
    "slug": "the-adversarial-triad",
    "observableIds": [
      "C5-O1",
      "C5-O2"
    ],
    "capabilityId": "C5",
    "shortDesc": "Eng/PM/Design are in constant conflict, treating each other as obstacles instead of partners. Chronic misalignment causes rework cycles, delivery slows, and the best people in all three functions leave.",
    "warningSigns": [
      "Blame between functions when things go wrong",
      "PM and Eng have separate roadmaps",
      "Design excluded from technical decisions",
      "'us vs them' language in team meetings"
    ],
    "impact": "Chronic misalignment; rework cycles; slow delivery; toxic culture; best people in all three functions leave; leadership has to mediate every disagreement",
    "recoveryActions": [
      "Joint retro with all three functions: 'what's broken in how we work together?' Shared OKRs (not separate Eng and PM OKRs).",
      "Regular triad syncs.",
      "Build personal relationships with PM and Design counterparts.",
      "Model collaborative behavior.",
      "Operate as co-equal partners (Engineering + Product + Design) with shared OKRs, structurally preventing the adversarial dynamic."
    ],
    "sourceTopic": "Cross-Functional Partnership",
    "mappingNotes": "Adversarial triad → triad health / PM disagreement observables"
  },
  {
    "id": "AP-23",
    "name": "The Surprise Factory",
    "slug": "the-surprise-factory",
    "observableIds": [
      "C7-O3",
      "C7-O4"
    ],
    "capabilityId": "C7",
    "shortDesc": "Leadership consistently learns about problems — project delays, team issues, partner friction — from other people rather than from the EM. Trust erodes rapidly, autonomy gets revoked, and the EM's reputation for reliability collapses as surprises accumulate.",
    "warningSigns": [
      "VP asks about a project delay you haven't reported",
      "Partner team escalates an issue you thought was contained",
      "Your manager is blindsided in a meeting",
      "'why am I hearing about this from [someone else]?'"
    ],
    "impact": "Trust destroyed; autonomy revoked; micromanagement increases; reputation for being unreliable; career advancement stalls",
    "recoveryActions": [
      "Over-communicate, especially about risks and problems.",
      "Rule: your manager should never be surprised by anything about your team.",
      "Flag risks early with your assessment and plan — even if you're still working on it.",
      "Bad news doesn't age well.",
      "Treat escalation as a leadership behavior, not a failure — weekly business reviews are designed to catch problems before they surprise leadership."
    ],
    "sourceTopic": "Stakeholder Management & Influence",
    "mappingNotes": "Surprise factory → proactive status / bad news delivery observables"
  },
  {
    "id": "AP-24",
    "name": "Empire Building",
    "slug": "empire-building",
    "observableIds": [
      "C5-O10",
      "C5-O7"
    ],
    "capabilityId": "C5",
    "shortDesc": "EM expands scope and absorbs teams to grow their org size rather than for customer or business impact. Resources get spread thin across non-critical work, peers lose respect for the political maneuvering, and the bloated org becomes a target during the next rebalancing cycle.",
    "warningSigns": [
      "Politicking for headcount without clear need",
      "Absorbing teams without clear benefit",
      "Optimizing for org size rather than org impact",
      "'more reports = more important'"
    ],
    "impact": "Bloated org with unclear purpose; resources wasted on non-critical work; peers and leadership lose respect; eventually cut during rebalancing; reputation damaged",
    "recoveryActions": [
      "Ask yourself: does this scope expansion serve customers/business, or my ego? Can I articulate the impact in non-political terms? Would I make this case if it meant giving up headcount elsewhere? Build reputation on impact, not empire size.",
      "Tie headcount allocation to demonstrated impact, not org size; VP reviews should evaluate impact-per-engineer, making empire building visible and unrewarded.",
      "Institute a quarterly impact-per-engineer review: for every team under your org, calculate key outcomes divided by headcount. If any team's ratio is declining while headcount grows, trigger a scope audit before approving further expansion."
    ],
    "sourceTopic": "Stakeholder Management & Influence",
    "mappingNotes": "Empire building → scope navigation / political capital observables"
  },
  {
    "id": "AP-25",
    "name": "The Blank Check Mentality",
    "slug": "the-blank-check-mentality",
    "observableIds": [
      "C10-O1",
      "C10-O3",
      "C10-O5"
    ],
    "capabilityId": "C10",
    "shortDesc": "EM asks for resources without cost consciousness, ROI justification, or capacity models linking headcount to deliverables. Finance and leadership see engineering as a cost center, budget requests get denied, and the team eventually faces forced cuts.",
    "warningSigns": [
      "Every problem's solution is 'we need more headcount'",
      "No cost tracking for cloud/infra",
      "Proposals lack ROI analysis",
      "Never voluntarily reducing spend",
      "Surprised by budget discussions",
      "Headcount requests lack quantitative justification",
      "Hiring doesn't improve velocity proportionally",
      "No tracking of cost-per-outcome"
    ],
    "impact": "Finance and leadership see engineering as cost center; budget requests denied; eventually forced cuts because spending wasn't justified; credibility as business partner damaged; team grows without proportional output gains; organization becomes bloated over time",
    "recoveryActions": [
      "Track team costs proactively.",
      "Know your cost-per-engineer-month.",
      "Quantify ROI for every investment.",
      "Voluntarily identify savings opportunities.",
      "Frame asks in business terms, not 'we need more people'",
      "Build a capacity model linking headcount to deliverables.",
      "Track cost-per-feature or cost-per-initiative.",
      "Prove you can do more with current team before asking for more people.",
      "Include three options (minimum/target/stretch) with quantified ROI for each in every resource request; have finance partners validate assumptions before approval."
    ],
    "sourceTopic": "Budget, Headcount & Resource Planning",
    "mappingNotes": "Blank check mentality → headcount ROI / cost management observables"
  },
  {
    "id": "AP-26",
    "name": "Analysis Paralysis",
    "slug": "analysis-paralysis",
    "observableIds": [
      "C7-O6",
      "C2-O3"
    ],
    "capabilityId": "C7",
    "shortDesc": "Every decision requires exhaustive analysis and data-gathering before action, trapping the team in perpetual research mode. Opportunities expire, competitors ship, and the team's risk aversion becomes the biggest risk of all.",
    "warningSigns": [
      "Simple decisions take weeks because 'we need more data'",
      "Team waits for EM approval on everything",
      "Meetings to plan meetings",
      "'Let's get more data' is the default response to any decision",
      "Decision documents grow to 10+ pages for reversible choices"
    ],
    "impact": "Slow delivery; team frustration; missed opportunities; competitors move faster; perception as indecisive leader",
    "recoveryActions": [
      "Classify every decision by reversibility: reversible decisions (most) should be made in under 24 hours with monitoring.",
      "Set analysis deadlines: irreversible decisions get a time-boxed analysis period, not open-ended research.",
      "Default to action with monitoring, not more analysis.",
      "Delegate decisions aggressively — ask 'what's the worst that happens if we're wrong?'",
      "Classify decisions as Type 1 (irreversible, need deep analysis) vs. Type 2 (reversible, bias for action) — most decisions are Type 2 and should be made at 70% information.",
      "Set a 48-hour SLA for Type 2 decisions and a 1-week SLA for Type 1 decisions. Track decision throughput weekly in standup: count decisions made vs. decisions pending. If the pending queue grows two sprints in a row, mandate that the next pending decision is made by end-of-day with a DACI owner."
    ],
    "sourceTopic": "Decision Making & Prioritization",
    "mappingNotes": "Analysis paralysis → DACI framework / reversibility matching observables"
  },
  {
    "id": "AP-27",
    "name": "The Yes Machine",
    "slug": "the-yes-machine",
    "observableIds": [
      "C2-O2",
      "C2-O5"
    ],
    "capabilityId": "C2",
    "shortDesc": "EM agrees to every request from stakeholders without pushing back or making trade-offs visible, leaving the team chronically overcommitted. Deadlines slip, quality degrades, and credibility collapses as the gap between promises and delivery widens every quarter.",
    "warningSigns": [
      "Team overcommitted",
      "Every stakeholder thinks their request is accepted",
      "Deadlines consistently missed",
      "Team working overtime",
      "EM apologizing constantly"
    ],
    "impact": "Team burnout; credibility destroyed (promises not kept); stakeholders lose trust; team loses respect for EM; quality degrades",
    "recoveryActions": [
      "Practice saying: 'Yes, and here's what we'd need to deprioritize.' Track commitments against capacity (make overcommitment visible).",
      "One uncomfortable 'no' is better than ten broken 'yes' commitments.",
      "Your team will thank you for protecting their focus.",
      "Voice dissent with data before committing — saying no with evidence is expected, not punished.",
      "Maintain a visible capacity dashboard showing committed vs. available capacity for the current and next sprint. Require every new request to explicitly name what it displaces. Review the dashboard in weekly stakeholder syncs so overcommitment is impossible to hide."
    ],
    "sourceTopic": "Decision Making & Prioritization",
    "mappingNotes": "Yes machine → planning process / trade-off visibility observables"
  },
  {
    "id": "AP-28",
    "name": "The IC Trap",
    "slug": "the-ic-trap",
    "observableIds": [
      "C6-O5",
      "C6-O4"
    ],
    "capabilityId": "C6",
    "shortDesc": "EM spends their time writing code and reviewing PRs instead of doing the actual management work their team needs. Team growth stalls, 1:1s become status updates, and the EM becomes an expensive IC while management gaps compound.",
    "warningSigns": [
      "EM has more GitHub commits than some ICs",
      "1:1s frequently rescheduled or cut short because of 'a bug I need to fix'",
      "Career conversations haven't happened in quarters",
      "EM is deep in code during incidents instead of coordinating",
      "Team members learn about promotions and calibration from peers, not their manager"
    ],
    "impact": "Team members stall in career development because nobody is sponsoring or coaching them; high performers leave for managers who invest in their growth; EM becomes a mediocre IC and a mediocre manager simultaneously; skip-level feedback reveals people feel unsupported; EM's leadership skills atrophy while their code skills stay just current enough to be dangerous",
    "recoveryActions": [
      "Set a hard limit: zero production code, maximum 2 hours per week on code review (for context, not gatekeeping).",
      "Block 1:1 time as immovable.",
      "Write down each report's career goals and review monthly.",
      "Ask yourself: 'If I disappeared for a week, would the code or the people suffer more?' The answer tells you where you're underinvesting.",
      "Find your IC fix through mentoring, architecture reviews, or prototyping — not through owning production code paths.",
      "Measure your effectiveness through engagement surveys and promotion rates of direct reports — coding output is not a management effectiveness metric."
    ],
    "sourceTopic": "Coaching & Talent Development",
    "mappingNotes": "IC trap → career development / growth plan observables"
  },
  {
    "id": "AP-29",
    "name": "Death by Metrics",
    "slug": "death-by-metrics",
    "observableIds": [
      "C9-O1",
      "C9-O4"
    ],
    "capabilityId": "C9",
    "shortDesc": "EM introduces 20+ metrics, dashboards, and reports. Team spends more time measuring than delivering. Engineers game metrics or ignore them entirely because the volume is overwhelming.",
    "warningSigns": [
      "Every meeting opens with 15 minutes of charts",
      "Team spends hours per sprint updating Jira for metric accuracy",
      "Engineers asked to justify time not reflected in dashboards",
      "New metrics added but old ones never retired",
      "Team privately jokes about 'feeding the dashboard'",
      "Sprint velocity discussed more than customer impact"
    ],
    "impact": "Within 1 quarter: team resents measurement overhead; engineers optimize for metric gaming rather than actual outcomes; signal lost in noise; trust in metrics collapses.",
    "recoveryActions": [
      "Cut to 5 metrics maximum. For each, it must answer: 'What decision does this inform?'",
      "Use metric pairings (speed + quality) to prevent gaming — never measure one dimension alone.",
      "Start simple and evolve: 3 DORA metrics for 2 quarters before adding anything else.",
      "Remove any metric that hasn't informed a decision in 90 days."
    ],
    "sourceTopic": "Engineering Excellence & Delivery",
    "mappingNotes": "Death by metrics → outcome metrics / activity metrics observables"
  },
  {
    "id": "AP-30",
    "name": "The Consensus Trap",
    "slug": "the-consensus-trap",
    "observableIds": [
      "C7-O1",
      "C7-O6"
    ],
    "capabilityId": "C7",
    "shortDesc": "EM treats every decision as requiring unanimous agreement, turning consensus into a procrastination mechanism. Decisions stall for weeks, the loudest dissenter holds veto power, and the team learns that objecting is the easiest way to delay anything.",
    "warningSigns": [
      "Reversible decisions take weeks because 'we need to get everyone aligned'",
      "Meetings end with 'let's discuss offline' and nothing happens",
      "One dissenter can block any decision indefinitely",
      "EM restates everyone's position but never makes a call",
      "Team describes the process as 'death by committee'",
      "Follow-up meetings scheduled to continue discussions from previous meetings"
    ],
    "impact": "Velocity collapses; team frustrated by inability to move forward; opinionated engineers leave because they feel unheard (paradoxically, in a consensus culture); competitors ship while you deliberate; EM perceived as indecisive by leadership; decisions eventually made under time pressure are worse than ones made deliberately would have been",
    "recoveryActions": [
      "Distinguish between decisions that need consensus (values, team norms, architectural bets) and those that need a driver (most implementation decisions, process tweaks, tooling choices).",
      "Use DACI or similar framework: one Decider, not a committee.",
      "Set decision deadlines: 'We'll decide by Thursday. Share input by Wednesday.' When consensus isn't reached, make the call yourself and explain your reasoning.",
      "Remember: consensus means 'everyone can commit,' not 'everyone's first choice wins.'",
      "Assign a single-threaded owner to make the call after gathering input — consensus is explicitly not required for Type 2 (reversible) decisions."
    ],
    "sourceTopic": "Decision Framing & Communication",
    "mappingNotes": "Consensus trap → decision framework / DACI observables"
  },
  {
    "id": "AP-31",
    "name": "Meeting Debt & Process Cargo Cult",
    "slug": "meeting-debt",
    "observableIds": [
      "C4-O1",
      "C4-O7"
    ],
    "capabilityId": "C4",
    "shortDesc": "Team drowns in ceremonies and rituals nobody can justify. Engineers get less than two hours of uninterrupted focus time per day; real work happens only after hours.",
    "warningSigns": [
      "Engineers' calendars are a wall of color with less than 2 hours of focus time per day",
      "Standups are status reports to the manager, not team coordination",
      "Retros generate the same action items every sprint but they're never completed",
      "Sprint planning is task assignment, not collaborative estimation",
      "Nobody can explain which meetings are mandatory vs optional, or why any ceremony exists",
      "'Quick sync' is the most common calendar invite",
      "Process was adopted wholesale from another team or textbook without adaptation"
    ],
    "impact": "Within 1-2 quarters: deep work becomes impossible; complex problems don't get solved; engineers burned out from context-switching; senior engineers leave for teams that protect focus time.",
    "recoveryActions": [
      "Calendar audit: have every team member count hours of uninterrupted 2+ hour blocks per week. Target minimum 4 hours of focus time per day.",
      "For every ceremony, ask: 'What problem does this solve for our team?' If no one can answer, cancel it.",
      "Cancel bottom 30% of recurring meetings immediately and monitor whether anyone notices.",
      "Adapt remaining ceremonies to actual needs — async standups, shorter retros, meeting-free blocks.",
      "Measure ceremony value: does the retro generate completed action items? Does standup unblock people? Kill what doesn't deliver."
    ],
    "sourceTopic": "Operational Leadership & Rhythm",
    "mappingNotes": "Meeting debt → operational cadence / focus protection observables"
  },
  {
    "id": "AP-32",
    "name": "Documentation Hoarding",
    "slug": "documentation-hoarding",
    "observableIds": [
      "C4-O9",
      "C4-O3"
    ],
    "capabilityId": "C4",
    "shortDesc": "EM mandates documentation for everything but never ensures it's maintained, discoverable, or useful. A graveyard of stale pages accumulates; engineers stop trusting docs.",
    "warningSigns": [
      "Confluence has hundreds of pages but nobody can find anything",
      "Onboarding docs reference systems that were decommissioned two years ago",
      "Engineers write docs to satisfy process but nobody reads them",
      "'it's documented somewhere' is followed by 30 minutes of searching",
      "PRs require design docs for trivial changes",
      "Documentation is a checkbox, not a communication tool"
    ],
    "impact": "Within 2 quarters: team loses trust in documentation and reverts to tribal knowledge; onboarding takes longer because new hires can't distinguish current from stale docs; time invested in writing is wasted.",
    "recoveryActions": [
      "Quality over quantity: fewer, better docs that are actually maintained.",
      "Assign owners to critical docs with quarterly review dates.",
      "Archive ruthlessly — stale docs are worse than no docs because they erode trust.",
      "Make documentation discoverable: if people can't find it in 60 seconds, it effectively doesn't exist.",
      "Only mandate documentation for high-leverage items: onboarding, runbooks, architecture decisions, API contracts."
    ],
    "sourceTopic": "Operational Leadership & Rhythm",
    "mappingNotes": "Documentation hoarding → knowledge management / operational process observables"
  },
  {
    "id": "AP-33",
    "name": "The Hero EM",
    "slug": "the-hero-em",
    "observableIds": [
      "C6-O1",
      "C6-O7"
    ],
    "capabilityId": "C6",
    "shortDesc": "Manager solves every problem personally, takes over every escalation, and absorbs all challenging work. The team never develops independence, senior engineers atrophy, and the EM becomes a bottleneck that limits the org's capacity to their personal bandwidth.",
    "warningSigns": [
      "EM is on every incident bridge and every escalation call",
      "Team defaults to 'let's ask the EM' instead of figuring it out",
      "When EM takes PTO, things fall apart because no one else has context",
      "Team members aren't growing because EM absorbs all the challenging problems",
      "EM's Slack activity is 3x anyone else's"
    ],
    "impact": "EM burns out — not if, when; team never develops problem-solving skills or ownership; single point of failure at the management layer; when EM inevitably leaves (burnout, promotion, or departure), the team collapses; creates unsustainable expectations for the next EM; team members' careers stall because they never get stretch opportunities; the organization rewards the dysfunction by praising the EM's heroics",
    "recoveryActions": [
      "Stop being the first responder.",
      "When someone brings you a problem, ask 'what do you think we should do?' before offering your solution.",
      "Explicitly assign escalation ownership to team members as growth opportunities.",
      "Set boundaries: pick two nights a week you're offline and stick to it.",
      "Track how many problems you personally solved vs coached someone else through — shift the ratio.",
      "Tell your manager: 'I'm going to step back from X so [team member] can grow into it.",
      "Things might be bumpier for a sprint.' Your job is to build a team that doesn't need you for daily operations.",
      "Manager effectiveness should be measured by whether the team develops independence — heroic individual contribution by the manager should score poorly.",
      "The acid test: can your team handle a major production incident during your vacation without you?"
    ],
    "sourceTopic": "Coaching & Talent Development",
    "mappingNotes": "Hero EM → coaching approach / team independence observables"
  },
  {
    "id": "AP-34",
    "name": "The Infinite Runway",
    "slug": "the-infinite-runway",
    "capabilityId": "C10",
    "observableIds": [
      "C10-O2",
      "C10-O3"
    ],
    "shortDesc": "EM treats cloud/infra spend as unbounded and never prioritizes cost optimization. Costs grow unchecked until finance forces an emergency reduction, diverting the team from planned work to reactive cost-cutting under pressure.",
    "warningSigns": [
      "No cloud cost dashboards or attribution",
      "Cost spikes go unnoticed until finance flags them",
      "'we'll optimize later' is repeated quarterly",
      "No cost targets per service",
      "Engineers have no visibility into cost of their services"
    ],
    "impact": "Cloud costs can grow disproportionately faster than revenue if unmanaged; surprise budget cuts when finance intervenes; engineers don't develop cost awareness; team has no practice cutting when eventually forced to",
    "recoveryActions": [
      "Tag every cloud resource with team and service owner within 2 weeks. Run a monthly cost report showing spend by team, trend vs. prior month, and cost per transaction. Share in leadership review with a 'top 3 savings opportunities' section.",
      "Set cost targets and review monthly.",
      "Make cloud cost a regular sprint metric visible to engineers.",
      "Run a quarterly cost optimization sprint.",
      "Include cost impact in design reviews for new features.",
      "FinOps practice requires every team to track cost-per-transaction and present monthly cost attribution reports, treating infrastructure cost as a first-class design constraint."
    ],
    "sourceTopic": "Resource Allocation & Cost Management",
    "mappingNotes": "Unbounded spending → cost management / reprioritization observables"
  },
  {
    "id": "AP-35",
    "name": "Compliance Cramming",
    "slug": "compliance-cramming",
    "capabilityId": "C13",
    "observableIds": [
      "C13-O3",
      "C13-O4"
    ],
    "shortDesc": "EM treats compliance as a periodic audit exercise, achieving minimal technical compliance without genuine security posture improvement. Day-to-day practices drift from documented standards, and audit preparation becomes a recurring fire drill that disrupts planned work.",
    "warningSigns": [
      "Compliance activities spike before audits",
      "Access reviews happen annually instead of quarterly; permissions accumulate without review",
      "Evidence collection is manual and rushed, only during audit prep",
      "Security reviews are copy-paste from templates",
      "Same audit findings recur year after year",
      "Security champion role is ceremonial; compliance artifacts are stale",
      "Security training is annual checkbox with no retention"
    ],
    "impact": "Audit findings and remediation consume weeks; compliance gaps create legal/financial risk; team treats compliance as punishment rather than practice; access sprawl creates security vulnerabilities",
    "recoveryActions": [
      "Shift from compliance-driven to threat-driven security.",
      "Automate evidence collection and compliance monitoring continuously.",
      "Set all elevated permissions to auto-expire after 24 hours by default. Run quarterly access audits: export all accounts with elevated access, verify each has a business justification, and revoke any without one within 5 business days.",
      "Build compliance checks into deployment pipeline.",
      "Track unique audit findings — repeat findings indicate a checkbox approach.",
      "Require genuine threat models, not template copies.",
      "Treat compliance posture as an ongoing metric, not a periodic event.",
      "Treat compliance as a byproduct of strong security posture, not the goal — continuous monitoring and automated evidence collection replace periodic audit scrambles."
    ],
    "sourceTopic": "Security & Compliance Posture",
    "mappingNotes": "Periodic compliance → access management / continuous compliance observables"
  },
  {
    "id": "AP-36",
    "name": "The Ivory Tower Architect",
    "slug": "the-ivory-tower-architect",
    "capabilityId": "C3",
    "observableIds": [
      "C3-O1",
      "C3-O4"
    ],
    "shortDesc": "EM pushes architectural purity over pragmatic delivery. Systems are technically impressive but ship late; the team gold-plates solutions nobody asked for.",
    "warningSigns": [
      "Every project requires a multi-page design doc before any code is written, regardless of scope or reversibility",
      "Simple features get enterprise-grade architecture (e.g., microservices for a single-use endpoint)",
      "Team velocity is below peer teams despite strong engineers",
      "Architecture reviews block PRs for style/pattern issues, not correctness or risk"
    ],
    "impact": "Within 1-2 quarters: delivery pace falls below 50% of peer teams; team frustration surfaces in retros; business stakeholders lose confidence in engineering timelines.",
    "recoveryActions": [
      "Match architectural rigor to project scope: use a decision matrix mapping project size/reversibility to required documentation level.",
      "Time-box design phases: 1 day for small projects, 1 week for large.",
      "Ask 'what's the simplest thing that could work?' before 'what's the ideal architecture?'",
      "Celebrate pragmatic solutions in team retros alongside technical elegance."
    ],
    "sourceTopic": "Systems Design & Architecture",
    "mappingNotes": "Over-engineering → tech strategy / tech debt visibility observables"
  },
  {
    "id": "AP-37",
    "name": "The Missing Stair",
    "slug": "the-missing-stair",
    "observableIds": [
      "C12-O1",
      "C12-O8"
    ],
    "capabilityId": "C12",
    "shortDesc": "EM tolerates a known toxic or underperforming person because the team has learned to 'work around' them. High performers leave frustrated by the inequity, team standards erode, and the EM's credibility collapses when the situation inevitably escalates.",
    "warningSigns": [
      "New hires warned informally about 'how to handle' a specific person",
      "Team has workarounds for someone's behavior",
      "Complaints in skip-levels about someone EM hasn't addressed",
      "Code review avoidance patterns around one person",
      "1:1s with other reports frequently mention the same person"
    ],
    "impact": "Psychological safety destroyed for everyone except the problem person; high performers leave because they shouldn't have to compensate; team energy diverted to managing around one person instead of doing work; EM's credibility eroded — team sees the avoidance",
    "recoveryActions": [
      "Name it: acknowledge the problem exists instead of hoping it resolves.",
      "Document specific behaviors (not personality traits) with dates and impact.",
      "Deliver direct feedback with clear expectations and timeline.",
      "If behavior doesn't change, escalate to PIP or managed exit.",
      "The team is watching how you handle this — inaction is a decision that signals tolerance.",
      "Research shows that teams with one unaddressed toxic member have 30-40% lower psychological safety scores — the cost of inaction is measurable."
    ],
    "sourceTopic": "Engineering Culture & Team Identity",
    "mappingNotes": "Missing stair → psychological safety / toxic behavior intervention observables"
  },
  {
    "id": "AP-38",
    "name": "Culture Tourism",
    "slug": "culture-tourism",
    "observableIds": [
      "C12-O2",
      "C12-O4"
    ],
    "capabilityId": "C12",
    "shortDesc": "EM treats culture building as periodic events — off-sites, pizza, team lunches — rather than daily structural practices. Surface-level perks mask unaddressed dysfunction, and engineers see through the performative gestures to the real culture underneath.",
    "warningSigns": [
      "Team building = quarterly off-sites",
      "Culture initiative = Slack emoji reactions",
      "No written team norms",
      "Culture conversations only happen when engagement scores drop",
      "Fun activities substituted for addressing real team dysfunction",
      "'we have great culture' but people are leaving"
    ],
    "impact": "Surface-level camaraderie masks deep team dysfunction; real culture problems never addressed; budget spent on events instead of structural changes; team cynical about 'culture initiatives'; new hires experience disconnect between marketed culture and reality",
    "recoveryActions": [
      "Culture is daily decisions, not quarterly events.",
      "Audit: are your team norms written? Does hiring reinforce desired culture? Does recognition align with values? Does the feedback process surface real issues? Events complement structural culture — they can't substitute for it.",
      "Real culture is embedded in writing culture, hiring rubrics, and feedback norms — not in office perks or events."
    ],
    "sourceTopic": "Engineering Culture & Team Identity",
    "mappingNotes": "Culture tourism → team charter / recognition rituals observables"
  },
  {
    "id": "AP-39",
    "name": "Peanut Butter Spreading",
    "slug": "peanut-butter-spreading",
    "observableIds": [
      "C10-O2",
      "C10-O8"
    ],
    "capabilityId": "C10",
    "shortDesc": "EM distributes resources evenly across all projects instead of concentrating on highest-impact bets. Everything moves slowly, nothing ships with impact, and the team delivers mediocre results across the board instead of excellence where it matters.",
    "warningSigns": [
      "Every project gets exactly 1-2 engineers",
      "No project has enough resources to succeed",
      "Everything moves slowly",
      "Nothing gets killed",
      "Team feels spread thin",
      "'we're working on everything' but shipping nothing meaningful"
    ],
    "impact": "No project gets enough investment to succeed; velocity is slow everywhere; team frustrated by inability to finish anything; leadership sees lots of activity but no outcomes; competitors who focus beat you on every front",
    "recoveryActions": [
      "Rank projects by expected impact.",
      "Staff top 2-3 projects properly.",
      "Explicitly pause or kill bottom-tier projects.",
      "Better to ship 3 things excellently than 8 things poorly.",
      "Communicate the focusing decision with rationale.",
      "Small autonomous team models prevent peanut-butter spreading — if a project can't justify a full team, it either gets killed or folded into an existing team's scope."
    ],
    "sourceTopic": "Resource Allocation & Cost Management",
    "mappingNotes": "Peanut butter spreading → aggressive reprioritization / capacity reallocation observables"
  },
  {
    "id": "AP-40",
    "name": "The Telephone Game",
    "slug": "the-telephone-game",
    "observableIds": [
      "C7-O2",
      "C7-O9"
    ],
    "capabilityId": "C7",
    "shortDesc": "Information degrades as it passes through management layers because there's no source-of-truth documentation. Teams operate on conflicting versions of reality, decisions get relitigated, and alignment meetings multiply to compensate for broken information flow.",
    "warningSigns": [
      "Team's understanding of strategy differs from leadership's intent",
      "'I heard from my manager who heard from their manager' is common",
      "Written strategy docs don't exist or are stale",
      "Different teams interpret the same directive differently",
      "Confusion after all-hands about what was actually decided"
    ],
    "impact": "Teams work on the wrong things because they received garbled strategy; leadership confused why execution doesn't match intent; trust erodes as different managers tell different stories; rework when misalignment is eventually discovered",
    "recoveryActions": [
      "Write it down.",
      "Every important decision, strategy shift, and priority change should have a single written source of truth accessible to everyone affected.",
      "Use 'strategy on a page' documents.",
      "After verbal communication, follow up with written summary.",
      "Ask team to reflect back what they understood — check for signal loss.",
      "Written narrative documents (6-pagers or equivalent) ensure that strategic communication is precise and readable by anyone in the org — eliminating the telephone game by design."
    ],
    "sourceTopic": "Decision Framing & Communication",
    "mappingNotes": "Telephone game → communication altitude / scaling communication observables"
  },
  {
    "id": "AP-41",
    "name": "The Recency Trap",
    "slug": "the-recency-trap",
    "observableIds": [
      "C14-O1",
      "C14-O7"
    ],
    "capabilityId": "C14",
    "shortDesc": "EM's performance assessment is dominated by the last 6 weeks instead of the full review period. Engineers who had a strong H1 but quiet H2 get underrated, while those who time visible work to review season get overrated, eroding trust in the process.",
    "warningSigns": [
      "Review feedback mostly references recent projects",
      "Engineers who had strong H1 but quiet H2 get middling reviews",
      "Someone's one mistake in November overshadows 10 months of excellence",
      "EM can't cite specific examples from early in the review period",
      "Performance rating correlates with what happened last month, not the full cycle"
    ],
    "impact": "Reviews feel unfair; high performers who had a rough final month get underrated; coasting employees who happened to have a good recent week get overrated; trust in the review process erodes; engineers learn to time their visible work around review deadlines",
    "recoveryActions": [
      "Keep running performance notes updated weekly — not reconstructed at review time.",
      "Block 30 minutes at the start, midpoint, and end of each performance cycle to re-read your running notes. At midpoint, write a preliminary assessment. At cycle end, compare your preliminary to your final — if they differ significantly, investigate which period you're over-weighting.",
      "Ask for peer feedback that covers the full cycle.",
      "Use brag docs that engineers maintain throughout the cycle.",
      "Structure your review to explicitly address each quarter.",
      "Maintain weekly performance snippets for each report — brief notes that ensure full-cycle visibility rather than recency-dependent assessment."
    ],
    "sourceTopic": "Performance, Calibration & Growth",
    "mappingNotes": "Recency trap → continuous performance documentation / evidence-based reviews observables"
  },
  {
    "id": "AP-42",
    "name": "The Vibes-Based Hire",
    "slug": "the-vibes-based-hire",
    "observableIds": [
      "C11-O1",
      "C11-O9"
    ],
    "capabilityId": "C11",
    "shortDesc": "EM relies on gut feeling and 'culture fit' rather than structured evaluation in hiring. Biased and unpredictable outcomes follow — the team clones its existing demographic, strong candidates are rejected for vague reasons, and hiring quality varies wildly by interviewer.",
    "warningSigns": [
      "Interview feedback uses subjective language ('not a good fit', 'seems sharp', 'I liked them')",
      "No scoring rubrics",
      "Debrief decisions driven by loudest interviewer",
      "Interviewers not calibrated",
      "'culture fit' assessment has no defined criteria",
      "Hire rate correlates with candidate similarity to existing team"
    ],
    "impact": "Inconsistent hiring quality; unconscious bias amplified; diverse candidates systematically disadvantaged; regrettable hires increase; team composition becomes homogeneous; legal risk from undocumented decision criteria",
    "recoveryActions": [
      "Build a scoring rubric for each interview round with 3-5 competencies and behavioral anchors at each level (1-5). Require interviewers to score independently before debrief. Calibrate interviewers quarterly by having 2 people score the same candidate and comparing variance.",
      "Require independent scoring before debrief.",
      "Replace 'culture fit' with specific behavioral criteria ('culture add').",
      "Train all interviewers on rubric use and bias awareness.",
      "Audit hire/reject patterns by demographic.",
      "Structured interviewing with predefined rubrics and independent scoring reduces hiring variance by ~40% compared to unstructured interviews."
    ],
    "sourceTopic": "Talent Acquisition & Team Building",
    "mappingNotes": "Vibes-based hire → structured interview / competency rubric observables"
  },
  {
    "id": "AP-43",
    "name": "The Lagging-Only Dashboard",
    "slug": "the-lagging-only-dashboard",
    "observableIds": [
      "C9-O1",
      "C9-O9"
    ],
    "capabilityId": "C9",
    "shortDesc": "Dashboard shows only lagging indicators (incident count, velocity, release date) with no leading indicators. Problems are visible only after they've already caused damage.",
    "warningSigns": [
      "Dashboard shows outcomes (deployment frequency, MTTR, attrition) but no predictive signals",
      "Team finds out about problems only after incidents or missed deadlines",
      "No developer experience surveys",
      "No leading indicators for team health",
      "Every problem feels like a surprise",
      "Metrics are retrospective, never prospective"
    ],
    "impact": "Within 2 quarters: team can only react to problems, never prevent them; leadership loses trust because bad news always arrives late; no early warning system for delivery or quality risks.",
    "recoveryActions": [
      "Add at least one leading indicator per lagging indicator: pair incident rate with error budget burn rate; pair velocity with WIP count.",
      "Review leading indicators weekly, lagging indicators monthly.",
      "Define action thresholds for leading indicators: 'When WIP exceeds 3x team size, we stop starting and start finishing.'",
      "Track whether leading indicators predicted lagging outcomes — calibrate thresholds quarterly."
    ],
    "sourceTopic": "Metrics, Measurement & Outcomes",
    "mappingNotes": "Lagging-only dashboard → metric pairing / leading indicator observables"
  },
  {
    "id": "AP-44",
    "name": "The Invisible Tech Debt",
    "slug": "the-invisible-tech-debt",
    "observableIds": [
      "C3-O4",
      "C3-O11"
    ],
    "capabilityId": "C3",
    "shortDesc": "Tech debt accumulates invisibly because it's never quantified or communicated in business terms. Leadership sees no reason to fund remediation; velocity degrades until a catastrophic failure forces attention.",
    "warningSigns": [
      "No tech debt registry, backlog, or tracking system exists",
      "Tech debt is only discussed during incident post-mortems",
      "Sprint planning has no explicit tech debt allocation",
      "Engineers do guerrilla debt cleanup without visibility or approval",
      "Deployment frequency has decreased >20% over 2 quarters with no corresponding scope increase"
    ],
    "impact": "Within 2-3 quarters: deployment frequency drops measurably; incident rate increases from accumulated risk; engineering credibility suffers when leadership is blindsided by velocity collapse.",
    "recoveryActions": [
      "Create a tech debt registry within 2 weeks: list top 10 items with business impact estimates (engineer-days lost per sprint, incident risk, blocked features).",
      "Quantify each item: 'This debt costs us X engineer-days per sprint in workarounds and creates Y% probability of incident per quarter.'",
      "Secure minimum 15-20% capacity allocation in next sprint planning cycle.",
      "Track remediation ROI: before/after deployment frequency and incident rate for each resolved item."
    ],
    "sourceTopic": "Systems Design & Architecture",
    "mappingNotes": "Invisible tech debt → tech debt quantification / business-terms communication observables"
  },
  {
    "id": "AP-45",
    "name": "The Silo Builder",
    "slug": "the-silo-builder",
    "observableIds": [
      "C5-O1",
      "C5-O16"
    ],
    "capabilityId": "C5",
    "shortDesc": "EM optimizes for their team's local success at the expense of cross-team collaboration and org-wide outcomes. Peer teams learn to route around the silo, leadership loses trust, and the team's wins come at the cost of org-wide velocity.",
    "warningSigns": [
      "Team builds custom solutions rather than contributing to shared platforms",
      "EM hoards information that would help other teams",
      "Team's velocity is high but cross-team projects stall",
      "EM avoids cross-team meetings",
      "Team's success metrics disconnected from org goals",
      "'not my team's problem' is a common response to cross-team requests"
    ],
    "impact": "Duplicated effort across teams; cross-team initiatives fail; other teams can't depend on your team; EM's reputation suffers despite strong local results; eventually leadership intervenes to force collaboration; org-level outcomes suffer while team-level metrics look great",
    "recoveryActions": [
      "Tie team success metrics to org-level outcomes, not just team-level velocity.",
      "Participate actively in cross-team forums.",
      "Contribute to shared platforms even when it's not the fastest path for your team.",
      "Track cross-team dependencies and proactively unblock them.",
      "Ask: 'Would a peer team describe us as good partners?' Measure teams on shared outcomes, not just team-level metrics — team health checks should include collaboration quality with partner teams."
    ],
    "sourceTopic": "Cross-Functional Partnership",
    "mappingNotes": "Silo builder → cross-team collaboration / triad alignment observables"
  },
  {
    "id": "AP-46",
    "name": "The Accidental Manager",
    "slug": "the-accidental-manager",
    "observableIds": [
      "C6-O1",
      "C6-O13"
    ],
    "capabilityId": "C6",
    "shortDesc": "Someone is promoted into management because they were the best IC, without any management training, coaching, or support. The org loses a great engineer and gains a struggling manager, and the team suffers through months of avoidable mistakes.",
    "warningSigns": [
      "New EM still doing 50%+ IC work",
      "1:1s are technical problem-solving sessions, not coaching conversations",
      "New EM hasn't been trained on feedback delivery, performance management, or conflict resolution",
      "Skip-level feedback reveals reports feel unsupported",
      "EM frustrated by management responsibilities interfering with 'real work'"
    ],
    "impact": "New EM burns out trying to be both manager and IC; reports don't get the coaching and career development they need; strong IC skills wasted on management tasks they haven't been trained for; EM either reverts to IC work (neglecting team) or struggles visibly; some of the best potential managers are lost because the transition was unsupported",
    "recoveryActions": [
      "Never promote to management without an explicit transition plan.",
      "First 90 days: pair with experienced EM mentor, reduce IC commitments to <20%, provide management training (feedback, 1:1s, performance management).",
      "Write down your top 5 success criteria as an EM — team delivery predictability, attrition rate, developer satisfaction, incident response quality, and hiring pipeline health. Track them monthly. Share with your manager in your next 1:1 and agree on which 2-3 matter most this quarter.",
      "Regular check-ins with skip-level on transition health.",
      "Offer an explicit return-to-IC path if management isn't the right fit — removing this stigma retains great engineers who tried management honestly.",
      "Provide structured EM training for all new managers, and explicitly offer return-to-IC without stigma within the first year."
    ],
    "sourceTopic": "Coaching & Talent Development",
    "mappingNotes": "Accidental manager → career pathway / IC-to-EM transition observables"
  },
  {
    "id": "AP-47",
    "name": "The Task-List OKR",
    "slug": "the-task-list-okr",
    "observableIds": [
      "C2-O1",
      "C2-O6"
    ],
    "capabilityId": "C2",
    "shortDesc": "Team's OKRs are formatted as task lists with binary checkboxes rather than measurable outcomes. Goal-setting becomes project management theater — checking boxes doesn't prove impact, and the team never learns whether their work actually moved the needle.",
    "warningSigns": [
      "Key results are all binary (ship feature X, complete project Y)",
      "OKRs read like a project plan, not strategy",
      "Team measures completion, not impact",
      "Objectives depend on each other sequentially",
      "8+ OKRs per team per quarter",
      "OKRs are set by leadership and handed to the team",
      "Nobody can articulate why these objectives matter to the business"
    ],
    "impact": "Team ships features without measuring impact; 'achieved 100% of OKRs' but business outcomes didn't move; OKRs become performative — teams game the system by setting easily achievable tasks; real strategic thinking replaced by task execution; team loses agency because goals were dictated, not co-created",
    "recoveryActions": [
      "Key results should answer 'how will we know we succeeded?' not 'what will we build?' Rewrite binary KRs as measurable outcomes: instead of 'Ship search feature' → 'Reduce time-to-find from 45s to 15s.' Limit to 2-4 objectives with 2-3 KRs each.",
      "Let the team co-create OKRs — when people own their goals, they're motivated by them.",
      "Make objectives independent — you should be able to achieve any objective without first achieving another.",
      "Schedule a 30-minute OKR check-in at the 6-week mark of every quarter. Ask: 'Are these still the right outcomes? Has context changed?' If yes, update the OKR with a changelog entry explaining why. Share changes with stakeholders within 48 hours.",
      "Separate objectives (qualitative, inspirational) from key results (quantitative, measurable); 0.6-0.7 average completion signals targets were appropriately ambitious."
    ],
    "sourceTopic": "Strategic Prioritization & Goals",
    "mappingNotes": "Task-list OKR → outcome measurement / goal-setting quality observables"
  },
  {
    "id": "AP-48",
    "name": "The Urgency Treadmill",
    "slug": "the-urgency-treadmill",
    "observableIds": [
      "C2-O1",
      "C2-O5"
    ],
    "capabilityId": "C2",
    "shortDesc": "Everything is urgent and nothing is strategic — the team constantly reacts to the loudest stakeholder or latest fire. Long-term initiatives never gain traction, engineers burn out from context-switching, and the team's impact stays flat despite working harder.",
    "warningSigns": [
      "Roadmap changes weekly based on whoever shouted last",
      "Team can't finish multi-week projects because priorities shift mid-sprint",
      "Engineers describe their work as 'always firefighting'",
      "Quarterly goals abandoned by month 2",
      "Leadership asks 'why isn't X done?' but keeps adding new urgent requests",
      "'we don't have time for technical investment' is the permanent state"
    ],
    "impact": "Strategic work never happens; tech debt compounds because investment is perpetually deferred; engineers lose motivation because nothing ships to completion; the team appears busy but produces little lasting value; best engineers leave for teams with clearer direction",
    "recoveryActions": [
      "Distinguish urgent from important using a simple framework: urgent (real deadline, real consequence) vs. important (high impact, no immediate deadline) vs. noise (someone wants it, no real consequence).",
      "Protect capacity: explicitly reserve 30-40% of team capacity for planned strategic work that cannot be interrupted.",
      "Force stakeholder prioritization: when a new 'urgent' request arrives, ask which current commitment should be deprioritized to make room — if nothing can be deprioritized, the new request isn't actually the priority.",
      "Make the cost visible: track how many planned commitments were completed vs. interrupted each quarter.",
      "Present this to leadership as 'we completed 3 of 8 quarterly goals because 5 were interrupted by ad-hoc requests.' The data makes the problem undeniable.",
      "Time-box work into fixed cycles (4-6 weeks) with no interruptions allowed mid-cycle — force stakeholders to wait for the next cycle.",
      "Only P0 incidents can break the sprint contract; make the cost of interruption visible through a formal exception process.",
      "Engineers need 4+ hour blocks of uninterrupted time for complex work — protect focus time structurally, not aspirationally."
    ],
    "sourceTopic": "Strategic Prioritization & Goals",
    "mappingNotes": "Urgency treadmill → prioritization discipline / stakeholder management observables"
  },
  {
    "id": "AP-49",
    "name": "The Wrecking Ball",
    "slug": "the-wrecking-ball",
    "observableIds": [
      "C1-O13",
      "C1-O4"
    ],
    "capabilityId": "C1",
    "shortDesc": "New leader makes drastic changes in their first weeks without understanding context, scrapping processes and reorganizing teams based on assumptions. Institutional knowledge is destroyed, trust collapses, and the very problems the changes aimed to fix get worse.",
    "warningSigns": [
      "Announces major changes before completing 1:1s with all directs",
      "Dismisses existing processes as 'broken' without understanding why they exist",
      "Cancels projects or changes priorities in first two weeks",
      "Uses phrases like 'at my last company we did it differently' as justification",
      "Team morale drops sharply within first month"
    ],
    "impact": "Within 1-2 quarters: trust destroyed before it's built; high performers who built the existing systems leave; changes create new problems because they didn't account for local context; team becomes resistant to all future changes.",
    "recoveryActions": [
      "Follow a structured 30-60-90 day plan: listen and assess (30), earn credibility through quick wins (60), initiate structural changes with buy-in (90).",
      "Produce a written team assessment by day 30 before proposing any structural changes.",
      "Identify and preserve what's working before changing what isn't.",
      "Earn credibility through 2-3 quick wins on pain points the team already identified."
    ],
    "sourceTopic": "Org-Level Thinking (New Leader Entry)",
    "mappingNotes": "Based on LeadDev 'Five management anti-patterns and why they happen' — The Wrecking Ball pattern."
  },
  {
    "id": "AP-50",
    "name": "Budget in a Vacuum",
    "slug": "budget-in-a-vacuum",
    "observableIds": [
      "C10-O1",
      "C10-O6"
    ],
    "capabilityId": "C10",
    "shortDesc": "Engineering leader plans budget in isolation without cross-functional input from product, finance, or peer engineering teams. Budget reviews become adversarial, asks get cut disproportionately, and the org loses credibility as a strategic partner.",
    "warningSigns": [
      "Budget proposal doesn't reference product roadmap or business goals",
      "Headcount requests say 'we need more engineers' without specifying deliverables",
      "Finance pushback is a surprise rather than an anticipated negotiation",
      "Product leader has a different number for the same initiative",
      "Cannot explain last year's budget variance"
    ],
    "impact": "Budget gets cut significantly because it lacks business justification. Engineering is perceived as a cost center rather than a strategic investment. Adversarial relationship with finance develops. Product team feels engineering isn't aligned. Cycle repeats annually with increasing frustration.",
    "recoveryActions": [
      "Schedule joint planning sessions with product counterpart before submitting budget.",
      "Link every headcount request to a specific deliverable with timeline impact.",
      "Build three-tier budget proposals (minimum, recommended, investment) with quantified trade-offs.",
      "Meet with finance to understand their constraints and evaluation criteria before the formal review.",
      "Pull last year's approved budget and actual spend side by side. Calculate variance per line item. In your next budget proposal, lead with: 'Last year we estimated X, spent Y, and here's why.' Finance teams trust managers who show they understand their own spending patterns.",
      "Co-create budget proposals with product and finance counterparts using a shared 'investment thesis' framework that ties every dollar to measurable business outcomes, preventing the isolation trap."
    ],
    "sourceTopic": "Resource Allocation & Tradeoffs (Budget Planning)",
    "mappingNotes": "Based on LeadDev articles on annual budget planning best practices."
  },
  {
    "id": "AP-51",
    "name": "The Stress Funnel",
    "slug": "the-stress-funnel",
    "observableIds": [
      "C4-O7",
      "C4-O13"
    ],
    "capabilityId": "C4",
    "shortDesc": "Manager passes organizational pressure, political conflicts, and exec anxiety directly to engineers instead of filtering it into actionable context. Team becomes anxious and distracted; productivity drops.",
    "warningSigns": [
      "Team knows about every executive disagreement and political conflict",
      "Manager shares raw Slack threads from leadership arguing about priorities",
      "Engineers feel anxious about company politics rather than focused on their work",
      "Every new request comes with 'the VP is really worried about this' framing",
      "Team morale tracks leadership drama rather than their own delivery"
    ],
    "impact": "Within 1 quarter: engineers anxious about problems they can't solve; focus time destroyed by perceived emergencies; best engineers leave because the environment feels chaotic; manager mistakes transparency for dumping.",
    "recoveryActions": [
      "Apply a filter before sharing: 'Does this information change what my team should do today?' If no, absorb it. If yes, translate into a specific action item — never forward raw anxiety.",
      "Translate executive concerns into specific, actionable asks before sharing with the team.",
      "Share decisions and relevant context, not the sausage-making process.",
      "Practice the 3-sentence rule: situation, what it means for us, what we're doing about it.",
      "Track interrupt count per sprint and team sentiment quarterly. Target fewer than two unplanned redirections per sprint."
    ],
    "sourceTopic": "Operational Leadership (Information Filtering)",
    "mappingNotes": "Based on LeadDev 'Five management anti-patterns' — The Funnel pattern. Manager acts as stress amplifier rather than filter."
  },
  {
    "id": "AP-52",
    "name": "Alert Noise Normalization",
    "slug": "alert-noise-normalization",
    "observableIds": [
      "C8-O3",
      "C8-O8"
    ],
    "capabilityId": "C8",
    "shortDesc": "Team normalizes high alert volume — hundreds of alerts per week, most ignored. Real signals drown in noise; when a genuine incident occurs, it's missed because alerts have lost credibility.",
    "warningSigns": [
      "On-call engineers routinely mute or snooze alerts without investigating",
      "New alert rules get added after incidents but old ones are never removed",
      "Page volume is high but most pages result in 'no action needed'",
      "On-call handoff includes tribal knowledge about which alerts to ignore",
      "Mean time to acknowledge real incidents is increasing quarter over quarter"
    ],
    "impact": "Within 1-2 quarters: genuine incidents missed because on-call ignores pages; MTTR increases as alert fatigue grows; on-call engineers develop habit of silencing alerts without investigating.",
    "recoveryActions": [
      "Run an alert audit: classify every alert from the last 30 days as actionable, noise, or duplicate.",
      "Delete or silence all non-actionable alerts immediately — target alert-to-action ratio of 1:2 or better.",
      "Set a monthly alert hygiene review cadence with the team.",
      "Track alert volume and false positive rate on the on-call dashboard; set threshold: <2 off-hours pages per night."
    ],
    "sourceTopic": "Incidents, Risk & Reliability (Alert Management)",
    "mappingNotes": "Based on LeadDev 'How to avoid alert fatigue' — most alerts accumulate because engineers are scared of removing them."
  },
  {
    "id": "AP-53",
    "name": "The Interview Marathon",
    "slug": "the-interview-marathon",
    "observableIds": [
      "C11-O1",
      "C11-O11"
    ],
    "capabilityId": "C11",
    "shortDesc": "Hiring process has grown to 7-8 rounds through risk aversion, with each bad hire triggering another interview stage rather than improving existing ones. Top candidates drop out mid-process, offer acceptance rate plummets, and the team selects for persistence over talent.",
    "warningSigns": [
      "Candidate drop-off rate exceeds 40% between offer and first interview",
      "Total interview process takes more than 3 weeks end-to-end",
      "Each round is 'just one more check' added after a bad hire",
      "Interviewers can't articulate what unique signal their round provides",
      "Competing companies close candidates before you make an offer"
    ],
    "impact": "Lose top candidates who have multiple offers and won't wait. Process favors candidates with time flexibility (employed at slow-moving companies) over those with high demand. Each additional round adds cost without adding signal. Hiring velocity drops, leaving teams understaffed longer. Candidate experience damages employer brand.",
    "recoveryActions": [
      "Map each interview round to a specific, non-overlapping signal it provides — eliminate rounds with redundant signal.",
      "Set a maximum of 4-5 total interactions (including screen) with a 2-week end-to-end target.",
      "Replace 'add another round' instinct with 'improve rubric quality in existing rounds'",
      "Track conversion rates at each stage and identify where drop-off exceeds industry benchmarks.",
      "Run a candidate experience survey and act on the feedback.",
      "Cap engineering interview processes at 4-5 total rounds with a 2-week end-to-end target — research shows prediction accuracy plateaus after 4 interviews; additional rounds add process cost without meaningfully improving signal quality."
    ],
    "sourceTopic": "Hiring, Onboarding & Role Design (Process Design)",
    "mappingNotes": "Based on LeadDev articles on rethinking engineer hiring strategy and 2025 hiring trends — bloated processes chase away talent."
  },
  {
    "id": "AP-54",
    "name": "The Order Taker",
    "slug": "the-order-taker",
    "observableIds": [
      "C5-O3",
      "C5-O1"
    ],
    "capabilityId": "C5",
    "shortDesc": "Engineering leader defers entirely to product for direction, never challenging priorities, proposing technical opportunities, or shaping strategy. Engineering becomes a feature factory with no technical voice, and the leader's credibility as a strategic partner erodes.",
    "warningSigns": [
      "EM never pushes back on feasibility, timeline, or approach during planning",
      "Engineers complain that 'product just throws things over the wall'",
      "Technical debt grows unchecked because no one advocates for engineering priorities",
      "Team has no input into what gets built, only how",
      "Engineering leader is absent from product discovery, customer research, and strategy discussions"
    ],
    "impact": "Engineering team loses ownership and motivation. Technical debt accumulates because no one advocates for platform investment. Product makes commitments without understanding technical constraints, leading to rushed delivery and quality issues. Best engineers leave for organizations where they have more influence. Engineering becomes a cost center in leadership's eyes rather than a strategic partner.",
    "recoveryActions": [
      "Start contributing to product discovery — attend customer research sessions, review analytics, propose solutions not just implementations.",
      "Reserve 15-20% of engineering capacity for technical investment and defend it in planning.",
      "Frame technical proposals in business language: 'reducing API latency by 200ms increases conversion by X%'",
      "Build a regular cadence of engineering-led product insights (data you see that product doesn't)",
      "Co-own the roadmap with your product counterpart rather than receiving a spec.",
      "Measure engineering on impact (metrics moved), not output (features shipped) — this structurally prevents the order-taker dynamic.",
      "Use a dual-track model where engineers participate in discovery alongside designers and PMs, embedding engineering in the 'what' as well as the 'how.'",
      "Give engineering leaders full ownership of both technical and product direction for their domain — the EM is expected to have a product opinion, not just execute someone else's."
    ],
    "sourceTopic": "Cross-Functional Influence (Product Partnership)",
    "mappingNotes": "Based on LeadDev 'Drive product gaps as an engineering leader' — engineering leaders can influence product effectively even without strong product management."
  },
  {
    "id": "AP-55",
    "name": "The Feedback Avoidance Loop",
    "slug": "the-feedback-avoidance-loop",
    "observableIds": [
      "C14-O4",
      "C14-O7"
    ],
    "capabilityId": "C14",
    "shortDesc": "Manager avoids giving negative performance feedback until it's too late, letting small issues compound for months without a signal to course-correct. The first honest feedback arrives as a PIP or low rating, destroying trust and making recovery nearly impossible because the engineer never had a chance to improve.",
    "warningSigns": [
      "1:1 notes show only positive feedback for an engineer whose performance is declining",
      "Manager uses phrases like 'they'll figure it out' or 'I don't want to micromanage'",
      "Performance review rating is a surprise to the engineer",
      "Manager confuses being nice with being kind — avoids discomfort at the cost of the person's growth",
      "High performers on the team are frustrated that low performance goes unaddressed"
    ],
    "impact": "The underperforming engineer loses months or years of potential growth because nobody told them the truth. High performers leave because they see uneven accountability. When feedback finally arrives, it feels punitive rather than developmental. Manager loses credibility with the team. Leadership debt compounds — the cost of delayed feedback is always higher than the cost of an uncomfortable conversation.",
    "recoveryActions": [
      "Schedule a direct conversation within 48 hours using SBI format (Situation-Behavior-Impact)",
      "Acknowledge the delay: 'I should have raised this sooner, and that's on me'",
      "Write documented feedback for every direct report at least monthly. Block 15 minutes per person on the last Friday of the month. Use the SBI format (Situation, Behavior, Impact) and share in the next 1:1.",
      "Build conflict literacy through practice — start with lower-stakes feedback and build up.",
      "Reframe feedback as a gift: withholding it is not kindness, it's neglect.",
      "Treat withholding feedback as a failure of leadership responsibility — deliver direct, actionable feedback within 48 hours of observing an issue, not accumulated for review season."
    ],
    "sourceTopic": "Performance Management & Calibration (Feedback Delivery)",
    "mappingNotes": "Based on LeadDev 'The cost of skipping hard conversations' — leadership debt from delayed feedback compounds faster than technical debt."
  },
  {
    "id": "AP-56",
    "name": "Incident Amnesia",
    "slug": "incident-amnesia",
    "observableIds": [
      "C8-O2",
      "C8-O7"
    ],
    "capabilityId": "C8",
    "shortDesc": "Post-mortems are written but never referenced; the same incidents recur because learnings aren't integrated into team practices. Post-mortem documents accumulate in a wiki nobody reads.",
    "warningSigns": [
      "Post-mortem action item completion rate is below 50%",
      "The same root cause appears in multiple post-mortems across quarters",
      "Engineers say 'we talked about this last time' during incident response",
      "Action items are assigned but never tracked or reviewed",
      "Post-mortems feel performative — people attend but don't expect change"
    ],
    "impact": "Within 2-3 quarters: repeat incidents from known root causes; team loses faith in post-mortem process; action items are completed but systemic patterns go unaddressed.",
    "recoveryActions": [
      "Run quarterly meta-review of post-mortem themes — identify the top 3 recurring patterns across incidents.",
      "Create an incident knowledge base linked to runbooks — every resolved incident should improve a runbook.",
      "Track repeat incident rate as the primary measure of post-mortem effectiveness.",
      "Reference previous post-mortems in new post-mortems when patterns overlap — make the connection explicit."
    ],
    "sourceTopic": "Incidents, Risk & Reliability (Learning from Incidents)",
    "mappingNotes": "Based on LeadDev 'How to turn an engineering incident into an opportunity' and post-mortem best practices."
  },
  {
    "id": "AP-57",
    "name": "Resume-Driven Architecture",
    "slug": "resume-driven-architecture",
    "observableIds": [
      "C3-O3",
      "C3-O1"
    ],
    "capabilityId": "C3",
    "shortDesc": "Technology choices driven by resume appeal rather than problem fit. Team adopts complex tools (Kubernetes for 3 services, microservices for 5 engineers) adding operational complexity that dwarfs engineering value.",
    "warningSigns": [
      "Architecture designed for 10x scale the product has never reached",
      "Nobody can articulate what business problem the chosen technology solves better than simpler alternatives",
      "Engineering proposals lead with technology ('let's use Kafka') rather than problem ('we need reliable async processing')",
      "New hire onboarding exceeds 3 months due to stack complexity",
      "Operational on-call burden exceeds 20% of team capacity"
    ],
    "impact": "Within 1-2 quarters: operational complexity consumes >30% of team capacity; onboarding time doubles; deployment frequency drops as infrastructure fighting displaces feature work.",
    "recoveryActions": [
      "Require every architecture proposal to start with 'What problem does this solve?' and 'What's the simplest solution?'",
      "Write a 1-page TCO analysis for every technology decision: license cost, operational burden (on-call hours), hiring pool size, and cognitive load.",
      "Adopt a 'boring technology' default: proven tools unless there's a quantified reason the established option fails.",
      "Create a team tech radar with explicit adopt/trial/assess/hold categories."
    ],
    "sourceTopic": "Systems Design & Architecture (Technology Selection)",
    "mappingNotes": "Based on resume-driven development patterns widely discussed in engineering leadership. The incentive structure is broken when choosing complexity is rewarded more than choosing simplicity."
  },
  {
    "id": "AP-58",
    "name": "The Ostrich Leader",
    "slug": "the-ostrich-leader",
    "observableIds": [
      "C1-O4",
      "C1-O7"
    ],
    "capabilityId": "C1",
    "shortDesc": "Leader avoids making difficult organizational decisions — unclear ownership, toxic team members, misaligned team boundaries — hoping problems resolve themselves. They never do; avoidance concentrates dysfunction and raises the cost of intervention.",
    "warningSigns": [
      "Known team dysfunction persists for months without intervention",
      "Leader deflects organizational problems with 'let's give it more time'",
      "Skip-level conversations reveal issues the leader has been told about but hasn't addressed",
      "Team members route around known problems rather than escalating, because escalating hasn't worked"
    ],
    "impact": "Within 2-3 quarters: 'missing stair' pattern becomes entrenched; high performers leave because they don't trust leadership to act; organizational dysfunction compounds as each avoided decision creates new ones.",
    "recoveryActions": [
      "Audit: list all known organizational issues open >60 days without a plan.",
      "Commit to a decision timeline for each issue — even if the decision is 'we're not changing this, here's why.'",
      "Use skip-level conversations to identify issues the leader may be avoiding.",
      "Track 'decision latency' as a personal leadership metric: time from issue identification to resolution."
    ],
    "sourceTopic": "Org-Level Thinking (Organizational Decision Making)",
    "mappingNotes": "Complementary to The Wrecking Ball (AP-54) — where that anti-pattern is about acting too fast, this one is about acting too slow."
  },
  {
    "id": "AP-59",
    "name": "The Goodhart Trap",
    "slug": "the-goodhart-trap",
    "observableIds": [
      "C9-O3",
      "C9-O9"
    ],
    "capabilityId": "C9",
    "shortDesc": "A metric becomes a target, then gets gamed. Deployment frequency is optimized by splitting PRs into tiny changes; cycle time is optimized by skipping reviews. The metric improves but actual outcomes don't.",
    "warningSigns": [
      "Story points per sprint increase but customer satisfaction or feature adoption doesn't improve",
      "Team debates how to 'count' work to maximize the metric rather than how to deliver value",
      "Metrics dashboard shows green while stakeholders report slow progress",
      "Engineers game metrics by splitting work into smaller tickets or inflating estimates",
      "No counter-metrics exist to catch single-metric optimization"
    ],
    "impact": "Within 1-2 quarters: metric looks great but real outcomes deteriorate; team loses trust in measurement; leadership makes bad decisions based on gamed numbers; gaming becomes cultural norm.",
    "recoveryActions": [
      "Always pair metrics: deployment frequency with change failure rate; cycle time with escaped defect rate; velocity with customer satisfaction.",
      "Look for metric-outcome divergence: if the metric improves but the experience doesn't, the metric is being gamed.",
      "Make gaming visible by tracking paired metrics on the same dashboard — divergence is the signal.",
      "Rotate metrics quarterly to prevent optimization lock-in."
    ],
    "sourceTopic": "Metrics, Measurement & Outcomes (Metric Dysfunction)",
    "mappingNotes": "Based on Goodhart's Law and LeadDev articles on metrics gaming and the 'flawed five' engineering productivity metrics."
  },
  {
    "id": "AP-60",
    "name": "The Talent Hostage",
    "slug": "the-talent-hostage",
    "observableIds": [
      "C11-O6",
      "C11-O10"
    ],
    "capabilityId": "C11",
    "shortDesc": "Manager hoards talent by blocking internal transfers, withholding growth opportunities, or creating dependency. The team becomes a place people can't leave rather than a place they want to stay, and the best engineers leave the company entirely.",
    "warningSigns": [
      "Internal transfer requests are discouraged or slow-walked",
      "Manager gives vague growth plans but blocks concrete opportunities (leading projects on other teams, rotations)",
      "High performers feel stuck but the manager frames it as 'the team needs you'",
      "Attrition is low but engagement surveys show declining satisfaction",
      "Manager counter-offers or guilt-trips engineers who express interest in other roles"
    ],
    "impact": "Engineers eventually leave the company entirely instead of transferring internally. Team builds resentment rather than loyalty. Manager's reputation as a talent hoarder spreads, making recruiting harder. The company loses people it could have retained in a different role. Growth-oriented engineers avoid joining the team.",
    "recoveryActions": [
      "Reframe internal transfers as a positive signal — you're developing people other teams want.",
      "Build a succession plan so no single departure creates a crisis.",
      "Actively sponsor engineer growth even when it means losing them: 'I'd rather you grow here than leave the company'",
      "Track internal mobility as a positive metric alongside external attrition.",
      "Create rotation opportunities proactively — 3-month cross-team projects build skills and expand networks.",
      "Track internal mobility as a positive signal of engineering culture health — managers who develop engineers that other teams want should be rated higher, not punished for 'losing' talent."
    ],
    "sourceTopic": "Hiring, Onboarding & Role Design (Talent Mobility)",
    "mappingNotes": "Based on LeadDev retention articles — talent hoarding creates the opposite of the intended effect. The best retention strategy is making people want to stay, not making it hard to leave."
  },
  {
    "id": "AP-61",
    "name": "The Ivory Tower Strategy",
    "slug": "the-ivory-tower-strategy",
    "observableIds": [
      "C2-O1",
      "C2-O6"
    ],
    "capabilityId": "C2",
    "shortDesc": "Leadership creates a strategic vision in documents and presentations but never translates it into actionable work for teams. Engineers cannot connect their daily work to the strategy, the gap between intent and execution widens quarterly, and OKRs drift into busywork.",
    "warningSigns": [
      "Engineers cannot articulate how their current project connects to the company's top 3 priorities",
      "Strategy documents use abstract language ('leverage synergies,' 'drive innovation') that doesn't map to concrete engineering decisions",
      "Teams discover conflicting priorities because the strategy was too high-level to resolve real trade-offs",
      "Quarterly planning feels disconnected from the annual strategy — teams plan bottom-up with no top-down constraint",
      "Leadership presents the same strategic slide deck for 3+ quarters while the market and team have moved on"
    ],
    "impact": "Engineering effort scatters across locally-optimal projects that don't add up to strategic progress. Teams make reasonable decisions in isolation that are globally incoherent. The strategy becomes a source of cynicism rather than alignment — people stop reading strategy documents because they've never influenced daily work. Leadership blames execution when the real failure is translation: turning strategic intent into engineering priorities with clear success criteria.",
    "recoveryActions": [
      "Translate every strategic pillar into 2-3 specific engineering bets with measurable outcomes and time horizons.",
      "Run a 'strategy connection' exercise: ask each team lead to map their top 3 projects to strategic priorities — gaps reveal translation failures.",
      "Replace abstract strategy language with concrete decision criteria: 'When we face a trade-off between X and Y, we choose X because...'",
      "Schedule a 2-hour strategy refresh every quarter: bring updated data (OKR progress, market changes, customer feedback), identify 1-2 assumptions that no longer hold, and publish an updated strategy one-pager within one week. Share the diff with the team.",
      "Make the strategy visible in sprint planning and roadmap reviews, not just annual kickoffs.",
      "Start from a specific customer problem and work backward to a solution — every strategic initiative should be explainable to a customer, not just a board."
    ],
    "sourceTopic": "Strategy & Vision (Strategy-Execution Translation)",
    "mappingNotes": "Based on LeadDev 'When the movie isn't like the book: Failure modes in strategic alignment' — strategy fails not because it's wrong but because it's never translated into terms teams can act on."
  },
  {
    "id": "AP-62",
    "name": "The Toil Spiral",
    "slug": "the-toil-spiral",
    "observableIds": [
      "C4-O4",
      "C4-O3"
    ],
    "capabilityId": "C4",
    "shortDesc": "Team neglects operational health — toil reduction, developer experience, tech debt — because feature delivery always wins. Accumulated neglect eventually degrades velocity so badly that the team can't ship features either.",
    "warningSigns": [
      "Engineers spend more than 30% of their time on repetitive manual tasks that could be automated",
      "The team has had 'fix the build pipeline' or 'reduce deploy time' on the backlog for 3+ quarters without progress",
      "Feature velocity is declining quarter over quarter despite stable headcount",
      "New engineers say 'this is painful' about basic workflows that the team has normalized",
      "Every sprint planning starts with 'we'll do tech debt next sprint' and it never happens"
    ],
    "impact": "Within 2-3 quarters: toil compounds into a death spiral where declining velocity creates pressure for more shortcuts, which generate more toil. Developer experience degrades until senior engineers leave. The remaining team lacks capacity to dig out.",
    "recoveryActions": [
      "Reserve 20-30% of every sprint capacity for operational health, non-negotiably.",
      "Track toil explicitly — measure hours spent on repetitive manual tasks weekly and set reduction targets.",
      "Start with the highest-leverage automation: the one manual task every engineer does multiple times per week.",
      "Make DX metrics visible to leadership — deploy frequency, build time, onboarding time-to-first-commit.",
      "Frame operational investment in business terms: 'Reducing deploy time from 45 min to 5 min recovers X engineer-hours per week.'"
    ],
    "sourceTopic": "Operational & Process Excellence (Operational Health)",
    "mappingNotes": "Based on LeadDev 'What is toil and why is it damaging your engineering org?' and 'How to break the cycle of tech debt' — toil creates a self-reinforcing trap where the team lacks capacity to fix the thing that's consuming their capacity."
  },
  {
    "id": "AP-63",
    "name": "The Stakeholder Surprise",
    "slug": "the-stakeholder-surprise",
    "observableIds": [
      "C5-O2",
      "C5-O6"
    ],
    "capabilityId": "C5",
    "shortDesc": "Engineering leader avoids surfacing technical risks, timeline slips, and scope changes until the last possible moment. Stakeholders are repeatedly blindsided, trust erodes rapidly, and leadership starts micromanaging or routing around engineering entirely.",
    "warningSigns": [
      "Stakeholders learn about project delays in the same meeting where the deadline was supposed to be met",
      "Engineering leader uses optimistic framing ('we're working on it') when the project is materially off track",
      "Product or business leaders say they 'don't trust engineering timelines' based on past surprises",
      "Status reports consistently show green/on-track until suddenly flipping to red with no amber warning period",
      "The engineering leader avoids 1:1s with their VP or product counterpart during difficult periods"
    ],
    "impact": "Each surprise erodes stakeholder trust compoundingly — the first late project is forgiven, the third triggers micromanagement. Business leaders lose confidence in engineering's ability to self-manage and start demanding detailed progress reports, daily standups with leadership, or scope commitments with penalty clauses. The engineering leader's credibility and autonomy shrink with each surprise. Eventually, product and business make decisions without engineering input because they've learned that engineering's input is unreliable.",
    "recoveryActions": [
      "Adopt a 'no surprises' principle: any risk with >30% likelihood of impacting timeline gets communicated within 48 hours of identification.",
      "Practice the formula: 'Here's what happened, here's the impact, here's what we're doing about it, here's when I'll update you next'",
      "Build in regular risk reviews with stakeholders — weekly 15-minute syncs where you proactively share what's on track and what's not.",
      "Reframe early bad news as a trust-building opportunity: leaders who surface problems early are seen as reliable, not as failures.",
      "Track your prediction accuracy — compare initial estimates to actuals and use the data to calibrate future commitments.",
      "Run weekly business reviews that force surfacing deviations from plan within the same week they occur — the meeting structure makes it structurally impossible to hide problems for more than 7 days."
    ],
    "sourceTopic": "Stakeholder & Product (Stakeholder Communication)",
    "mappingNotes": "Based on LeadDev 'Getting good at delivering bad news' — the cost of late bad news is always higher than the cost of early bad news. Trust is built by reliability of information, not by optimism."
  },
  {
    "id": "AP-64",
    "name": "The 100% Utilization Fallacy",
    "slug": "the-100-percent-utilization-fallacy",
    "observableIds": [
      "C10-O8",
      "C10-O1"
    ],
    "capabilityId": "C10",
    "shortDesc": "Manager allocates 100% of engineering capacity to planned work, leaving zero buffer for unplanned work, interrupts, or learning. Projects chronically miss deadlines because the plan assumed ideal conditions that never exist, and the team has no capacity to absorb reality.",
    "warningSigns": [
      "Sprint commitments assume every engineer is available for 8 productive hours per day, 5 days per week",
      "Any unplanned work (production incident, urgent bug, exec request) immediately puts the sprint at risk",
      "Engineers report having no time for code review, mentoring, documentation, or learning",
      "Project timelines are built on best-case estimates with no contingency buffer",
      "The team consistently delivers 60-70% of sprint commitments but the plan is never adjusted"
    ],
    "impact": "When capacity is planned at 100%, every surprise becomes a crisis. Teams develop a reputation for missing deadlines even though they're working harder than ever. Engineers burn out because there's no slack for the human aspects of work — thinking, learning, helping colleagues. Quality declines because code review and testing are squeezed out by delivery pressure. The paradox: teams running at 70-80% planned utilization actually deliver more reliably than teams planned at 100%, because they can absorb variability without cascading failures.",
    "recoveryActions": [
      "Plan capacity at 70-80% — reserve 20-30% for unplanned work, collaboration, and investment.",
      "Track actual vs. planned capacity over 4-6 sprints to establish your team's real available capacity.",
      "Make the buffer explicit and visible: 'We plan 32 of 40 hours per engineer per week' — this isn't laziness, it's engineering for reliability.",
      "When leadership asks why the team isn't 'fully utilized,' explain with data: show the correlation between planned utilization rate and delivery predictability.",
      "Categorize unplanned work (incidents, support, urgent requests) to identify patterns that can be planned for.",
      "Build 20% slack into engineering capacity — not as unstructured free time but as recognition that 100% planned utilization is a fiction, and that innovation and unplanned work require real capacity."
    ],
    "sourceTopic": "Resource Allocation (Capacity Planning)",
    "mappingNotes": "Based on LeadDev 'How to reserve engineering capacity and deliver projects on time' — teams at 70-80% planned utilization outperform those at 100% because they can absorb the variability that always exists in knowledge work."
  },
  {
    "id": "AP-65",
    "name": "The Reassurance Trap",
    "slug": "the-reassurance-trap",
    "observableIds": [
      "C12-O1",
      "C12-O7"
    ],
    "capabilityId": "C12",
    "shortDesc": "During periods of uncertainty — layoffs, reorgs, strategy pivots — the manager offers vague reassurances that ring hollow. Trust is destroyed faster than honest acknowledgment of difficulty would, and the team disengages or begins job-searching in silence.",
    "warningSigns": [
      "Manager says 'don't worry about it' when the team raises concerns about organizational changes",
      "Reassurances prove wrong repeatedly — 'no more layoffs' is followed by another round within months",
      "Team members stop asking questions in all-hands because they've learned the answers won't be honest",
      "Manager avoids the topic entirely, hoping uncertainty will resolve itself without acknowledgment",
      "Private Slack channels and hallway conversations replace official channels as the trusted information source"
    ],
    "impact": "Each hollow reassurance depletes the manager's credibility account. The team develops a discount rate for everything the manager says — they assume optimistic framing means things are worse than stated. Psychological safety collapses because the team learns that raising concerns gets dismissal rather than honesty. Top performers, who have the most options, leave first because they trust their own assessment more than management's reassurances. The remaining team is anxious, disengaged, and focused on self-preservation rather than team goals.",
    "recoveryActions": [
      "Replace reassurance with honesty: 'Here's what I know, here's what I don't know, and here's when I expect to know more'",
      "Acknowledge what you cannot promise: 'I can't guarantee there won't be more changes, but I can tell you what I'm doing to advocate for this team'",
      "Create a regular cadence for uncertainty updates — even if the update is 'no new information,' the consistency builds trust.",
      "Name the emotions: 'I know this is unsettling. It's okay to feel that way. Let's talk about what we can control'",
      "After the uncertainty resolves, do a retrospective on how communication went — learn from what helped and what didn't.",
      "Earning trust includes transparency during uncertainty as a specific behavioral indicator — share what you know honestly, acknowledge what you don't know, and never offer false comfort."
    ],
    "sourceTopic": "Team Culture & Belonging (Communication During Uncertainty)",
    "mappingNotes": "Based on LeadDev 'Build psychological safety in a world of layoffs' — vague optimism during uncertainty is more damaging than honest difficulty because it signals that the leader either doesn't know the truth or doesn't respect the team enough to share it."
  },
  {
    "id": "AP-66",
    "name": "Vulnerability Noise Blindness",
    "slug": "vulnerability-noise-blindness",
    "observableIds": [
      "C13-O2",
      "C13-O6"
    ],
    "capabilityId": "C13",
    "shortDesc": "Automated security scanners generate hundreds of findings per week without severity-based triage or noise reduction. Volume desensitizes the team: engineers stop reading scan results, critical vulnerabilities get lost among low-severity noise, and the backlog grows until a real breach exposes a finding that was flagged months ago but never triaged.",
    "warningSigns": [
      "Security scanner dashboard shows 500+ open findings and the number grows every week",
      "Engineers dismiss scanner alerts as 'mostly false positives' without verifying",
      "Critical vulnerabilities sit unpatched for weeks because they're buried in a backlog of low-severity findings",
      "The team disabled or ignored security scanning in CI because it blocked too many deploys for non-issues",
      "Nobody owns vulnerability triage — findings go into a shared queue that nobody monitors"
    ],
    "impact": "The security scanning investment delivers negative value — it generates noise that obscures real risk rather than reducing it. Engineers develop contempt for security tooling because it cries wolf constantly. When a genuine critical vulnerability appears, it gets the same treatment as the hundreds of low-severity findings: acknowledged, backlogged, forgotten. The organization has the appearance of security diligence (tools are running, dashboards exist) while actual security posture degrades.",
    "recoveryActions": [
      "Assign a single owner for vulnerability triage — one person or rotating role who reviews every finding weekly.",
      "Publish severity-based SLAs visible to the whole team: critical = patch within 48 hours, high = 7 days, medium = 30 days, low = next quarter. Track SLA compliance weekly. Escalate any breach to the EM within 24 hours of the deadline.",
      "Aggressively tune scanners: suppress known false positives, configure severity thresholds, and accept risk on low-impact findings formally.",
      "Track signal-to-noise ratio: what percentage of scanner findings led to actual fixes vs. dismissals — if >80% are dismissed, the tool needs reconfiguration.",
      "Separate the vulnerability backlog from the feature backlog — make security work visible with its own tracking and reporting.",
      "Use severity-based SLOs with escalating response times (critical: 48 hours, high: 7 days) and automated tracking — triaging noise is as important as fixing the real issues.",
      "See Playbook: 'Responding to a Critical Dependency Vulnerability' (P-C13-5) for how to triage and prioritize vulnerability response when the backlog is overwhelming."
    ],
    "sourceTopic": "Security & Compliance (Vulnerability Management)",
    "mappingNotes": "Based on LeadDev articles on security-engineering relationship dynamics and vulnerability management — the scanner is only as valuable as the triage process behind it. Without effective prioritization, more scanning creates more noise, not more security."
  },
  {
    "id": "AP-67",
    "name": "The Shadow Bypass",
    "slug": "the-shadow-bypass",
    "observableIds": [
      "C13-O1",
      "C13-O6"
    ],
    "capabilityId": "C13",
    "shortDesc": "Engineers routinely circumvent security controls — disabling scanners, skipping threat modeling, using personal credentials — because security is perceived as friction. Each bypass normalizes the next, widening the gap between documented security posture and actual practice.",
    "warningSigns": [
      "Engineers disable or skip security scanning steps in CI pipelines to unblock deploys",
      "Threat modeling is marked 'N/A' for features that clearly handle sensitive data because the team is behind schedule",
      "Developers use personal accounts, hardcoded tokens, or shared credentials for testing and staging",
      "Security exceptions become the norm — more features ship with exceptions than without",
      "The security team discovers bypassed controls only during quarterly audits or after incidents"
    ],
    "impact": "The organization pays the cost of security processes (tooling, policy documentation, team time) without getting the benefit. Real vulnerabilities ship to production through the gaps created by bypasses. When an incident occurs, investigation reveals that existing controls would have caught it if they hadn't been circumvented. The security team loses credibility because their processes clearly aren't working, even though the root cause is organizational, not technical. Engineers develop a habit of treating security as optional, which becomes very hard to reverse.",
    "recoveryActions": [
      "Audit current bypass patterns — survey engineers anonymously about which security steps they routinely skip and why.",
      "Fix the top 3 friction points: if scanning takes 20 minutes, invest in making it take 2 minutes rather than making it mandatory-but-slow.",
      "Make security controls non-bypassable for high-risk paths (production deploys, auth changes, data access) while allowing more flexibility for low-risk changes.",
      "Reframe security as a shared engineering quality standard, not an external gate — include security in definition of done alongside tests and code review.",
      "Track bypass rate as a metric: percentage of deploys that skip any security step — trending down means the process is becoming less painful.",
      "Design controls that can't be bypassed rather than relying on compliance — production deploy pipelines should enforce scanning automatically, making it harder to skip security than to do it.",
      "See Playbook: 'Balancing Security Risk With Shipping Speed After a Close Call' (P-C13-4) for how to design security controls that engineers actually follow rather than bypass."
    ],
    "sourceTopic": "Security & Compliance (Security Process Adoption)",
    "mappingNotes": "Based on LeadDev 'Overcoming security hurdles to push engineering velocity' — engineers bypass security not because they don't care, but because the controls are designed for compliance rather than developer experience."
  },
  {
    "id": "AP-68",
    "name": "The Certification Facade",
    "slug": "the-certification-facade",
    "observableIds": [
      "C13-O4",
      "C13-O2"
    ],
    "capabilityId": "C13",
    "shortDesc": "Team maintains compliance certifications and passes audits, but certified practices are only followed during audit preparation. Day-to-day security posture drifts far from documented standards, creating a dangerous gap between paper compliance and actual risk.",
    "warningSigns": [
      "Audit preparation is a 2-4 week scramble to gather evidence and update documentation that's been stale since the last audit",
      "Engineers can't describe the security practices the company is certified for without looking at the compliance docs",
      "Access reviews happen the week before the audit, not continuously — stale permissions accumulate for months",
      "Compliance evidence is generated retroactively rather than as a byproduct of actual security practices",
      "The certification badge is prominently displayed on the sales page but the security team knows the daily reality doesn't match"
    ],
    "impact": "The certification creates a false sense of security for customers, leadership, and the team itself. Real risk goes unmanaged because the compliance program measures documentation, not actual practice. When a breach occurs, investigation reveals that the certified controls weren't being followed, creating legal liability and destroying customer trust far more severely than if the company had never been certified. The compliance program consumes significant resources (audit fees, preparation time, documentation) without reducing actual risk.",
    "recoveryActions": [
      "Automate compliance evidence collection — access logs, change records, vulnerability scan results should be generated continuously, not prepared for audits.",
      "Map each certification control to an actual engineering practice. For controls that require manual evidence, automate the collection. For controls that don't match real workflow, escalate to security leadership with a proposal: 'We can sustain control X if we change it to Y.'",
      "Run internal mini-audits monthly — spot-check 3-5 controls to ensure continuous compliance rather than periodic cramming.",
      "Make compliance metrics visible to engineering: access review completion rate, vulnerability SLA adherence, training completion — treat these like engineering SLOs.",
      "Build compliance into engineering workflows so evidence is a byproduct of work, not a separate artifact: automated access management, built-in audit trails, continuous scanning.",
      "Embed security controls into every engineering workflow — threat models are part of design docs, security scanning runs in every CI pipeline, and access reviews happen continuously, not just during audit preparation windows.",
      "See Playbook: 'Your Team Keeps Failing Security Audits With the Same Findings' (P-C13-3) for how to move from audit-time compliance theater to continuous security practices."
    ],
    "sourceTopic": "Security & Compliance (Compliance Integrity)",
    "mappingNotes": "Based on LeadDev articles on compliance-heavy industry testing and born-left security — certifications should reflect reality, not create an alternative reality that only exists during audit windows."
  },
  {
    "id": "AP-69",
    "name": "The Big-Bang Deploy",
    "slug": "the-big-bang-deploy",
    "observableIds": [
      "C4-O5",
      "C4-O10"
    ],
    "capabilityId": "C4",
    "shortDesc": "Team batches weeks of changes into monolithic releases, turning every deploy into a high-risk event. Deploy frequency drops because nobody wants to trigger the next crisis.",
    "warningSigns": [
      "Deploys happen weekly or less frequently, always as large batches",
      "Release day requires a war room or dedicated 'release captain'",
      "Rollbacks are manual, untested, or nonexistent",
      "Engineers avoid deploying on Fridays (or any day near a deadline)",
      "Every deploy requires a change advisory board meeting",
      "Post-deploy incidents are considered normal, not exceptional",
      "Feature branches live for weeks before merging"
    ],
    "impact": "Within 1-2 quarters: each deploy carries accumulated risk making root cause analysis impossible when things break; engineers develop deployment anxiety; feedback loops stretch from hours to weeks; culture of risk aversion develops.",
    "recoveryActions": [
      "Measure current deploy frequency and batch size — make the problem visible with data.",
      "Invest in automated rollback capability before increasing deploy frequency.",
      "Ship every new feature behind a feature flag by default. Set a 90-day flag lifecycle.",
      "Move to trunk-based development with short-lived branches (< 1 day).",
      "Target multiple deploys per day; roll out through canary pipeline (1%→10%→50%→100%) with automatic rollback triggers.",
      "Replace change advisory boards with automated change risk scoring for routine changes."
    ],
    "sourceTopic": "Operational Leadership (Release Engineering)",
    "mappingNotes": "Big-bang deploys → release process / change management observables. Based on DORA research showing elite teams deploy multiple times per day with low change failure rates."
  },
  {
    "id": "AP-70",
    "name": "The Vanity OKR",
    "slug": "the-vanity-okr",
    "observableIds": [
      "C9-O4",
      "C9-O5"
    ],
    "capabilityId": "C9",
    "shortDesc": "OKRs are aspirational statements with no measurable key results. 'Improve developer experience' without defining what improvement looks like or how to measure it. Team can't tell if they succeeded.",
    "warningSigns": [
      "Key results use words like 'improve,' 'enhance,' or 'optimize' without quantified targets",
      "Every team hits 100% of OKRs every quarter — targets are sandbagged",
      "OKRs are set in January and never referenced until the next planning cycle",
      "Key results measure activity (ship X features) rather than outcomes (reduce churn by Y%)",
      "Team can't explain how their daily work connects to their stated OKRs",
      "OKRs are copy-pasted from the previous quarter with minor wording changes",
      "Quarterly OKR review is a formality where everyone presents green slides"
    ],
    "impact": "Within 1 quarter: OKRs don't influence daily work; prioritization decisions are disconnected from stated objectives; quarterly reviews are opinion-based because there's no measurement.",
    "recoveryActions": [
      "Rewrite every key result to include a number: 'Reduce CI build time from 45min to 10min' not 'Improve CI performance.'",
      "Limit to 3 objectives with 3 key results each — if you can't prioritize, you have too many.",
      "Score OKRs quarterly with brutal honesty: 0.6-0.8 is healthy; 1.0 means you didn't stretch; below 0.4 means the OKR was wrong.",
      "Test each OKR: 'Would we make different daily decisions if this OKR didn't exist?' If no, rewrite it."
    ],
    "sourceTopic": "Metrics, Measurement & Outcomes (Goal Setting)",
    "mappingNotes": "Vanity OKRs → outcome-oriented OKR / business translation observables. Based on common OKR dysfunction patterns in engineering organizations."
  }
]