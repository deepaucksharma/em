[
  {
    "capabilityId": "C1",
    "leadingIndicators": [
      "Org topology registry maintained and up-to-date",
      "Cross-team handoff frequency tracked as % of total work",
      "Team cognitive load assessed via quarterly health survey (services owned, domains, on-call scope)",
      "Ownership ambiguity incidents logged and tracked",
      "Strategic alignment visible in team-level OKRs and documented connections",
      "Re-org signals monitored via coordination overhead and team health metrics"
    ],
    "laggingIndicators": [
      "Org structure changes are proactive (signals detected early), not reactive (after productivity impacts)",
      "Cross-team coordination overhead <5% of total sprint work",
      "Re-orgs execute with zero regrettable attrition (measured post-re-org)",
      "Team productivity restored within 3-4 weeks of structural change",
      "Ownership disputes resolved in <24 hours on average",
      "Deployment frequency stable or improving after structural changes"
    ],
    "measurementAntiPatterns": [
      "Using team size as a proxy for leadership scope",
      "Measuring reorg count instead of reorg outcomes",
      "Evaluating org design in isolation from delivery metrics"
    ],
    "suggestedCadence": "quarterly",
    "dataSourceExamples": [
      "HRIS attrition data",
      "Engagement survey results",
      "Delivery tracking tools",
      "Org chart and role mapping tools"
    ]
  },
  {
    "capabilityId": "C2",
    "leadingIndicators": [
      "Quarterly roadmap published with business outcome mapping",
      "Prioritization scorecard applied consistently to all incoming requests",
      "Monthly re-prioritization gate held with documented decisions",
      "Tech debt allocation visible in quarterly planning (15-20% capacity)",
      "Pre-execution design review completed for all planned initiatives >20% quarter capacity",
      "Roadmap communicated to all stakeholders with clear rationale"
    ],
    "laggingIndicators": [
      "Roadmap delivery rate 80%+ for 3+ consecutive quarters",
      "Planning forecast accuracy improving quarter-over-quarter",
      "Unplanned priority shifts <1 per sprint",
      "Stakeholder satisfaction with planning process >4.0/5.0",
      "Business outcome statements included in 95%+ of planned initiatives",
      "Tech debt inventory maintained and prioritized quarterly"
    ],
    "measurementAntiPatterns": [
      "Equating busyness with strategic alignment",
      "Measuring number of features shipped without tying to business outcomes",
      "Allowing loudest stakeholder to override prioritization framework"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "Roadmap and planning tools (Jira, Linear, Shortcut)",
      "OKR tracking platforms",
      "Time allocation reports",
      "Stakeholder feedback surveys"
    ]
  },
  {
    "capabilityId": "C3",
    "leadingIndicators": [
      "Architecture Decision Records (ADRs) created for all major changes",
      "Tech debt backlog is triaged and prioritized quarterly",
      "System reliability targets (SLOs) are defined for all critical services",
      "Architecture review board meets regularly with clear decision log",
      "Design doc completion rate and review turnaround time tracked",
      "Tech debt registry maintained with business impact estimates per item"
    ],
    "laggingIndicators": [
      "System uptime meets defined SLO targets (e.g., 99.9%)",
      "Median time to integrate a new service decreased over 6 months",
      "Tech debt ratio (debt work / total work) remains within target band",
      "No architecture-related production incidents recurring after fix",
      "Design doc adoption rate and architecture decision reversal frequency tracked as quality signals",
      "Tech debt ratio stable or improving quarter-over-quarter"
    ],
    "measurementAntiPatterns": [
      "Measuring architecture quality by document volume rather than decision outcomes",
      "Using code coverage as a sole proxy for system quality",
      "Ignoring operational complexity when evaluating architecture decisions"
    ],
    "suggestedCadence": "quarterly",
    "dataSourceExamples": [
      "ADR repositories and wikis",
      "Monitoring and observability platforms (Datadog, Grafana)",
      "Incident tracking systems",
      "Code complexity and dependency analysis tools"
    ]
  },
  {
    "capabilityId": "C4",
    "leadingIndicators": [
      "Sprint planning and retrospectives happen on schedule every cycle",
      "Blockers are identified and escalated within 24 hours",
      "Team working agreements are documented and reviewed quarterly",
      "Delivery forecasts are shared with stakeholders before each cycle",
      "Squad health check scores tracked as leading indicator of operational rhythm",
      "Engineers report 4+ hours of daily uninterrupted focus time",
      "Every team ceremony has documented purpose and measured value delivery"
    ],
    "laggingIndicators": [
      "Sprint commitment accuracy is 85%+ consistently",
      "Cycle time for standard work items is stable or improving quarter-over-quarter",
      "Meeting effectiveness scores above 3.5/5 in team surveys",
      "Zero missed external deadlines in the quarter",
      "Mechanism completion rate — percentage of identified process gaps with automated solutions",
      "Retro action item completion rate above 80%"
    ],
    "measurementAntiPatterns": [
      "Optimizing for velocity without measuring outcome quality",
      "Treating all process deviations as failures rather than learning signals",
      "Measuring hours worked instead of output and flow efficiency"
    ],
    "suggestedCadence": "weekly",
    "dataSourceExamples": [
      "Project management tools (Jira, Linear, Asana)",
      "DORA metrics dashboards",
      "Team health check surveys",
      "Standup and retro notes"
    ]
  },
  {
    "capabilityId": "C5",
    "leadingIndicators": [
      "Regular triad syncs (weekly or bi-weekly) with documented agendas and decisions",
      "Shared constraints and capacity planning documents reviewed monthly",
      "Early engineering involvement in planning cycles (pre-commitment validation)",
      "Cross-functional satisfaction survey scores for collaboration quality",
      "Decision authority matrix documented and referenced in dispute resolution",
      "Peer 1:1s with PM and Design leads scheduled regularly and consistent"
    ],
    "laggingIndicators": [
      "Cross-functional escalations (fewer than 1 per quarter)",
      "Scope change requests mid-sprint (trend toward zero)",
      "Peer feedback from Product/Design on engineering collaboration credibility",
      "Commitment accuracy (scope delivered vs. scope committed) stays above 85%",
      "Cross-functional satisfaction survey trending upward",
      "EM invited to strategic planning conversations based on merit"
    ],
    "measurementAntiPatterns": [
      "Measuring influence by meeting attendance rather than decision impact",
      "Confusing being agreeable with being influential",
      "Counting cross-functional projects without assessing relationship quality"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "Cross-functional stakeholder surveys",
      "Project retrospective reports",
      "Meeting and collaboration tool analytics",
      "360-degree feedback results"
    ]
  },
  {
    "capabilityId": "C6",
    "leadingIndicators": [
      "Skip-level conversations conducted quarterly with >90% participation",
      "Performance review calibration sessions held before finalizing ratings",
      "Progression framework documented and visible to team",
      "Promotion register maintained with pre-promotion conversations",
      "Structured 1:1 agenda with development focus implemented",
      "High-performer career conversations completed annually",
      "Mentor assignments for junior and new engineers tracked",
      "Underperformance issues addressed within 30 days of identification"
    ],
    "laggingIndicators": [
      "Retention of high performers >90% (vs. regrettable attrition <5%/year)",
      "Internal promotion rate >30% of promotion opportunities",
      "Employee confidence in career path (survey): >80% can describe next-level criteria",
      "Junior engineer L2→L3 promotion rate increased 20%+ year-over-year",
      "Zero surprised rejections in promotion process",
      "Team satisfaction with growth opportunities >4.0/5.0",
      "Skip-level feedback themes addressed with manager coaching",
      "Underperformance resolution rate: 70% improvement, 30% transition or exit within 60-90 days"
    ],
    "measurementAntiPatterns": [
      "Measuring coaching by number of 1:1s held rather than development outcomes",
      "Using retention alone as a coaching metric without distinguishing regrettable vs. healthy attrition",
      "Treating all promotions as evidence of coaching without assessing readiness"
    ],
    "suggestedCadence": "quarterly",
    "dataSourceExamples": [
      "1:1 tracking tools (Lattice, Culture Amp, 15Five)",
      "Career development plan documents",
      "Engagement and manager effectiveness surveys",
      "Promotion and internal mobility data"
    ]
  },
  {
    "capabilityId": "C7",
    "leadingIndicators": [
      "Decision memos written for all decisions >1 engineer-week of effort",
      "DACI roles assigned to significant decisions (documented)",
      "Weekly proactive status updates sent to manager with outcomes, risks, and asks",
      "Bad-news escalation SLA tracked (Sev1 <4 hours, Sev2 <24 hours)",
      "Roadmap includes explicit 'What we are NOT doing' section with trade-off rationale",
      "Three-altitude communication versions prepared for major announcements"
    ],
    "laggingIndicators": [
      "First-meeting decision approval rate >80% (decisions approved without rework)",
      "Decision revisitation rate <10% (decisions don't reopen without changed facts)",
      "Manager feedback: no surprises, learns about issues from EM first",
      "Stakeholder satisfaction with communication >4/5",
      "Roadmap change tracking: zero items cut without prior stakeholder notification",
      "Exec feedback on communication quality and presence"
    ],
    "measurementAntiPatterns": [
      "Measuring communication volume (emails sent, docs written) rather than clarity and reach",
      "Assuming silence means alignment after a decision announcement",
      "Valuing speed of decision over quality of framing",
      "Measuring number of decisions made without tracking quality or revisitation rate",
      "Conflating consensus-seeking with good communication"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "Decision log or wiki (Notion, Confluence)",
      "Meeting notes and action item tracking",
      "Skip-level feedback on communication clarity",
      "Presentation feedback from leadership reviews"
    ]
  },
  {
    "capabilityId": "C8",
    "leadingIndicators": [
      "On-call rotation is staffed and reviewed monthly",
      "Runbooks exist for top 10 failure scenarios and are tested quarterly",
      "Incident response roles (commander, scribe, comms) are defined and trained",
      "Risk register is reviewed and updated monthly",
      "Google SRE targets error budget burn rate per quarter as a leading indicator — sustained burn above 5% triggers reliability investment"
    ],
    "laggingIndicators": [
      "Mean time to detect (MTTD) is under target threshold",
      "Mean time to resolve (MTTR) trending downward quarter-over-quarter",
      "Repeat incidents for the same root cause are zero after remediation",
      "Post-incident action items are completed within committed timeline (>90%)",
      "Amazon's COE process tracks action item completion rate and time-to-close as post-incident effectiveness measures"
    ],
    "measurementAntiPatterns": [
      "Counting incidents without weighting by severity and customer impact",
      "Blaming individuals in post-mortems rather than identifying systemic causes",
      "Using low incident count as proof of reliability without considering detection gaps"
    ],
    "suggestedCadence": "weekly",
    "dataSourceExamples": [
      "Incident management platforms (PagerDuty, Opsgenie, FireHydrant)",
      "Post-incident review documents",
      "Monitoring and alerting dashboards",
      "SLO tracking tools"
    ]
  },
  {
    "capabilityId": "C9",
    "leadingIndicators": [
      "Key metrics are defined before project kickoff, not after launch",
      "Dashboards are reviewed in team meetings at least biweekly",
      "Data quality checks are automated for critical metrics",
      "A/B test or experiment framework is available and used",
      "Google and Microsoft's SPACE framework recommends measuring developer satisfaction alongside DORA metrics as complementary leading indicators",
      "Every tracked metric paired with a complementary metric preventing single-dimension optimization",
      "Monthly audit: 'What decision did this metric inform?' applied to all active metrics"
    ],
    "laggingIndicators": [
      "Decisions reference specific data points in decision documents",
      "Product outcomes match or exceed pre-defined success criteria (>70% of launches)",
      "Data-informed pivots happen within one cycle of negative signal",
      "Metric definitions are consistent across teams (no conflicting dashboards)",
      "Google's DORA research tracks four key metrics (deploy frequency, lead time, change failure rate, MTTR) as the definitive lagging measure of delivery capability",
      "Zero metrics maintained that haven't informed a decision in 90 days",
      "Team reports metrics are useful (not surveillance) in engagement surveys"
    ],
    "measurementAntiPatterns": [
      "Tracking vanity metrics that do not tie to business or user outcomes",
      "Measuring everything without distinguishing signal from noise",
      "Using metrics to justify pre-determined conclusions rather than inform decisions"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "Analytics platforms (Amplitude, Mixpanel, Looker)",
      "A/B testing tools (LaunchDarkly, Optimizely)",
      "Business intelligence dashboards",
      "OKR and goal tracking systems"
    ]
  },
  {
    "capabilityId": "C10",
    "leadingIndicators": [
      "Headcount plan is documented with clear rationale per role",
      "Investment allocation (build vs. maintain vs. innovate) is explicitly tracked",
      "Budget reviews happen monthly with variance analysis",
      "Tradeoff decisions are documented with alternatives considered",
      "Amazon tracks cost-per-transaction and monthly cost attribution reports as leading indicators of financial discipline",
      "Every resource request includes tiered options with quantified ROI",
      "Cost-per-outcome dashboards reviewed monthly with engineering and finance"
    ],
    "laggingIndicators": [
      "Budget variance is within 5% of plan at quarter-end",
      "Time-to-fill for approved roles is within target (e.g., <60 days)",
      "ROI of major investments is measured and reported within 6 months of completion",
      "No critical projects stalled due to unplanned resource constraints",
      "Netflix measures engineering output per dollar spent as a lagging indicator, optimizing for talent density over headcount",
      "Resource reallocation decisions made proactively (before quarterly planning forces them)",
      "Engineering ROI improving quarter-over-quarter by cost-per-outcome metrics"
    ],
    "measurementAntiPatterns": [
      "Measuring resource allocation by headcount alone without considering skill mix",
      "Treating all engineering time as fungible across project types",
      "Optimizing for utilization rate rather than throughput and outcomes",
      "Distributing resources equally across all projects regardless of impact potential",
      "Measuring headcount as the primary resource metric instead of capacity utilization and impact-per-engineer"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "Financial planning tools (Adaptive, Anaplan)",
      "HRIS and headcount tracking systems",
      "Project portfolio management tools",
      "Vendor and infrastructure cost dashboards (AWS Cost Explorer, Cloudability)"
    ]
  },
  {
    "capabilityId": "C11",
    "leadingIndicators": [
      "Job descriptions are reviewed and updated within the last 6 months",
      "Structured rubrics with behavioral anchors used for every interview, calibrated across all interviewers",
      "Onboarding checklist has 30/60/90-day milestones with clear owners",
      "Hiring funnel conversion rates tracked weekly with bottleneck identification",
      "Amazon tracks Bar Raiser override rate as a hiring quality signal; Google tracks interviewer calibration scores to ensure rubric consistency"
    ],
    "laggingIndicators": [
      "Offer acceptance rate is above 80%",
      "New hire productivity ramp (time to first meaningful contribution) is within target",
      "90-day new hire retention is above 95%",
      "Hiring manager satisfaction with candidates is above 4/5",
      "Google tracks 6-month new hire effectiveness ratings and Amazon measures regrettable attrition within 12 months of hire",
      "Regrettable hires below 10% at 12-month mark"
    ],
    "measurementAntiPatterns": [
      "Measuring hiring speed without assessing hire quality over time",
      "Using time-to-fill as the primary hiring metric without tracking new hire success",
      "Optimizing for pipeline volume rather than pipeline quality and conversion"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "ATS platforms (Greenhouse, Lever, Ashby)",
      "Onboarding tracking tools",
      "New hire surveys (30/60/90-day)",
      "Hiring quality retrospectives"
    ]
  },
  {
    "capabilityId": "C12",
    "leadingIndicators": [
      "Team working agreements and cultural norms are explicitly documented and revisited quarterly",
      "Psychological safety is explicitly discussed in retrospectives",
      "Recognition practices happen at least weekly (shoutouts, kudos)",
      "Manager models desired cultural behaviors visibly in meetings and communications",
      "Google measures psychological safety through quarterly team surveys using Project Aristotle methodology",
      "Culture onboarding materials updated and used for every new hire"
    ],
    "laggingIndicators": [
      "Psychological safety scores are above team benchmark in engagement surveys",
      "Diverse voices participate in team discussions (not dominated by 1-2 individuals)",
      "Team conflict is resolved constructively without escalation above manager",
      "Employee engagement scores are above org median",
      "Spotify tracks team Health Check trend scores across quarters as a lagging indicator of sustained cultural investment",
      "Psychological safety scores stable through rapid team growth periods",
      "No 'missing stair' patterns reported in skip-level feedback"
    ],
    "measurementAntiPatterns": [
      "Measuring culture by perks and social events rather than behavioral norms",
      "Using survey scores alone without qualitative feedback or observation",
      "Assuming uniform culture across the team without checking subgroup experiences",
      "Using team event attendance as a proxy for culture health",
      "Measuring diversity numbers without tracking inclusion and belonging metrics"
    ],
    "suggestedCadence": "quarterly",
    "dataSourceExamples": [
      "Engagement and pulse survey platforms (Culture Amp, Peakon, Officevibe)",
      "Retrospective notes and action items",
      "Recognition platform data (Bonusly, Lattice)",
      "Skip-level interview notes"
    ]
  },
  {
    "capabilityId": "C13",
    "leadingIndicators": [
      "Security training completion rate is above 95% for all engineers",
      "Vulnerability scanning is automated and runs on every deploy",
      "Compliance requirements are mapped to engineering controls with clear owners",
      "Threat modeling is completed for all new services before launch",
      "Google's BeyondCorp model tracks automated security scan coverage rate across all CI/CD pipelines as a leading indicator",
      "Security champion active and rotating on every team",
      "Automated security scanning integrated in CI/CD with severity-based gates"
    ],
    "laggingIndicators": [
      "Critical and high vulnerabilities are remediated within SLA (e.g., 7 days for critical)",
      "Zero compliance audit findings related to engineering controls",
      "Security incidents caused by known vulnerability classes decreased year-over-year",
      "Time to patch critical CVEs is under 72 hours",
      "Amazon tracks mean-time-to-patch for critical CVEs and zero-day vulnerabilities as a lagging security posture metric",
      "Zero critical/high vulnerabilities reaching production",
      "Vulnerability remediation SLA compliance >90%"
    ],
    "measurementAntiPatterns": [
      "Counting security training completions without testing knowledge retention",
      "Measuring vulnerability count without weighting by exploitability and exposure",
      "Treating compliance checkbox completion as evidence of actual security posture"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "Vulnerability scanning tools (Snyk, Dependabot, Qualys)",
      "Compliance management platforms (Vanta, Drata)",
      "Security training platforms (KnowBe4, security awareness LMS)",
      "Audit reports and findings trackers"
    ]
  },
  {
    "capabilityId": "C14",
    "leadingIndicators": [
      "Performance review timeline is published and communicated at least 4 weeks in advance",
      "Calibration criteria and rubric are documented and shared with all managers",
      "Managers complete calibration prep (draft ratings with evidence) before sessions",
      "Underperformance is addressed with documented feedback within 2 weeks of identification",
      "Google's calibration committee pre-reads are submitted 1 week before sessions, and preparation quality is tracked as a leading indicator",
      "Running performance notes maintained and current for all reports",
      "Quarterly career conversations completed with all reports (including high performers)"
    ],
    "laggingIndicators": [
      "Performance ratings distribution is consistent with org guidelines and calibrated across teams",
      "Employees report reviews as fair and useful (>4/5 in post-review survey)",
      "PIP-to-resolution cycle time is within policy guidelines",
      "Zero successful legal challenges to termination decisions",
      "Netflix measures Keeper Test conversation frequency and outcome alignment as indicators of honest performance assessment culture",
      "Zero 'surprise' complaints in post-review engagement surveys",
      "High performer retention rate above 90% annually"
    ],
    "measurementAntiPatterns": [
      "Forcing a bell curve distribution without regard for actual team performance",
      "Measuring manager effectiveness by how many PIPs they initiate rather than outcomes",
      "Using recency bias in reviews rather than evaluating the full review period",
      "Using review cycle preparation time as a proxy for review quality",
      "Measuring promotion rate without tracking post-promotion success at new level"
    ],
    "suggestedCadence": "quarterly",
    "dataSourceExamples": [
      "Performance management platforms (Lattice, Workday, BambooHR)",
      "Calibration session notes and rating distributions",
      "Post-review employee feedback surveys",
      "HR case management systems for PIPs and exits"
    ]
  }
]
