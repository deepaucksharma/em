[
  {
    "capabilityId": "C1",
    "leadingIndicators": [
      "Cross-team dependency mapping is current (updated within 30 days)",
      "Org health dashboard reviewed weekly by leadership",
      "Succession plan exists for all critical roles",
      "Quarterly org design review is scheduled and attended",
      "Quarterly org health surveys achieve >80% response rate"
    ],
    "laggingIndicators": [
      "Zero regrettable attrition post-reorg",
      "Teams achieve 85%+ delivery commitment rate",
      "Cross-team escalations decreased quarter-over-quarter",
      "Employee engagement scores stable or improving through org changes",
      "Single-threaded ownership maintained across all services (0% orphaned services)"
    ],
    "measurementAntiPatterns": [
      "Using team size as a proxy for leadership scope",
      "Measuring reorg count instead of reorg outcomes",
      "Evaluating org design in isolation from delivery metrics"
    ],
    "suggestedCadence": "quarterly",
    "dataSourceExamples": [
      "HRIS attrition data",
      "Engagement survey results",
      "Delivery tracking tools",
      "Org chart and role mapping tools"
    ]
  },
  {
    "capabilityId": "C2",
    "leadingIndicators": [
      "Prioritization framework is documented and shared with stakeholders",
      "Roadmap has explicit not-doing list with rationale",
      "Investment allocation ratios are reviewed monthly",
      "Stakeholders can articulate the current top 3 priorities",
      "Written opportunity-cost analysis required before any initiative is prioritized"
    ],
    "laggingIndicators": [
      "Feature delivery aligns with stated priorities (>80% match)",
      "Unplanned work remains below 20% of total capacity",
      "Strategic bets delivered measurable outcomes within expected timeframe",
      "Stakeholder satisfaction with prioritization decisions is above 4/5",
      "OKR completion rates tracked by investment tier (core/strategic/venture) to assess prioritization accuracy"
    ],
    "measurementAntiPatterns": [
      "Equating busyness with strategic alignment",
      "Measuring number of features shipped without tying to business outcomes",
      "Allowing loudest stakeholder to override prioritization framework"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "Roadmap and planning tools (Jira, Linear, Shortcut)",
      "OKR tracking platforms",
      "Time allocation reports",
      "Stakeholder feedback surveys"
    ]
  },
  {
    "capabilityId": "C3",
    "leadingIndicators": [
      "Architecture Decision Records (ADRs) created for all major changes",
      "Tech debt backlog is triaged and prioritized quarterly",
      "System reliability targets (SLOs) are defined for all critical services",
      "Architecture review board meets regularly with clear decision log",
      "Design doc completion rate and review turnaround time tracked",
      "Tech debt registry maintained with business impact estimates per item"
    ],
    "laggingIndicators": [
      "System uptime meets defined SLO targets (e.g., 99.9%)",
      "Median time to integrate a new service decreased over 6 months",
      "Tech debt ratio (debt work / total work) remains within target band",
      "No architecture-related production incidents recurring after fix",
      "Design doc adoption rate and architecture decision reversal frequency tracked as quality signals",
      "Tech debt ratio stable or improving quarter-over-quarter"
    ],
    "measurementAntiPatterns": [
      "Measuring architecture quality by document volume rather than decision outcomes",
      "Using code coverage as a sole proxy for system quality",
      "Ignoring operational complexity when evaluating architecture decisions"
    ],
    "suggestedCadence": "quarterly",
    "dataSourceExamples": [
      "ADR repositories and wikis",
      "Monitoring and observability platforms (Datadog, Grafana)",
      "Incident tracking systems",
      "Code complexity and dependency analysis tools"
    ]
  },
  {
    "capabilityId": "C4",
    "leadingIndicators": [
      "Sprint planning and retrospectives happen on schedule every cycle",
      "Blockers are identified and escalated within 24 hours",
      "Team working agreements are documented and reviewed quarterly",
      "Delivery forecasts are shared with stakeholders before each cycle",
      "Squad health check scores tracked as leading indicator of operational rhythm",
      "Engineers report 4+ hours of daily uninterrupted focus time",
      "Every team ceremony has documented purpose and measured value delivery"
    ],
    "laggingIndicators": [
      "Sprint commitment accuracy is 85%+ consistently",
      "Cycle time for standard work items is stable or improving quarter-over-quarter",
      "Meeting effectiveness scores above 3.5/5 in team surveys",
      "Zero missed external deadlines in the quarter",
      "Mechanism completion rate — percentage of identified process gaps with automated solutions",
      "Retro action item completion rate above 80%"
    ],
    "measurementAntiPatterns": [
      "Optimizing for velocity without measuring outcome quality",
      "Treating all process deviations as failures rather than learning signals",
      "Measuring hours worked instead of output and flow efficiency"
    ],
    "suggestedCadence": "weekly",
    "dataSourceExamples": [
      "Project management tools (Jira, Linear, Asana)",
      "DORA metrics dashboards",
      "Team health check surveys",
      "Standup and retro notes"
    ]
  },
  {
    "capabilityId": "C5",
    "leadingIndicators": [
      "Regular 1:1s scheduled with key cross-functional peers (PM, Design, Sales)",
      "Engineering input is included in cross-functional planning documents",
      "Stakeholder map is maintained with relationship health scores",
      "Pre-alignment meetings happen before major cross-functional decisions",
      "Triad health measured through quarterly joint retrospectives between Eng, PM, and Design leads"
    ],
    "laggingIndicators": [
      "Cross-functional projects delivered on time with no late-stage surprises",
      "Stakeholder NPS for engineering partnership is above 40",
      "Requests from other functions are resolved within agreed SLAs",
      "Engineering is invited to strategic planning conversations proactively",
      "Cross-team dependency resolution time tracked as a lagging indicator of partnership effectiveness"
    ],
    "measurementAntiPatterns": [
      "Measuring influence by meeting attendance rather than decision impact",
      "Confusing being agreeable with being influential",
      "Counting cross-functional projects without assessing relationship quality"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "Cross-functional stakeholder surveys",
      "Project retrospective reports",
      "Meeting and collaboration tool analytics",
      "360-degree feedback results"
    ]
  },
  {
    "capabilityId": "C6",
    "leadingIndicators": [
      "All direct reports have documented career development plans",
      "1:1s happen weekly with documented notes and action items",
      "Manager has completed coaching or leadership training in past 12 months",
      "Stretch assignments are actively tracked for each team member",
      "Manager coaching effectiveness measured through anonymous upward feedback surveys"
    ],
    "laggingIndicators": [
      "Internal promotion rate is at or above org benchmark",
      "Direct reports report high manager effectiveness in surveys (>4/5)",
      "Regrettable attrition is below org average",
      "At least one direct report ready for next-level role within 12 months",
      "Promotion rate and retention rate of direct reports serve as lagging indicators of coaching investment"
    ],
    "measurementAntiPatterns": [
      "Measuring coaching by number of 1:1s held rather than development outcomes",
      "Using retention alone as a coaching metric without distinguishing regrettable vs. healthy attrition",
      "Treating all promotions as evidence of coaching without assessing readiness"
    ],
    "suggestedCadence": "quarterly",
    "dataSourceExamples": [
      "1:1 tracking tools (Lattice, Culture Amp, 15Five)",
      "Career development plan documents",
      "Engagement and manager effectiveness surveys",
      "Promotion and internal mobility data"
    ]
  },
  {
    "capabilityId": "C7",
    "leadingIndicators": [
      "Decision documents follow a consistent template (options, tradeoffs, recommendation)",
      "Key decisions are communicated to all affected parties within 48 hours",
      "Decision log is maintained and accessible to the team",
      "Pre-reads are shared at least 24 hours before decision meetings",
      "Written narrative documents (6-pagers or equivalent) used as a leading indicator of decision quality — well-structured narratives correlate with better outcomes",
      "Decision authority matrix documented and team can reference it",
      "Communication scaling mechanisms (RFCs, decision logs) adopted and maintained"
    ],
    "laggingIndicators": [
      "Decisions are rarely revisited or reversed due to miscommunication",
      "Team members can articulate the rationale behind recent key decisions",
      "Executive asks for clarification less than 10% of the time on proposals",
      "Decision-to-action latency is under 1 week for standard decisions",
      "Decision reversal rate for Type 2 decisions serves as a signal that bias-for-action is calibrated correctly",
      "Decision revisitation rate below 10% per quarter",
      "Meeting hours per engineer trending down as team scales"
    ],
    "measurementAntiPatterns": [
      "Measuring communication volume (emails sent, docs written) rather than clarity and reach",
      "Assuming silence means alignment after a decision announcement",
      "Valuing speed of decision over quality of framing",
      "Measuring number of decisions made without tracking quality or revisitation rate",
      "Conflating consensus-seeking with good communication"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "Decision log or wiki (Notion, Confluence)",
      "Meeting notes and action item tracking",
      "Skip-level feedback on communication clarity",
      "Presentation feedback from leadership reviews"
    ]
  },
  {
    "capabilityId": "C8",
    "leadingIndicators": [
      "On-call rotation is staffed and reviewed monthly",
      "Runbooks exist for top 10 failure scenarios and are tested quarterly",
      "Incident response roles (commander, scribe, comms) are defined and trained",
      "Risk register is reviewed and updated monthly",
      "Error budget burn rate per quarter tracked as a leading indicator — sustained burn above 5% triggers reliability investment"
    ],
    "laggingIndicators": [
      "Mean time to detect (MTTD) is under target threshold",
      "Mean time to resolve (MTTR) trending downward quarter-over-quarter",
      "Repeat incidents for the same root cause are zero after remediation",
      "Post-incident action items are completed within committed timeline (>90%)",
      "COE (Correction of Error) process tracks action item completion rate and time-to-close as post-incident effectiveness measures"
    ],
    "measurementAntiPatterns": [
      "Counting incidents without weighting by severity and customer impact",
      "Blaming individuals in post-mortems rather than identifying systemic causes",
      "Using low incident count as proof of reliability without considering detection gaps"
    ],
    "suggestedCadence": "weekly",
    "dataSourceExamples": [
      "Incident management platforms (PagerDuty, Opsgenie, FireHydrant)",
      "Post-incident review documents",
      "Monitoring and alerting dashboards",
      "SLO tracking tools"
    ]
  },
  {
    "capabilityId": "C9",
    "leadingIndicators": [
      "Key metrics are defined before project kickoff, not after launch",
      "Dashboards are reviewed in team meetings at least biweekly",
      "Data quality checks are automated for critical metrics",
      "A/B test or experiment framework is available and used",
      "The SPACE framework recommends measuring developer satisfaction alongside DORA metrics as complementary leading indicators",
      "Every tracked metric paired with a complementary metric preventing single-dimension optimization",
      "Monthly audit: 'What decision did this metric inform?' applied to all active metrics"
    ],
    "laggingIndicators": [
      "Decisions reference specific data points in decision documents",
      "Product outcomes match or exceed pre-defined success criteria (>70% of launches)",
      "Data-informed pivots happen within one cycle of negative signal",
      "Metric definitions are consistent across teams (no conflicting dashboards)",
      "DORA research tracks four key metrics (deploy frequency, lead time, change failure rate, MTTR) as the definitive lagging measure of delivery capability",
      "Zero metrics maintained that haven't informed a decision in 90 days",
      "Team reports metrics are useful (not surveillance) in engagement surveys"
    ],
    "measurementAntiPatterns": [
      "Tracking vanity metrics that do not tie to business or user outcomes",
      "Measuring everything without distinguishing signal from noise",
      "Using metrics to justify pre-determined conclusions rather than inform decisions"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "Analytics platforms (Amplitude, Mixpanel, Looker)",
      "A/B testing tools (LaunchDarkly, Optimizely)",
      "Business intelligence dashboards",
      "OKR and goal tracking systems"
    ]
  },
  {
    "capabilityId": "C10",
    "leadingIndicators": [
      "Headcount plan is documented with clear rationale per role",
      "Investment allocation (build vs. maintain vs. innovate) is explicitly tracked",
      "Budget reviews happen monthly with variance analysis",
      "Tradeoff decisions are documented with alternatives considered",
      "Cost-per-transaction and monthly cost attribution reports serve as leading indicators of financial discipline",
      "Every resource request includes tiered options with quantified ROI",
      "Cost-per-outcome dashboards reviewed monthly with engineering and finance"
    ],
    "laggingIndicators": [
      "Budget variance is within 5% of plan at quarter-end",
      "Time-to-fill for approved roles is within target (e.g., <60 days)",
      "ROI of major investments is measured and reported within 6 months of completion",
      "No critical projects stalled due to unplanned resource constraints",
      "Engineering output per dollar spent serves as a lagging indicator, optimizing for talent density over headcount",
      "Resource reallocation decisions made proactively (before quarterly planning forces them)",
      "Engineering ROI improving quarter-over-quarter by cost-per-outcome metrics"
    ],
    "measurementAntiPatterns": [
      "Measuring resource allocation by headcount alone without considering skill mix",
      "Treating all engineering time as fungible across project types",
      "Optimizing for utilization rate rather than throughput and outcomes",
      "Distributing resources equally across all projects regardless of impact potential",
      "Measuring headcount as the primary resource metric instead of capacity utilization and impact-per-engineer"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "Financial planning tools (Adaptive, Anaplan)",
      "HRIS and headcount tracking systems",
      "Project portfolio management tools",
      "Vendor and infrastructure cost dashboards (AWS Cost Explorer, Cloudability)"
    ]
  },
  {
    "capabilityId": "C11",
    "leadingIndicators": [
      "Job descriptions are reviewed and updated within the last 6 months",
      "Structured rubrics with behavioral anchors used for every interview, calibrated across all interviewers",
      "Onboarding checklist has 30/60/90-day milestones with clear owners",
      "Hiring funnel conversion rates tracked weekly with bottleneck identification",
      "Bar raiser override rate tracked as a hiring quality signal; interviewer calibration scores ensure rubric consistency"
    ],
    "laggingIndicators": [
      "Offer acceptance rate is above 80%",
      "New hire productivity ramp (time to first meaningful contribution) is within target",
      "90-day new hire retention is above 95%",
      "Hiring manager satisfaction with candidates is above 4/5",
      "6-month new hire effectiveness ratings and regrettable attrition within 12 months of hire tracked as quality signals",
      "Regrettable hires below 10% at 12-month mark"
    ],
    "measurementAntiPatterns": [
      "Measuring hiring speed without assessing hire quality over time",
      "Using time-to-fill as the primary hiring metric without tracking new hire success",
      "Optimizing for pipeline volume rather than pipeline quality and conversion"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "ATS platforms (Greenhouse, Lever, Ashby)",
      "Onboarding tracking tools",
      "New hire surveys (30/60/90-day)",
      "Hiring quality retrospectives"
    ]
  },
  {
    "capabilityId": "C12",
    "leadingIndicators": [
      "Team working agreements and cultural norms are explicitly documented and revisited quarterly",
      "Psychological safety is explicitly discussed in retrospectives",
      "Recognition practices happen at least weekly (shoutouts, kudos)",
      "Manager models desired cultural behaviors visibly in meetings and communications",
      "Psychological safety measured through quarterly team surveys using validated methodology",
      "Culture onboarding materials updated and used for every new hire"
    ],
    "laggingIndicators": [
      "Psychological safety scores are above team benchmark in engagement surveys",
      "Diverse voices participate in team discussions (not dominated by 1-2 individuals)",
      "Team conflict is resolved constructively without escalation above manager",
      "Employee engagement scores are above org median",
      "Team Health Check trend scores across quarters serve as a lagging indicator of sustained cultural investment",
      "Psychological safety scores stable through rapid team growth periods",
      "No 'missing stair' patterns reported in skip-level feedback"
    ],
    "measurementAntiPatterns": [
      "Measuring culture by perks and social events rather than behavioral norms",
      "Using survey scores alone without qualitative feedback or observation",
      "Assuming uniform culture across the team without checking subgroup experiences",
      "Using team event attendance as a proxy for culture health",
      "Measuring diversity numbers without tracking inclusion and belonging metrics"
    ],
    "suggestedCadence": "quarterly",
    "dataSourceExamples": [
      "Engagement and pulse survey platforms (Culture Amp, Peakon, Officevibe)",
      "Retrospective notes and action items",
      "Recognition platform data (Bonusly, Lattice)",
      "Skip-level interview notes"
    ]
  },
  {
    "capabilityId": "C13",
    "leadingIndicators": [
      "Security training completion rate is above 95% for all engineers",
      "Vulnerability scanning is automated and runs on every deploy",
      "Compliance requirements are mapped to engineering controls with clear owners",
      "Threat modeling is completed for all new services before launch",
      "Automated security scan coverage rate across all CI/CD pipelines tracked as a leading indicator",
      "Security champion active and rotating on every team",
      "Automated security scanning integrated in CI/CD with severity-based gates"
    ],
    "laggingIndicators": [
      "Critical and high vulnerabilities are remediated within SLA (e.g., 7 days for critical)",
      "Zero compliance audit findings related to engineering controls",
      "Security incidents caused by known vulnerability classes decreased year-over-year",
      "Time to patch critical CVEs is under 72 hours",
      "Mean-time-to-patch for critical CVEs and zero-day vulnerabilities tracked as a lagging security posture metric",
      "Zero critical/high vulnerabilities reaching production",
      "Vulnerability remediation SLA compliance >90%"
    ],
    "measurementAntiPatterns": [
      "Counting security training completions without testing knowledge retention",
      "Measuring vulnerability count without weighting by exploitability and exposure",
      "Treating compliance checkbox completion as evidence of actual security posture"
    ],
    "suggestedCadence": "monthly",
    "dataSourceExamples": [
      "Vulnerability scanning tools (Snyk, Dependabot, Qualys)",
      "Compliance management platforms (Vanta, Drata)",
      "Security training platforms (KnowBe4, security awareness LMS)",
      "Audit reports and findings trackers"
    ]
  },
  {
    "capabilityId": "C14",
    "leadingIndicators": [
      "Performance review timeline is published and communicated at least 4 weeks in advance",
      "Calibration criteria and rubric are documented and shared with all managers",
      "Managers complete calibration prep (draft ratings with evidence) before sessions",
      "Underperformance is addressed with documented feedback within 2 weeks of identification",
      "Calibration committee pre-reads submitted 1 week before sessions, with preparation quality tracked as a leading indicator",
      "Running performance notes maintained and current for all reports",
      "Quarterly career conversations completed with all reports (including high performers)"
    ],
    "laggingIndicators": [
      "Performance ratings distribution is consistent with org guidelines and calibrated across teams",
      "Employees report reviews as fair and useful (>4/5 in post-review survey)",
      "PIP-to-resolution cycle time is within policy guidelines",
      "Zero successful legal challenges to termination decisions",
      "Keeper Test conversation frequency and outcome alignment measured as indicators of honest performance assessment culture",
      "Zero 'surprise' complaints in post-review engagement surveys",
      "High performer retention rate above 90% annually"
    ],
    "measurementAntiPatterns": [
      "Forcing a bell curve distribution without regard for actual team performance",
      "Measuring manager effectiveness by how many PIPs they initiate rather than outcomes",
      "Using recency bias in reviews rather than evaluating the full review period",
      "Using review cycle preparation time as a proxy for review quality",
      "Measuring promotion rate without tracking post-promotion success at new level"
    ],
    "suggestedCadence": "quarterly",
    "dataSourceExamples": [
      "Performance management platforms (Lattice, Workday, BambooHR)",
      "Calibration session notes and rating distributions",
      "Post-review employee feedback surveys",
      "HR case management systems for PIPs and exits"
    ]
  }
]
