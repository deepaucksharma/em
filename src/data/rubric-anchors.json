[
  {
    "anchorId": "C1-1",
    "capabilityId": "C1",
    "sourceTopic": "Org Design & Team Topologies",
    "level1Developing": "Inherits team structure and works within it. Recognizes ownership confusion when it causes problems. Understands Conway's Law conceptually but doesn't apply it proactively.",
    "level2Emerging": "Recognizes when team structure causes friction. Beginning to track ownership gaps. Starts to think about Conway's Law in practice but doesn't initiate structural changes.",
    "level3Competent": "Assesses team cognitive load and advocates for structural changes. Maintains clear ownership registry. Proposes and executes team splits/merges when needed. Applies Conway's Law to align architecture and org structure.",
    "level4Distinguished": "Proactively proposes team topology changes based on cognitive load analysis and value-stream mapping, influencing org-level design before bottlenecks surface. Executes team changes with minimal disruption. Conway's Law applied intentionally to shape architecture through team structure.",
    "level5Advanced": "Designs team topology for the org. Re-orgs executed with <1 regrettable departure per re-org cycle. Platform team justification and creation. Inverse Conway maneuver applied successfully. Team topology rationale articulated to VP+ with architectural alignment evidence. Company benchmark: Amazon's org design principles — single-threaded ownership, two-pizza teams, and API boundaries between teams — exemplify this level of structural thinking applied at scale.",
    "rationale": "Anchor 1/2: Covers structural design (team splits/merges, ownership, Conway's Law). Maps to C1-O1 through C1-O5."
  },
  {
    "anchorId": "C1-2",
    "capabilityId": "C1",
    "sourceTopic": "Cross-Team Strategy & Long-Horizon Planning",
    "level1Developing": "Focuses on own team's goals without considering cross-team impact. Planning horizon limited to current quarter. Relies on manager for org-level context and strategic direction.",
    "level2Emerging": "Beginning to track dependencies across adjacent teams. Starting to consider 2-quarter planning horizons. Occasionally contributes to org-level discussions when invited.",
    "level3Competent": "Proactively manages cross-team dependencies and alignment. 3-4 quarter planning horizon with clear milestones. Translates business strategy into team-level goals with explicit success criteria. Contributes to org-level strategy discussions. Identifies systemic issues that cross team boundaries.",
    "level4Distinguished": "Drives cross-team initiatives spanning multiple quarters with explicit dependency maps and milestone tracking. Anticipates org-level shifts (re-orgs, strategy pivots, market changes) and repositions team before impact hits. Influences strategic direction beyond own scope through written proposals that leadership acts on. Represents org perspective in senior leadership forums.",
    "level5Advanced": "Sets multi-year strategic direction for the org. Drives cross-org alignment on complex, ambiguous initiatives. Translates company strategy into team goals with measurable outcomes and cross-team dependency maps. Strategic thinking sought by VP+ leadership. Legacy decisions create lasting positive impact across the organization. Company benchmark: Google's cross-org alignment processes and Amazon's multi-year strategic planning (Working Backwards PR/FAQ) demonstrate how elite orgs sustain long-horizon thinking across teams.",
    "rationale": "Anchor 2/2: Covers strategic leadership altitude (multi-quarter planning, cross-team alignment, org-level influence, strategy-to-team translation). Maps to C1-O6 through C1-O16. Intentionally separate from structural design — strategy vs. structure are distinct sub-skills."
  },
  {
    "anchorId": "C2-1",
    "capabilityId": "C2",
    "sourceTopic": "Strategic Alignment & Roadmapping",
    "level1Developing": "Plans reactively (PM tells you what to build). OKRs exist but are task lists. Engineering impact described in technical terms. Trade-offs implicit, not communicated.",
    "level2Emerging": "Beginning to participate in planning beyond task execution. Starting to write OKRs with some outcome focus. Trade-offs acknowledged but not consistently documented.",
    "level3Competent": "Structured quarterly planning with capacity model. Outcome-oriented OKRs (max 3 per team). Engineering impact translated to business metrics. Trade-offs explicit in planning docs with 'what we're NOT doing' section. Roadmap communicated at multiple altitudes.",
    "level4Distinguished": "Planning process documented and shared with adjacent teams — at least one team adopts elements of the approach. OKRs produce measurable outcomes reviewed mid-quarter with course corrections. Engineering impact framed in terms leadership uses in their own communications (revenue, retention, risk reduction). PM partnership operates as strategic co-ownership at the roadmap level.",
    "level5Advanced": "Org-level planning drives cross-team alignment. OKRs directly tied to company strategy with measurable outcomes. Engineering ROI articulated in board-level business terms. Planning process is the standard others adopt. Strategic partner to PM/business leadership. Company benchmark: Amazon's PR/FAQ process forces opportunity-cost thinking before any initiative starts, and Google's OKR framework ties every team's goals measurably to company strategy.",
    "rationale": "Anchor 1/2: Covers strategic planning mechanics (planning process, OKRs, roadmapping, capacity models). Maps to C2-O1, C2-O5, C2-O6."
  },
  {
    "anchorId": "C2-2",
    "capabilityId": "C2",
    "sourceTopic": "Trade-Off Discipline & Decision Rigor",
    "level1Developing": "Difficulty saying no to requests. Trade-offs implicit or avoided. Decisions escalated or deferred rather than framed. First principles thinking is aspirational, not practiced.",
    "level2Emerging": "Beginning to push back on requests with basic trade-off framing. Starting to differentiate reversible from irreversible decisions. Maintains informal sense of what's not being done but doesn't communicate it explicitly.",
    "level3Competent": "Declines requests by making trade-offs visible ('yes, if we deprioritize X'). Maintains explicit 'not doing' list communicated to stakeholders. Matches decision rigor to reversibility — ships fast on reversible choices, invests analysis on irreversible ones. Challenges assumptions with first principles when the situation demands it.",
    "level4Distinguished": "Trade-off framing applied proactively to incoming requests — stakeholders receive options analysis before escalating. 'Not doing' list maintained as a living planning artifact that stakeholders reference independently. Decision rigor calibrated to stakes: reversible decisions ship within days, irreversible decisions include written alternatives analysis. First principles thinking reframes problems in ways that unlock novel solutions.",
    "level5Advanced": "Trade-off discipline is an org model — other teams adopt the practice. Strategic 'not doing' decisions create organizational clarity and focus. Decision framework handles ambiguity at scale. First principles thinking produces novel solutions that influence org strategy. Company benchmark: Amazon's 'disagree and commit' principle and Type 1/Type 2 decision framework provide structural clarity for balancing speed with rigor at scale.",
    "rationale": "Anchor 2/2: Covers prioritization discipline (saying no, trade-off framing, decision rigor, first principles thinking). Maps to C2-O2, C2-O3, C2-O4. Intentionally separate from planning mechanics — strategic discipline vs. process execution are distinct sub-skills."
  },
  {
    "anchorId": "C3-1",
    "capabilityId": "C3",
    "sourceTopic": "Technical Strategy & System Ownership",
    "level1Developing": "Scope: Individual. Key Behavior: Follows technical direction set by TL/Staff without contributing architectural input. Artifact: None — relies on others' design docs. Distinguishing Test: Cannot identify technical trade-offs in their team's system without prompting.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Participates in design reviews and forms technical opinions; begins advocating for system health based on data. Artifact: Written comments on design docs with specific technical concerns. Distinguishing Test: Can explain their team's architecture and identify one concrete improvement, but hasn't driven it.",
    "level3Competent": "Scope: Team (proactive). Key Behavior: Co-authors tech strategy with TL/Staff; runs design reviews with clear criteria; makes build-vs-buy decisions with TCO analysis. Artifact: Tech strategy doc with current→target→migration path; searchable decision log. Distinguishing Test: Design review criteria exist as a written checklist; build-vs-buy decisions include documented TCO.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Architecture decisions account for cross-team system interactions; design review criteria refined from post-mortems; influences platform direction through written proposals. Artifact: Cross-team architecture proposals with data-backed justification; post-mortem-derived review criteria. Distinguishing Test: Peer EMs seek technical input on their architecture decisions; design review improvements trace to specific production learnings.",
    "level5Advanced": "Scope: Org. Key Behavior: Sets multi-quarter technical vision adopted across teams; architecture review process institutionalized; drives platform thinking. Artifact: Org-wide technical strategy doc referenced by multiple teams; institutionalized review process with measured outcomes. Distinguishing Test: Technical governance scales across teams without creating bottlenecks; vision doc updated quarterly with delivery reality checks.",
    "rationale": "Anchor 1/2: Covers technical vision, design reviews, and system ownership. Maps to C3-O1, C3-O2, C3-O5 through C3-O8."
  },
  {
    "anchorId": "C3-2",
    "capabilityId": "C3",
    "sourceTopic": "Tech Debt & Platform Investment",
    "level1Developing": "Scope: Individual. Key Behavior: Acknowledges tech debt exists but doesn't track or prioritize it. Artifact: None — tech debt is undocumented. Distinguishing Test: Cannot quantify any tech debt item's impact on delivery velocity or incident risk.",
    "level2Emerging": "Scope: Team (informal). Key Behavior: Tracks tech debt informally; makes occasional build-vs-buy arguments with basic cost comparison. Artifact: Informal tech debt list; basic cost comparisons. Distinguishing Test: Can name the top 3 tech debt items but cannot quantify their business impact.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Manages 15-20% sprint capacity for debt; quantifies debt in business terms; makes build-vs-buy decisions with TCO analysis. Artifact: Tech debt registry with cost-of-delay and remediation estimates; documented build-vs-buy decisions. Distinguishing Test: Debt allocation is visible in sprint planning; each registry item has a business impact estimate.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Identifies systemic tech debt spanning team boundaries; proposes remediation with velocity impact measurement; evaluates platform ROI across consuming teams. Artifact: Cross-team debt remediation proposals with before/after velocity data; platform investment ROI analysis. Distinguishing Test: Tech debt allocation defended to leadership with delivery speed correlation data; platform decisions evaluated with multi-team adoption metrics.",
    "level5Advanced": "Scope: Org. Key Behavior: Drives strategic tech debt paydown with measurable velocity impact; platform investments create org-wide leverage with documented ROI. Artifact: Org-level tech debt report in business terms; platform adoption dashboard with multi-team ROI. Distinguishing Test: Tech debt investment is a first-class budget line item reviewed alongside feature delivery; platform teams have measurable internal adoption targets.",
    "rationale": "Anchor 2/2: Covers tech debt management, build-vs-buy, and platform investment decisions. Maps to C3-O3, C3-O4, C3-O9 through C3-O11. Intentionally separate from technical vision — investment prioritization is a distinct sub-skill."
  },
  {
    "anchorId": "C4-1",
    "capabilityId": "C4",
    "sourceTopic": "Operating Cadence & Process",
    "level1Developing": "Scope: Individual. Key Behavior: Follows existing processes without evaluating their effectiveness; interrupts handled reactively. Artifact: None — no documented cadence or operating norms. Distinguishing Test: Cannot describe the purpose of team ceremonies or how information flows.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: Establishing regular operating cadence; sprint predictability improving; interrupt management attempted but inconsistent. Artifact: Basic meeting schedule with stated purposes. Distinguishing Test: Can articulate why each ceremony exists but retro action items still incomplete >50% of the time.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Clear operating cadence with defined outputs; interrupt rotation implemented; retro action items tracked to >80% completion; engineers have 4+ hours of daily focus time. Artifact: Team operating manual; velocity bottleneck analysis using value stream mapping. Distinguishing Test: New hire understands the operating system within first week; focus time is structurally protected by calendar policy.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Operating cadence requires minimal intervention — team self-corrects when rituals drift; toil eliminated through quarterly reviews with measurable time reclaimed; process improvements shared with adjacent teams. Artifact: Documented process improvements with before/after metrics; async contribution rates measured. Distinguishing Test: Team operates effectively when EM is out for 2+ weeks; remote/hybrid participation is equitable by measured contribution.",
    "level5Advanced": "Scope: Org. Key Behavior: Operating rhythm is the org standard; teams self-manage within cadence; systemic velocity analysis and focus time protection are organizational norms. Artifact: Org-wide operating framework adopted by multiple teams with measured outcomes. Distinguishing Test: Other EMs adopt this operating model; remote/hybrid practices produce equal engagement measured across locations.",
    "rationale": "Anchor 1/2: Covers team operating system (cadence, process, interrupts, remote practices, velocity diagnosis, focus time protection). Maps to C4-O1, C4-O7 through C4-O13."
  },
  {
    "anchorId": "C4-2",
    "capabilityId": "C4",
    "sourceTopic": "Delivery Predictability & Execution",
    "level1Developing": "Scope: Individual. Key Behavior: Sprint velocity inconsistent; scope creep common; commitments aspirational not evidence-based; delivery risks surface at deadline. Artifact: None — no delivery tracking or capacity model. Distinguishing Test: Cannot state team's commitment accuracy rate or identify the top delivery bottleneck.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Beginning to track delivery metrics; risks identified mid-sprint but mitigation is reactive; starting to use data for capacity planning. Artifact: Basic delivery dashboard; some historical velocity data. Distinguishing Test: Can state last sprint's commitment accuracy but mitigation plans are after-the-fact.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Meeting 85%+ of committed scope for 3+ consecutive quarters; capacity model accounts for interrupts, on-call, and PTO; risks flagged early with mitigation plans. Artifact: Capacity model with interrupt/PTO buffers; delivery dashboard with trend data; documented scope negotiation with stakeholders. Distinguishing Test: Stakeholders trust delivery estimates; scope trade-offs are negotiated proactively, not at deadline.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Delivery predictability maintained through team transitions and scope changes; capacity model refined using actuals-vs-plan variance; teams self-manage scope trade-offs. Artifact: Variance analysis history; delivery practices documented for adoption. Distinguishing Test: Predictability sustained above 85% during organizational disruption; adjacent teams adopt delivery practices.",
    "level5Advanced": "Scope: Org. Key Behavior: Delivery practices are the org standard; predictability maintained through organizational change; cross-team delivery coordination is seamless. Artifact: Org-wide delivery framework with measured adoption and outcomes. Distinguishing Test: Elite-level DORA metrics maintained alongside high predictability — velocity and reliability are complementary, not competing.",
    "rationale": "Anchor 2/2: Covers delivery outcomes (predictability, capacity planning, scope management). Maps to C4-O2 through C4-O6. Intentionally separate from cadence — process vs. outcomes are distinct sub-skills."
  },
  {
    "anchorId": "C5-1",
    "capabilityId": "C5",
    "sourceTopic": "Cross-Functional Partnership",
    "level1Developing": "Works with PM on features but relationship is transactional. Design involvement is late. Dependencies are managed reactively. TPM relationship undefined.",
    "level2Emerging": "Building working relationship with PM beyond feature handoffs. Design involvement improving but still sometimes late. Starting to manage dependencies more proactively.",
    "level3Competent": "Healthy triad with PM/Design (regular syncs, shared ownership, constructive disagreements). Shared success metrics across Eng/PM/Design with regular triad syncs. Technical input shapes product direction. Dependencies negotiated proactively. TPM partnership has clear division of labor. DS and platform partnerships structured.",
    "level4Distinguished": "Technical insights shape product direction at the roadmap level — PM includes engineering perspective in quarterly planning as a co-equal input. Cross-functional disagreements resolved through structured trade-off analysis rather than hierarchy. TPM and DS partnerships produce joint deliverables with shared success criteria. Partner feedback collected and acted upon systematically.",
    "level5Advanced": "Triad is a model for the org. Shared OKRs and joint retrospectives are standard practice. Product strategy influenced by technical insights. Cross-org partnerships brokered at scale. Legal/privacy partnerships proactive. Partner feedback is consistently strong across all functions. Company benchmark: Meta's cross-functional triad model with shared OKRs and joint retrospectives treats Eng/PM/Design as co-equal owners of outcomes, not separate reporting chains.",
    "rationale": "Anchor 1/2: Covers peer-level cross-functional relationships (PM/Design triad, TPM, DS, platform partnerships, triad alignment cadence). Maps to C5-O1 through C5-O5, C5-O16."
  },
  {
    "anchorId": "C5-2",
    "capabilityId": "C5",
    "sourceTopic": "Upward Management & Sponsor Building",
    "level1Developing": "Relationship with manager is status-update focused. No sponsor relationships. Scope expansion happens to them, not by them. Political dynamics invisible or confusing.",
    "level2Emerging": "Beginning to invest in manager relationship beyond status updates. Recognizes the importance of sponsors and political capital. Starting to navigate org dynamics with awareness.",
    "level3Competent": "Manager relationship is strategic — proactive alignment, early risk signaling, mutual trust. Building sponsor relationships through consistent delivery and visibility. Navigates political dynamics constructively. Beginning to expand scope through demonstrated capability.",
    "level4Distinguished": "Manager relationship produces sponsorship for stretch assignments and leadership visibility opportunities. Multiple sponsors across leadership built through cross-team delivery and strategic contributions. Political capital invested to unblock cross-team initiatives — influence extends beyond direct reports. Scope expansion proposals grounded in demonstrated capability and strategic alignment.",
    "level5Advanced": "Sponsors at VP+ level actively advocate. Trusted advisor to senior leaders across the org. Political capital compounds through reputation. Scope expanded based on demonstrated strategic judgment. Others seek them for advice on navigating organizational dynamics. Company benchmark: Amazon's Leadership Principles expect leaders to actively build sponsor relationships and 'Earn Trust' through consistent delivery and transparent communication with senior leadership.",
    "rationale": "Anchor 2/2: Covers upward influence (managing up, sponsor relationships, political capital, scope navigation). Maps to C5-O6 through C5-O15. Intentionally separate from peer partnerships — upward influence is a distinct sub-skill."
  },
  {
    "anchorId": "C6-1",
    "capabilityId": "C6",
    "sourceTopic": "Team Health & Execution",
    "level1Developing": "Holds 1:1s but they're mostly status updates. Avoids difficult conversations. Capacity planning is rough estimate. Handles retention reactively (after notice given). Struggles with adversity communication.",
    "level2Emerging": "1:1s improving — mixing status with some career discussion. Beginning to address performance issues (though slowly). Capacity model emerging. Retention mostly reactive but starting to notice flight risk signals.",
    "level3Competent": "1:1s are trust-building with career development mix. Addresses underperformance within 2 weeks of identification. Capacity model accounts for interrupts, on-call, PTO. Retention managed proactively (stay interviews, flight risk identification). Leads team through adversity with honest communication.",
    "level4Distinguished": "Measures psychological safety with structured assessments (Edmondson scale or equivalent) and builds action plans for any dimension below threshold. Addresses underperformance through skill/will diagnosis with differentiated intervention paths. Stay interviews surface and address retention risks before they become resignations. Team members cite their manager as a reason for staying in engagement surveys.",
    "level5Advanced": "Team scores 4.0+ on psychological safety survey (Edmondson scale or equivalent) consistently. Zero avoidance of difficult conversations. Team executes predictably at sustainable pace. <1 regrettable departure per 12 engineers per rolling 12 months. Team navigates crises with minimal productivity loss. Company benchmark: Google's Project Oxygen and manager effectiveness research identified coaching as the #1 behavior distinguishing great managers, measured through anonymous upward feedback surveys.",
    "rationale": "Anchor 1/2: Covers EM-level coaching (1:1s, underperformance, retention, stretch assignments, Staff+ management, career development). Maps to C6-O1 through C6-O5, C6-O8 (stay interviews), C6-O9 (Staff+ management), C6-O10 (career conversations), C6-O13 (career pathways)."
  },
  {
    "anchorId": "C6-2",
    "capabilityId": "C6",
    "sourceTopic": "Managing Managers (Director Track)",
    "level1Developing": "n/a at EM level. Early Director: still doing EM-level work. Skip-levels infrequent or awkward. EMs operate independently without cross-calibration. Delegation is uncomfortable.",
    "level2Emerging": "n/a at EM level. Early Director: beginning to establish skip-level rhythm. Starting to coach EMs beyond project management. Cross-calibration conversations starting. Delegation improving.",
    "level3Competent": "Regular skip-level 1:1s providing org insight. EMs coached on management craft (not just project management). Cross-EM calibration sessions before perf cycles. Delegation boundaries clear. EM bench strength being developed.",
    "level4Distinguished": "Skip-level insights drive proactive interventions — org issues surfaced and addressed before they escalate. EMs coached on the full management craft (coaching, calibration, stakeholder management) with specific behavioral feedback. Cross-EM calibration produces consistent standards — rating distributions align across teams. EM bench development includes deliberate stretch assignments preparing next-generation managers.",
    "level5Advanced": "EMs are independently strong leaders. Skip-level insights drive proactive org interventions. Calibration standards consistent across all EMs. EM pipeline producing ready managers. Director operates at Director altitude consistently. Org health monitored and maintained systematically. Company benchmark: Amazon evaluates Directors on how independently their EMs operate — Director-level heroics are a failure mode, not a strength.",
    "rationale": "Anchor 2/2: Covers Director-level EM development (coaching EMs, skip-levels, bench building, succession planning). Maps to C6-O6, C6-O7, C6-O11 (coaching struggling EMs), C6-O12 (succession plans). Intentionally separate — different altitude of practice."
  },
  {
    "anchorId": "C7-1",
    "capabilityId": "C7",
    "sourceTopic": "Stakeholder Management & Influence",
    "level1Developing": "Communicates status when asked. Frames asks in engineering terms. Delivers bad news late or sugar-coated. Political capital not actively built. No executive presence.",
    "level2Emerging": "Starting to provide proactive updates without being asked. Beginning to frame proposals with some business context. Bad news delivery improving but still sometimes delayed.",
    "level3Competent": "Proactive weekly written updates. Proposals framed in business terms with ROI. Bad news delivered early with mitigation plan. Political capital built through consistent delivery and helping others. Demonstrates comfort in exec reviews.",
    "level4Distinguished": "Tailors communication altitude to audience — IC-level detail for engineers, business outcome framing for VPs. Proposals include pre-built options with trade-off analysis, not just a single recommendation. Bad news delivered with root cause analysis and mitigation options before being asked. Builds political capital by unblocking cross-team dependencies, not just delivering within own scope.",
    "level5Advanced": "Leadership trusts completely — maximum autonomy earned. Cross-org alignment driven without escalation. Executive communication is crisp and confident. Sponsors at VP+ level actively advocate for you. Seen as a leader beyond engineering function. Company benchmark: Amazon's 6-pager narrative format eliminates slide-deck hand-waving and forces structured thinking, while Netflix's 'context, not control' model delegates decisions with clear strategic framing.",
    "rationale": "Anchor 1/2: Covers external-facing communication (status updates, bad news delivery, exec presentations, managing up, difficult conversations, communication scaling). Maps to C7-O2 through C7-O5, C7-O8 (difficult conversations), C7-O9 (communication scaling), C7-O11 (explicit trade-off communication)."
  },
  {
    "anchorId": "C7-2",
    "capabilityId": "C7",
    "sourceTopic": "Decision Making & Prioritization",
    "level1Developing": "Decisions made in meetings without documentation. Difficulty saying no. Most decisions escalated or deferred. First principles thinking is aspirational, not practiced.",
    "level2Emerging": "Some decisions documented but inconsistently. Starting to differentiate reversible from irreversible decisions. Beginning to push back on requests with basic trade-off framing.",
    "level3Competent": "DACI/RFC culture established. Decisions classified by reversibility and handled appropriately. Says no with trade-off framing. First principles analysis applied to significant decisions. Systems thinking catches second-order effects.",
    "level4Distinguished": "Decision framework shared with peers and influencing adoption on adjacent teams. Makes high-stakes decisions under ambiguity with documented reasoning that holds up in retrospect. First principles analysis produces novel approaches — reframing problems, not just analyzing existing options. Systems thinking anticipates cross-team second-order effects before they materialize.",
    "level5Advanced": "Decision framework is the org standard. Prioritization under ambiguity is a strength — forward progress in uncertain situations. First principles thinking produces novel solutions. Systems thinking prevents cross-org negative externalities. Decision quality recognized by peers and leadership. Company benchmark: Amazon's Type 1/Type 2 decision framework and 'disagree and commit' principle provide structural clarity for decision-making at scale.",
    "rationale": "Anchor 2/2: Covers decision architecture (DACI/RFC, trade-off framing, reversibility, authority distribution). Maps to C7-O1, C7-O6, C7-O7, C7-O10 (decision authority distribution). Intentionally separate — communication vs decision-making are distinct sub-skills."
  },
  {
    "anchorId": "C8-1",
    "capabilityId": "C8",
    "sourceTopic": "Incident Response & Command",
    "level1Developing": "Scope: Individual. Key Behavior: Reacts to incidents without defined process; post-mortems blame individuals or don't happen; on-call health unmonitored. Artifact: None. Distinguishing Test: Cannot describe incident command roles or the team's on-call page rate.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Participates in incident response; post-mortems attempted but action items incomplete; on-call load tracked but not actively managed. Artifact: Post-mortem documents exist but completion tracking is informal. Distinguishing Test: Post-mortems happen but action item completion rate is below 50%.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: ICS roles defined and practiced; blameless post-mortems with >90% action item completion; on-call health maintained at <2 off-hours pages/night. Artifact: ICS roster, post-mortem template with tracked action items, on-call health dashboard. Distinguishing Test: Repeat incidents from same root cause drop to near-zero; on-call is viewed as sustainable.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Incident response patterns shared across teams; post-mortem themes analyzed quarterly for systemic improvements; on-call excellence is a team differentiator. Artifact: Cross-team incident pattern analysis; on-call health benchmarks across teams. Distinguishing Test: Other teams adopt incident response practices; MTTR consistently below area average.",
    "level5Advanced": "Scope: Org. Key Behavior: Incident management culture institutionalized; error budget model drives reliability investment; org-wide MTTR improving year-over-year. Artifact: Org-wide incident management framework with measured outcomes. Distinguishing Test: Reliability metrics used as first-class business KPIs; error budget policy automatically governs feature/reliability trade-offs.",
    "rationale": "Anchor 1/2: Covers reactive operational excellence (incident command, post-mortems, on-call health). Maps to C8-O1 through C8-O3."
  },
  {
    "anchorId": "C8-2",
    "capabilityId": "C8",
    "sourceTopic": "Systemic Risk Reduction",
    "level1Developing": "Scope: Individual. Key Behavior: Risk management is purely reactive — learns only from production incidents; no pre-launch risk assessment. Artifact: None. Distinguishing Test: Cannot identify the top 3 failure modes in their system.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: Conducts ad hoc risk assessments before major launches; beginning to track vendor dependencies; awareness of compliance requirements. Artifact: Launch checklist exists but not consistently used. Distinguishing Test: Can name critical dependencies but has no tested fallback for any of them.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: FMEA before every major launch; vendor risk registry maintained; compliance requirements mapped to engineering practices; quarterly game days run. Artifact: FMEA documents, vendor risk registry, compliance matrix, game day results. Distinguishing Test: Zero Sev1 incidents on launches; game days reveal and fix gaps proactively.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Risk management practices shared across teams; game days include cross-team failure scenarios; vendor management coordinated at area level. Artifact: Cross-team risk register; shared game day exercises. Distinguishing Test: Cross-team incidents handled smoothly; vendor risk mitigation coordinated rather than duplicated.",
    "level5Advanced": "Scope: Org. Key Behavior: Proactive resilience is an organizational norm; chaos engineering culture established; error budgets govern feature/reliability trade-offs. Artifact: Org-wide resilience framework with chaos engineering program. Distinguishing Test: Resilience investment justified with data; org handles major incidents without heroics.",
    "rationale": "Anchor 2/2: Covers proactive risk management (failure mode analysis, launch readiness, dependency risk, vendor management, chaos engineering, error budgets). Maps to C8-O4 through C8-O6, C8-O7 (chaos engineering and game days), C8-O8 (error budget management). Intentionally separate from incident response — proactive vs. reactive are distinct sub-skills."
  },
  {
    "anchorId": "C9-1",
    "capabilityId": "C9",
    "sourceTopic": "Metric Selection & Dashboard Design",
    "level1Developing": "Scope: Individual. Key Behavior: No metrics tracked; delivery health is anecdotal; dashboards don't exist or are never consulted. Artifact: None. Distinguishing Test: Cannot state team's deployment frequency or lead time.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Basic DORA metrics tracked; dashboards exist but checked infrequently; metrics used to report status, not drive decisions. Artifact: Basic delivery dashboard. Distinguishing Test: Can state DORA numbers but cannot name a decision driven by metric data in the last quarter.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: DORA metrics tracked weekly with trend analysis; developer satisfaction measured quarterly; metric pairings prevent single-metric gaming; feature flag lifecycle governed. Artifact: Delivery dashboard with trend data; developer satisfaction survey; metric pair documentation. Distinguishing Test: Every active metric has informed a decision within 90 days; no surveillance culture complaints.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Metric frameworks adapted to team maturity; metrics benchmarked across teams; insights shared to improve adjacent teams; metric sophistication evolves based on decisions driven. Artifact: Cross-team metric benchmarks; maturity-appropriate metric recommendations for each team. Distinguishing Test: Teams at different maturity levels have appropriately different metric sets; no death-by-metrics.",
    "level5Advanced": "Scope: Org. Key Behavior: Metrics culture established org-wide; measurement frameworks (DORA, SPACE) applied appropriately; engineering metrics integrated with business metrics. Artifact: Org-wide metrics framework with documented decision impact. Distinguishing Test: Business leadership references engineering metrics in planning; engineering metrics drive budget allocation.",
    "rationale": "Anchor 1/2: Covers metric infrastructure (DORA tracking, developer experience, delivery metrics, dashboards, metric pairings, maturity adaptation). Maps to C9-O1 through C9-O3, C9-O7, C9-O9 (metric pairings), C9-O10 (metric maturity adaptation)."
  },
  {
    "anchorId": "C9-2",
    "capabilityId": "C9",
    "sourceTopic": "Data-Driven Decision Making",
    "level1Developing": "Scope: Individual. Key Behavior: OKRs vague or absent; engineering outcomes not translated to business terms; attrition not classified. Artifact: None. Distinguishing Test: Cannot state team's OKRs or explain how engineering work connects to business outcomes.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: OKRs defined but not scored quarterly; some engineering outcomes translated to business terms; attrition tracked but not classified. Artifact: OKR document (not actively scored). Distinguishing Test: OKRs exist on paper but team cannot describe how they influenced a prioritization decision.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Outcome-oriented OKRs scored quarterly with 0.6-0.8 average; engineering outcomes translated to business metrics; attrition classified and patterns addressed; A/B testing with statistical rigor. Artifact: Scored OKRs with quarterly review; business metric translation in investment proposals; attrition classification. Distinguishing Test: >80% of engineering investment proposals include business ROI; regrettable attrition below benchmark.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: OKR quality improved through coaching; business metric translation is standard across teams; experimentation infrastructure supports multiple teams. Artifact: OKR quality rubric; cross-team experimentation platform. Distinguishing Test: Engineering leaders invited to business planning conversations based on metric credibility.",
    "level5Advanced": "Scope: Org. Key Behavior: Data-driven decision culture established; engineering metrics integrated into business strategy; experimentation at scale with organizational learning. Artifact: Org-wide experimentation platform; engineering-business metric integration. Distinguishing Test: Engineering investment decisions driven by data at board level; experimentation velocity is a competitive advantage.",
    "rationale": "Anchor 2/2: Covers decision-making with data (OKR measurement, business impact articulation, experimentation, ROI framing). Maps to C9-O4 through C9-O6, C9-O8. Intentionally separate from metric infrastructure — collecting data vs. using data are distinct sub-skills."
  },
  {
    "anchorId": "C10-1",
    "capabilityId": "C10",
    "sourceTopic": "Budget, Headcount & Resource Planning",
    "level1Developing": "Requests headcount without financial framing. Cloud costs not tracked at team level. Build-vs-buy decisions are gut feel. Budget is someone else's problem.",
    "level2Emerging": "Beginning to think about headcount in terms of ROI. Starting to track cloud costs at team level. Some awareness of build-vs-buy trade-offs. Budget awareness growing.",
    "level3Competent": "Headcount justified with ROI analysis. Cloud costs attributed and optimized per service. Build-vs-buy uses TCO framework. Contractor strategy is principled. Budget defense prepared for cuts.",
    "level4Distinguished": "Headcount proposals connect to business outcomes with multi-scenario ROI modeling (best/base/worst case). Cloud cost attribution drives team-level accountability — engineers understand and optimize their service costs. Build-vs-buy decisions documented with TCO analysis reusable for future decisions. Finance partners proactively consult on engineering investment questions.",
    "level5Advanced": "Org-level headcount narrative wins at VP/finance level. FinOps culture established across teams. Engineering investment framed as business asset with measurable returns. Proactive cost optimization (savings reinvested). Trusted steward of company resources. Company benchmark: Netflix's 'highly aligned, loosely coupled' model lets teams self-allocate within strategic guardrails, and Amazon's FinOps culture treats cost as a first-class engineering constraint.",
    "rationale": "Anchor 1/2: Covers financial stewardship (headcount justification, cloud costs, build-vs-buy, contractor strategy). Maps to C10-O1, C10-O3, C10-O4, C10-O5."
  },
  {
    "anchorId": "C10-2",
    "capabilityId": "C10",
    "sourceTopic": "Strategic Reallocation & ROI Framing",
    "level1Developing": "Resources stay where they were assigned regardless of changing priorities. No framework for comparing ROI across investments. Reacts to cuts rather than proactively optimizing allocation.",
    "level2Emerging": "Beginning to evaluate resource allocation against impact. Some awareness that reallocation is a tool, not just a response to cuts. Starting to frame engineering investments in outcome terms.",
    "level3Competent": "Presents resource requests as tiered options with quantified trade-offs at each level. Tracks cost-per-outcome to demonstrate engineering ROI. Proactively reallocates capacity to highest-impact work without waiting for top-down direction. During cuts, communicates explicitly what stops — not just what continues.",
    "level4Distinguished": "Tiered resource proposals adopted as the default format for engineering requests to leadership. Cost-per-outcome data actively informs quarterly reallocation decisions — not just tracked but acted upon. Reallocation happens within established boundaries without requiring executive approval for each shift. Adjacent teams begin adopting the tiered proposal framework.",
    "level5Advanced": "Resource allocation narrative presented at VP/finance level with rigorous ROI analysis. Proactive cost optimization generates savings that are reinvested in high-impact areas. Engineering investment framing is a recognized organizational capability. Teams across org adopt similar ROI tracking practices. Company benchmark: Amazon's FinOps culture treats cost as a first-class engineering constraint, with weekly business reviews comparing cost-per-outcome across teams and reinvesting savings into highest-ROI initiatives.",
    "rationale": "Anchor 2/2: Covers dynamic allocation (tiered proposals, ROI tracking, proactive reallocation, cut communication). Maps to C10-O2, C10-O6, C10-O7, C10-O8. Intentionally separate from financial stewardship — static budgeting vs. dynamic allocation are distinct sub-skills."
  },
  {
    "anchorId": "C11-1",
    "capabilityId": "C11",
    "sourceTopic": "Hiring Process & Bar Raising",
    "level1Developing": "Participates in hiring loops. Onboarding is informal (buddy system if lucky). Headcount requests are 'we need more people.' No engineering brand presence.",
    "level2Emerging": "Hiring loops becoming more structured. Some onboarding documentation exists. Starting to justify headcount with basic data. Referral network emerging.",
    "level3Competent": "Calibrated hiring loops with trained interviewers and rubrics. Structured interview processes evaluate demonstrated competencies over pedigree. Headcount justified with capacity model and business impact. Referral pipeline active. Bar raiser discipline maintained under hiring pressure. Hiring funnel conversion tracked at each stage.",
    "level4Distinguished": "Hiring bar maintained under volume pressure — interview calibration sessions run quarterly with inter-rater reliability tracked. Structured interview process produces consistent signal across interviewers. Headcount narrative connects team capacity to business strategy with specific revenue or risk impact. Diverse candidate slates achieved through structured sourcing, not just pipeline volume.",
    "level5Advanced": "Bar raiser discipline maintained under pressure. Interviewer calibration is org model. Headcount cases win at VP level. Hiring process produces consistently strong signal with high inter-rater reliability. Competitive hiring strategy wins talent beyond compensation through differentiated value proposition. Hiring funnel instrumented and optimized with data-driven pipeline management. Company benchmark: Amazon's Bar Raiser program gives independent interviewers veto power to protect hiring quality, and Google's structured hiring committees use calibrated rubrics to eliminate bias.",
    "rationale": "Anchor 1/2: Covers hiring mechanics (interview loops, bar raising, headcount justification, interviewer calibration, structured interviews, competitive hiring strategy, funnel optimization). Maps to C11-O1, C11-O2, C11-O7 through C11-O11."
  },
  {
    "anchorId": "C11-2",
    "capabilityId": "C11",
    "sourceTopic": "Onboarding Excellence & Talent Brand",
    "level1Developing": "Onboarding is informal — new hires figure it out. No engineering brand presence. Time-to-productivity not tracked. No referral pipeline.",
    "level2Emerging": "Basic onboarding documentation exists. Some buddy pairing. Beginning to track new hire experience. Occasional blog post or meetup participation.",
    "level3Competent": "Structured 30/60/90 onboarding with measurable milestones. Time-to-productivity tracked and improving. Buddy/mentor program formalized. Active blog or conference presence emerging. Referral pipeline maintained.",
    "level4Distinguished": "Onboarding continuously refined using new hire feedback surveys at 30/60/90 days. Time-to-productivity benchmarked against org averages with diagnosed bottlenecks. Engineering brand built through conference talks, blog posts, or open source contributions that generate inbound senior talent interest. Referral rate among highest on team — engineers actively recruit from their networks.",
    "level5Advanced": "Time-to-productivity measured and optimized to org-best levels. Engineering brand is a recruiting advantage — candidates cite it in interviews. Conference talks and open source presence attract senior talent. Onboarding process is a model other teams adopt. Company benchmark: Meta's Bootcamp onboarding gets new engineers productive within 6 weeks through structured mentorship, and Google's engineering brand (publications, open source) drives inbound senior talent.",
    "rationale": "Anchor 2/2: Covers talent brand and onboarding (30/60/90 programs, time-to-productivity, engineering brand, referrals). Maps to C11-O3 through C11-O6. Intentionally separate from hiring mechanics — acquisition vs. integration are distinct sub-skills."
  },
  {
    "anchorId": "C12-1",
    "capabilityId": "C12",
    "sourceTopic": "Team Charter & Engineering Principles",
    "level1Developing": "Team culture exists by accident (whatever norms emerged). Recognition is sporadic. Inclusion is 'we treat everyone the same.'",
    "level2Emerging": "Starting to be intentional about team norms. Some recognition happening but not systematized. Awareness of inclusion beyond 'same treatment.' Beginning to document engineering practices.",
    "level3Competent": "Team charter written and referenced. Engineering principles documented and used in design reviews. Regular recognition rituals. Active inclusion practices (meeting facilitation, async options, equitable opportunity distribution). Explicit feedback channels ensuring every voice is heard. Cultural continuity maintained through growth and leadership transitions. Toxic behaviors addressed within 48 hours.",
    "level4Distinguished": "Team charter reviewed and evolved quarterly based on team feedback and growth. Engineering principles referenced in code review and architecture decision feedback, not just design reviews. Recognition operates peer-to-peer without requiring manager initiation. Inclusion practices produce measurable engagement survey results with action plans for identified gaps.",
    "level5Advanced": "Culture is intentional, measurable, and self-reinforcing. Engineering principles adopted beyond own team. Cultural continuity maintained through rapid growth and leadership transitions without degradation. Inclusive environment measurable in surveys and independently confirmed. Toxic behaviors caught and addressed proactively — no 'missing stair' dynamics. Candidates cite team culture as top-3 reason for accepting offer in post-hire surveys. Company benchmark: Netflix's Freedom and Responsibility culture doc is the canonical example of intentional culture at scale, and Spotify's explicit team Health Checks make culture measurable and discussable.",
    "rationale": "Anchor 1/2: Covers culture foundations (team charter, engineering principles, recognition, inclusion, cultural continuity, inclusive feedback channels, toxic behavior intervention). Maps to C12-O1, C12-O2, C12-O4 through C12-O8."
  },
  {
    "anchorId": "C12-2",
    "capabilityId": "C12",
    "sourceTopic": "Knowledge Sharing & Documentation Culture",
    "level1Developing": "Knowledge sharing is informal — tribal knowledge dominates. Documentation sparse and outdated. No structured forums for cross-team learning.",
    "level2Emerging": "Some documentation emerging but inconsistent. Occasional knowledge-sharing sessions. Beginning to notice that tribal knowledge creates bottlenecks and bus-factor risk.",
    "level3Competent": "Documentation standards maintained and enforced. Regular tech talks or knowledge-sharing rituals. Design doc culture established. Onboarding documentation reduces ramp time measurably.",
    "level4Distinguished": "Team members contribute documentation as part of their workflow — updates happen alongside code changes, not as separate tasks. Design doc quality validated through structured peer review with clear approval criteria. Knowledge-sharing sessions run with rotating ownership — engineers volunteer to present. Documentation standards shared with at least one adjacent team.",
    "level5Advanced": "Knowledge sharing institutionalized across org (tech talks, wikis, design doc culture). Documentation coverage comprehensive and maintained. Knowledge-sharing practices are a model other teams adopt. Reduces organizational bus-factor risk at scale. Company benchmark: Stripe's writing culture and Amazon's design doc requirements institutionalize knowledge sharing as an organizational practice, not an individual habit.",
    "rationale": "Anchor 2/2: Covers knowledge management (documentation, tech talks, design docs, cross-team learning). Maps to C12-O3. Intentionally separate from culture foundations — knowledge sharing is a distinct operational practice."
  },
  {
    "anchorId": "C13-1",
    "capabilityId": "C13",
    "sourceTopic": "Security Practices & Vulnerability Management",
    "level1Developing": "Ensures team follows security practices when reminded. Patches vulnerabilities reactively. Change management is informal. Compliance is handled last-minute before audits.",
    "level2Emerging": "Security practices becoming more consistent without reminders. Vulnerability patching improving but SLA sometimes missed. Change management process emerging. Starting to prepare for audits proactively.",
    "level3Competent": "Security embedded in development workflow (threat modeling, automated scanning, champion rotation). Security champions rotate quarterly and attend security guild. Automated security scanning integrated into CI/CD with severity-based deployment gates. Vulnerability SLAs met consistently (P0 <48hrs, P1 <7 days). Threat models required for features touching auth, payments, or PII. Tiered change management process in place.",
    "level4Distinguished": "Security practices embedded in development culture — engineers initiate threat models without prompting for sensitive features. Vulnerability SLAs met consistently with mean resolution time trending downward. Security champion program produces engineers who improve team security practices independently. Change management process documented and influencing adoption by adjacent teams.",
    "level5Advanced": "Security culture driven across org. Change management standards adopted org-wide. Security champion program scaled beyond own team. Automated scanning and threat modeling are organizational standards. Teams proactively identify and mitigate security risks before they become vulnerabilities. Company benchmark: Google's BeyondCorp zero-trust model and Amazon's mandatory security review process demonstrate security as a proactive engineering discipline, not a compliance checkbox.",
    "rationale": "Anchor 1/2: Covers security operations (threat modeling, vulnerability management, change management, security champions, automated scanning, threat modeling for sensitive features). Maps to C13-O1 through C13-O3, C13-O5 through C13-O7."
  },
  {
    "anchorId": "C13-2",
    "capabilityId": "C13",
    "sourceTopic": "Compliance & Governance Automation",
    "level1Developing": "Compliance is handled last-minute before audits. Evidence collection is manual. Access controls inconsistent. No automated compliance checks.",
    "level2Emerging": "Beginning to embed compliance into workflows. Some automated scanning in place. Starting to track access control hygiene. Audit preparation moving earlier in the cycle.",
    "level3Competent": "Continuous compliance posture with automated evidence collection. Access governance automated (quarterly reviews, auto-expiring elevated permissions). Audit prep reduced from weeks to days.",
    "level4Distinguished": "Compliance evidence collected automatically with real-time dashboard — audit preparation requires hours, not days. Access governance runs exception-based reviews only — routine changes fully automated. Zero repeat findings across consecutive audit cycles. Governance automation patterns documented and shared with adjacent teams.",
    "level5Advanced": "Zero P1/P2 findings in 3+ consecutive audit cycles. Compliance is a competitive advantage, not a burden. Governance automation is an org model. Continuous compliance frameworks adopted beyond own team. Company benchmark: Netflix automates compliance evidence collection continuously, making audit readiness a dashboard metric rather than a quarterly project.",
    "rationale": "Anchor 2/2: Covers compliance and governance (audit readiness, evidence collection, access governance, compliance automation). Maps to C13-O3, C13-O4. Intentionally separate from security operations — compliance is a distinct discipline from vulnerability management."
  },
  {
    "anchorId": "C14-1",
    "capabilityId": "C14",
    "sourceTopic": "Performance Reviews & Calibration",
    "level1Developing": "Writes reviews but they're vague. Calibration prep is last-minute. Avoids PIPs. Career conversations are infrequent.",
    "level2Emerging": "Reviews improving in specificity. Starting to prepare for calibration earlier. Beginning to address performance issues with HR guidance. Career conversations starting to happen regularly.",
    "level3Competent": "Evidence-based reviews with specific impact and rubric language. Continuous performance documentation — running notes updated weekly, not reconstructed at review time. Calibration cases prepared with cross-team comparison data. PIPs executed when needed with HR partnership. High performers managed with differentiated development plans and retention-conscious conversations. Managed exits executed with dignity and clear process. Quarterly career conversations for all reports.",
    "level4Distinguished": "Calibration cases include cross-team comparison data that withstands committee scrutiny — ratings rarely require revision. Performance management covers the full spectrum without avoidance: high performers receive differentiated stretch assignments, underperformers receive structured improvement plans with clear milestones, managed exits execute with dignity and complete knowledge transfer. Career conversations produce written development plans referenced in subsequent reviews.",
    "level5Advanced": "Reviews set the standard for the org. Continuous performance documentation produces evidence-rich reviews with zero surprises. Calibration presence is authoritative and credible. High performer retention above 90% through differentiated investment. Performance management (including exits) handled cleanly — managed exits executed with dignity, full knowledge transfer, and zero legal escalations. Career development is the reason people join your team. Calibrates other EMs' performance standards. Company benchmark: Google's calibration committee model separates promotion decisions from direct managers, ensuring cross-team consistency and reducing individual bias in performance assessment.",
    "rationale": "Anchor 1/2: Covers performance operations (reviews, calibration, feedback, PIPs, continuous documentation, high performer management, managed exits). Maps to C14-O1 through C14-O4, C14-O6 through C14-O9."
  },
  {
    "anchorId": "C14-2",
    "capabilityId": "C14",
    "sourceTopic": "Promotion Readiness & Career Architecture",
    "level1Developing": "Promotion packets are reactive ('they've been doing this for a while'). No systematic approach to building promotion cases over time. Gap-filling stretch assignments not planned.",
    "level2Emerging": "Beginning to plan promotions 1-2 cycles ahead. Starting to identify and fill gaps through stretch assignments. Some awareness of what calibration committees look for.",
    "level3Competent": "Promo packets built over 2-3 cycles with deliberate gap-filling stretch assignments. Cross-team evidence gathering is systematic. Promotion cases well-structured with specific impact examples matching rubric criteria.",
    "level4Distinguished": "Promotion success rate demonstrates calibration skill — candidates prepared over multiple cycles with evidence portfolios that committees approve without extensive debate. Gap-filling stretch assignments deliberately designed to build specific next-level evidence. Career development reputation attracts internal transfers to the team. Coaches at least one peer EM on promotion readiness with structured frameworks.",
    "level5Advanced": "Promotion track record is among the best in the org. Promo packet construction is a model others follow. Career architecture creates clear, achievable growth paths. Development reputation is a recruiting advantage. Company benchmark: Amazon's promotion process requires documented evidence of sustained next-level impact over multiple review cycles, and Google's promo committees evaluate packets independently of manager advocacy.",
    "rationale": "Anchor 2/2: Covers career growth architecture (promotion planning, stretch assignments, evidence building, career paths). Maps to C14-O5. Intentionally separate from performance operations — evaluation vs. development are distinct sub-skills."
  }
]
