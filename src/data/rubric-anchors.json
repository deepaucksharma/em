[
  {
    "anchorId": "C1-1",
    "capabilityId": "C1",
    "sourceTopic": "Org Design & Team Topologies",
    "level1Developing": "Scope: Own team only. Key Behavior: Inherits team structure and works within it; recognizes ownership confusion when it causes problems. Artifact: None — relies on existing org charts. Distinguishing Test: Cannot articulate why the team is structured this way.",
    "level2Emerging": "Scope: Own team + adjacent. Key Behavior: Recognizes when team structure causes friction; beginning to track ownership gaps; identifies Conway's Law effects but doesn't initiate structural changes. Artifact: Ownership gap list or shared-responsibility inventory. Distinguishing Test: Can name structural friction points but hasn't proposed fixes.",
    "level3Competent": "Scope: Area (2-4 teams). Key Behavior: Assesses team cognitive load and advocates for structural changes; maintains clear ownership registry; proposes and executes team splits/merges when needed; applies Conway's Law to align architecture and org structure. Artifact: Team topology registry, cognitive load assessment, re-org proposal with rationale. Distinguishing Test: Has executed at least one intentional structural change based on data.",
    "level4Distinguished": "Scope: Org (5+ teams). Key Behavior: Proactively proposes team topology changes based on cognitive load analysis and value-stream mapping, influencing org-level design before bottlenecks surface; executes team changes with <4 weeks to full productivity. Artifact: Org-wide topology map with quarterly reviews, delegation charter, cross-team dependency board. Distinguishing Test: Org structure decisions are proactive (data-driven proposals) rather than reactive (responding to crises).",
    "level5Advanced": "Scope: Company / multi-org. Key Behavior: Designs organizational models at company scale using team topology principles (stream-aligned, platform, enabling, complicated-subsystem types) with quarterly cognitive load assessments; establishes org-design principles documented in a playbook that peer Directors reference when proposing structural changes; builds self-service org health tooling (ownership registries, health dashboards, cognitive load surveys) maintained by dedicated teams rather than individual leaders; reviews org-wide topology semi-annually against business strategy shifts and proposes structural adaptations with data-backed impact analysis; coaches peer Directors on org design, team splitting criteria, and ownership transfer protocols. Artifact: Org-design principles playbook with adoption tracking; self-service org health tooling (ownership registry, health dashboard, cognitive load survey); semi-annual topology review reports with structural proposals. Distinguishing Test: Peer Directors reference the org-design playbook when proposing structural changes; org health tooling runs without this leader maintaining it; topology reviews produce data-backed proposals, not reactive crisis responses. Company benchmark: Spotify's tribe/squad/chapter model demonstrates self-correcting org design at scale, while Amazon's two-pizza team principle enforces cognitive load limits through structural constraints rather than management oversight.",
    "level3WorkedExample": "After noticing her 12-person team struggling with context-switching across three unrelated services, Priya ran a cognitive load survey revealing that 8 of 12 engineers rated their load as 'high' or 'unsustainable.' She proposed splitting into two stream-aligned teams of 6, presented a written rationale to her Director with before/after ownership maps, and executed the split over 3 weeks. Within one quarter, both teams improved sprint predictability from 62% to 84%, and the ownership registry she created eliminated the weekly 'who owns this?' Slack threads.",
    "level4WorkedExample": "Marcus identified that three teams across his area were independently building service discovery solutions due to unclear platform ownership. He mapped the Conway's Law misalignment in a written proposal, facilitated a cross-team topology workshop with all three EMs, and proposed consolidating into one platform team plus two stream-aligned teams. The restructuring was completed in under 4 weeks with zero attrition, and cross-team dependency tickets dropped 70% in the following quarter.",
    "level5WorkedExample": "Diane authored a 15-page org-design principles playbook covering team types (stream-aligned, platform, enabling), splitting criteria (cognitive load thresholds, domain boundaries), and ownership transfer protocols. She deployed a self-service org health dashboard tracking cognitive load scores, ownership gaps, and team interaction patterns across 40+ teams. When the company acquired a 200-person startup, three peer Directors independently used her playbook to design the integration topology without her involvement, completing the structural integration in 6 weeks against a planned 12.",
    "level3CommonMisassessment": "Often rated L4 because the EM maintains a clean ownership registry and runs smooth team operations. True L4 requires proactively identifying cross-team structural misalignments and driving topology changes across multiple teams before bottlenecks surface, not just managing one's own team structure well.",
    "level4CommonMisassessment": "Often rated L5 because the EM has driven impressive cross-team restructurings. True L5 requires that org-design principles are codified in a playbook others reference independently, and that org health tooling runs without the leader maintaining it — personal influence over structure is L4, institutionalized design practice is L5.",
    "level5CommonMisassessment": "Often assessed based on the existence of an org-design playbook or health dashboard alone. True L5 requires demonstrated adoption — peer Directors must reference the playbook when proposing their own structural changes without prompting, and tooling must operate independently of the leader who built it.",
    "rationale": "Anchor 1/2: Covers structural design (team splits/merges, ownership, Conway's Law). Maps to C1-O1 through C1-O5."
  },
  {
    "anchorId": "C1-2",
    "capabilityId": "C1",
    "sourceTopic": "Cross-Team Strategy & Long-Horizon Planning",
    "level1Developing": "Scope: Own team, current quarter. Key Behavior: Focuses on own team's goals without considering cross-team impact; planning horizon limited to current quarter; relies on manager for org-level context. Artifact: Team-level sprint plans. Distinguishing Test: Cannot describe how their team's work connects to org strategy.",
    "level2Emerging": "Scope: Own team + adjacent, 2 quarters. Key Behavior: Beginning to track dependencies across adjacent teams; starting to consider 2-quarter planning horizons; occasionally contributes to org-level discussions when invited. Artifact: Dependency list for adjacent teams, multi-quarter roadmap draft. Distinguishing Test: Aware of cross-team dependencies but manages them reactively.",
    "level3Competent": "Scope: Area, 3-4 quarters. Key Behavior: Proactively manages cross-team dependencies and alignment; translates business strategy into team-level goals with explicit success criteria; contributes to org-level strategy discussions; identifies systemic issues crossing team boundaries. Artifact: Cascading OKRs with business outcome links, cross-team dependency board, strategy translation doc. Distinguishing Test: Can articulate how every team goal connects to a business outcome.",
    "level4Distinguished": "Scope: Org, 4+ quarters. Key Behavior: Drives cross-team initiatives spanning multiple quarters with explicit dependency maps and milestone tracking; anticipates org-level shifts and positions teams proactively; shapes org strategy as a peer contributor. Artifact: Multi-quarter strategic roadmap with dependency mapping, org-level OKR framework, crisis playbook. Distinguishing Test: Org-level strategy reflects their input; other Directors reference their strategic frameworks.",
    "level5Advanced": "Scope: Company, multi-year. Key Behavior: Authors multi-year engineering strategy documents connecting company-level objectives to org-level execution priorities with quarterly review cadence; leads M&A integration and divestiture execution using structured playbooks (service mapping, talent retention, culture bridging) with measurable success criteria; runs quarterly cross-org strategy alignment reviews ensuring every Director's roadmap connects to company priorities with documented dependency maps; designs strategy decomposition frameworks (cascading OKRs, press-release-first planning, dependency boards) that persist after the leader moves to a new role — peer Directors adopt them independently; coaches peer Directors on strategy translation, crisis leadership, and long-horizon planning technique. Artifact: Multi-year engineering strategy doc with quarterly review history; M&A integration playbook with success metrics; cross-org strategy alignment review reports; strategy decomposition framework templates. Distinguishing Test: Peer Directors use the strategy decomposition framework for their own planning without prompting; M&A integrations follow the playbook with measurable retention and consolidation targets met. Company benchmark: Amazon's 'working backwards' process forces strategic alignment from customer outcome to team-level execution, while Microsoft's multi-year platform bets under Nadella demonstrate how engineering strategy can drive company-level transformation.",
    "level3WorkedExample": "Raj translated the company's 'improve customer retention' objective into three specific team-level OKRs with measurable success criteria, created a cross-team dependency board showing how his 3-team area's work connected to the data platform team's API migration, and ran monthly strategy check-ins where he adjusted priorities based on mid-quarter signals. His cascading OKR doc was referenced by his Director in the quarterly business review as an example of clear strategy-to-execution translation.",
    "level4WorkedExample": "Elena noticed that four teams across two Directors' orgs were independently building customer segmentation features with no shared roadmap. She authored a 4-quarter strategic alignment proposal mapping the duplicated efforts, presented it at the Director staff meeting, and facilitated a joint planning session that consolidated the work into a shared platform initiative. The result saved an estimated 18 engineer-months of redundant work and delivered the unified feature 2 months ahead of the original fragmented timelines.",
    "level5WorkedExample": "After leading two successful M&A integrations, James documented a repeatable integration playbook covering service mapping, talent retention incentive structures, and culture bridging rituals with measurable success criteria (95% key-talent retention, service consolidation within 2 quarters). When the company acquired a third startup, the CPO assigned a different Director to lead using James's playbook. That Director completed the integration hitting all targets without James's involvement. Separately, James's strategy decomposition framework (press-release-first planning with dependency boards) was adopted by 5 of 7 peer Directors for their own annual planning.",
    "level3CommonMisassessment": "Often rated L4 because the EM runs effective quarterly planning and writes solid OKRs. True L4 requires driving cross-team strategic initiatives spanning multiple quarters and shaping org-level strategy as a peer contributor, not just translating strategy well within one's own area.",
    "level4CommonMisassessment": "Often rated L5 because the EM has significant influence on org-level strategy discussions. True L5 requires that strategy frameworks and planning processes persist after the leader moves roles — peer Directors adopt them independently. Personal strategic influence is L4; institutionalized strategic practice is L5.",
    "level5CommonMisassessment": "Often assessed based on the existence of strategy documents or M&A playbooks alone. True L5 requires demonstrated independent adoption — other leaders must successfully use the frameworks without the author's involvement, and the planning processes must survive leadership transitions.",
    "rationale": "Anchor 2/2: Covers strategic leadership altitude (multi-quarter planning, cross-team alignment, org-level influence, strategy-to-team translation). Maps to C1-O6 through C1-O15. Intentionally separate from structural design — strategy vs. structure are distinct sub-skills."
  },
  {
    "anchorId": "C2-1",
    "capabilityId": "C2",
    "sourceTopic": "Strategic Alignment & Roadmapping",
    "level1Developing": "Scope: Own team only. Key Behavior: Plans reactively — PM tells you what to build; OKRs are task lists; engineering impact described in technical terms only. Artifact: None — no capacity model or planning doc. Distinguishing Test: Cannot explain trade-offs made during planning or why specific work was prioritized.",
    "level2Emerging": "Scope: Own team (emerging). Key Behavior: Participates in planning beyond task execution; writes OKRs with some outcome focus; acknowledges trade-offs but documents them inconsistently. Artifact: Draft OKRs with partial outcome language. Distinguishing Test: Can describe what was deprioritized but rationale is verbal, not written.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Runs structured quarterly planning with capacity model; writes outcome-oriented OKRs (max 3 per team); translates engineering impact to business metrics; maintains explicit 'not doing' list. Artifact: Capacity model, planning doc with trade-offs, outcome-oriented OKRs. Distinguishing Test: Stakeholders can find the 'not doing' list and reference it independently.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Planning process documented and adopted by adjacent teams; OKRs produce measurable outcomes reviewed mid-quarter with course corrections; PM partnership operates as strategic co-ownership. Artifact: Shared planning template adopted by peers; mid-quarter OKR review with documented course corrections. Distinguishing Test: Engineering impact framed in terms leadership uses in their own communications (revenue, retention, risk reduction).",
    "level5Advanced": "Scope: Org. Key Behavior: Org-level planning drives cross-team alignment; OKRs directly tied to company strategy with measurable outcomes; planning process is the standard others adopt. Artifact: Org-wide planning framework with adoption metrics; board-level engineering ROI narrative. Distinguishing Test: Strategic partner to PM/business leadership; planning framework used by teams the leader doesn't manage. Company benchmark: Google's OKR system (adapted from Intel's Andy Grove) aligns team-level planning to company strategy through quarterly scoring and transparent cross-team visibility into priorities and trade-offs.",
    "level3WorkedExample": "Kenji implemented a structured quarterly planning process for his 8-person team: a capacity model accounting for 20% interrupt load and on-call rotation, outcome-oriented OKRs limited to 3 per quarter, and a published 'not doing' list shared with PM and stakeholders. When the VP asked why a requested feature wasn't on the roadmap, the PM pointed directly to the 'not doing' list with the documented trade-off rationale, resolving the conversation in minutes rather than triggering an escalation.",
    "level4WorkedExample": "Sofia's planning template — capacity model, outcome OKRs, and explicit trade-off documentation — was adopted by two adjacent teams after their EMs saw how her mid-quarter OKR reviews caught a scope drift early enough to course-correct. She facilitated a cross-team planning session where all three teams aligned on shared dependencies, and her engineering impact framing ('this migration reduces checkout latency by 200ms, projected to recover $1.2M in annual cart abandonment') was quoted verbatim by the VP in the board update.",
    "level5WorkedExample": "Tomoko designed an org-wide planning framework including a standardized capacity model template, OKR quality rubric, and quarterly review cadence that was adopted by 12 teams across 4 Directors' orgs. She presented the engineering ROI narrative at the quarterly board meeting, connecting infrastructure investment to customer retention metrics. When she moved to a different org 6 months later, the planning framework continued running without modification — new EMs onboarded into it during their first planning cycle.",
    "level3CommonMisassessment": "Often rated L4 because the EM runs a tight planning process with good OKRs. True L4 requires that the planning process is documented and adopted by adjacent teams, and that engineering impact is framed in terms leadership uses in their own communications — not just having a good process within one team.",
    "level4CommonMisassessment": "Often rated L5 because adjacent teams have adopted the planning template. True L5 requires that the planning framework drives org-level alignment as the standard process, and that the leader is a strategic partner to PM/business leadership — influence over a few teams' processes is L4, org-wide standard-setting is L5.",
    "level5CommonMisassessment": "Often assessed based on the existence of planning templates or OKR frameworks alone. True L5 requires that teams the leader doesn't manage use the framework as their default planning process, and that the leader contributes to board-level engineering ROI narratives — adoption breadth and strategic altitude matter, not just artifact quality.",
    "rationale": "Anchor 1/2: Covers strategic planning mechanics (planning process, OKRs, roadmapping, capacity models). Maps to C2-O1, C2-O5, C2-O6."
  },
  {
    "anchorId": "C2-2",
    "capabilityId": "C2",
    "sourceTopic": "Trade-Off Discipline & Decision Rigor",
    "level1Developing": "Scope: Own team only. Key Behavior: Difficulty saying no; trade-offs implicit or avoided; decisions escalated or deferred rather than framed. Artifact: None — no trade-off documentation. Distinguishing Test: Cannot name what was deprioritized to make room for current work.",
    "level2Emerging": "Scope: Own team (emerging). Key Behavior: Beginning to push back on requests with basic trade-off framing; starting to differentiate reversible from irreversible decisions. Artifact: Informal awareness of trade-offs but not documented. Distinguishing Test: Can explain trade-offs verbally but stakeholders don't have written reference.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Declines requests by making trade-offs visible ('yes, if we deprioritize X'); matches decision rigor to reversibility; challenges assumptions with first principles when situation demands it. Artifact: Written trade-off briefs; explicit 'not doing' list shared with stakeholders. Distinguishing Test: Stakeholders own the trade-off decision rather than feeling told no.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Trade-off framing applied proactively — stakeholders receive options analysis before escalating; decision rigor calibrated to stakes with written alternatives for irreversible choices; first principles thinking reframes problems to unlock novel solutions. Artifact: Living 'not doing' artifact stakeholders reference independently; alternatives analysis for irreversible decisions. Distinguishing Test: Reversible decisions ship within days; irreversible decisions include documented alternatives.",
    "level5Advanced": "Scope: Org. Key Behavior: Trade-off discipline is an org model — other teams adopt the practice; strategic 'not doing' decisions create organizational clarity; decision framework handles ambiguity at scale; first principles thinking produces novel solutions influencing org strategy. Artifact: Decision framework adopted across teams; documented track record of assumption challenges that saved significant investment. Distinguishing Test: Other leaders reference the decision framework without being asked to use it. Company benchmark: Amazon's Type 1/Type 2 decision framework and 'disagree and commit' principle provide structural clarity for balancing speed with rigor at scale — reversible decisions ship fast, irreversible decisions get written analysis.",
    "level3WorkedExample": "When the Head of Product pushed for adding real-time notifications to the Q3 roadmap, Aisha responded with a written trade-off brief: 'We can add notifications if we deprioritize the search rewrite, which delays the Q4 latency target by 6 weeks. Here are both options with projected impact.' The Head of Product chose to defer notifications after seeing the written trade-offs, and later told Aisha's Director that the framing made the decision easy. Aisha also classified the notifications decision as reversible (Type 2) and shipped a minimal version in 3 days to validate demand before committing full resources.",
    "level4WorkedExample": "Before the annual planning offsite, Carlos prepared a proactive options analysis for three investment areas, showing quantified ROI for each with explicit opportunity costs. When the VP proposed a new compliance initiative mid-quarter, Carlos had already modeled the capacity impact and presented three tiered options within 24 hours. His first-principles reframing of a 'build a new microservice' request as 'extend the existing event system with a new consumer' saved 4 engineer-months and became a reference example shared in the Director staff meeting.",
    "level5WorkedExample": "Fatima introduced an org-wide decision framework classifying every major decision as Type 1 (irreversible, requires written analysis with alternatives) or Type 2 (reversible, ship within 48 hours with rollback plan). She published a decision log showing 14 Type 2 decisions that shipped in under a week versus 3 Type 1 decisions that received full RFC treatment. When a peer Director's team spent 3 weeks debating a reversible API naming convention, another Director pointed them to Fatima's framework unprompted. Her first-principles challenge of the 'we need a data warehouse' assumption saved $2M by reframing the problem as a query optimization issue on existing infrastructure.",
    "level3CommonMisassessment": "Often rated L4 because the EM says no effectively and writes good trade-off briefs. True L4 requires proactively providing options analysis to stakeholders before they escalate, and applying first-principles thinking to reframe problems — not just disciplined pushback within one's own team scope.",
    "level4CommonMisassessment": "Often rated L5 because the EM's trade-off framing is excellent and adjacent teams have noticed. True L5 requires the decision framework to be formally adopted as an org model that other leaders reference without being asked — personal excellence at trade-off discipline is L4, institutionalized decision practice is L5.",
    "level5CommonMisassessment": "Often assessed based on the existence of a decision framework document. True L5 requires that the framework changes how other leaders actually make decisions — other teams must reference it unprompted, and strategic 'not doing' decisions must create measurable organizational clarity rather than just existing in a wiki.",
    "rationale": "Anchor 2/2: Covers prioritization discipline (saying no, trade-off framing, decision rigor, first principles thinking). Maps to C2-O2, C2-O3, C2-O4. Intentionally separate from planning mechanics — strategic discipline vs. process execution are distinct sub-skills."
  },
  {
    "anchorId": "C3-1",
    "capabilityId": "C3",
    "sourceTopic": "Technical Strategy & System Ownership",
    "level1Developing": "Scope: Individual. Key Behavior: Follows technical direction set by TL/Staff without contributing architectural input. Artifact: None — relies on others' design docs. Distinguishing Test: Cannot identify technical trade-offs in their team's system without prompting.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Participates in design reviews and forms technical opinions; begins advocating for system health based on data. Artifact: Written comments on design docs with specific technical concerns. Distinguishing Test: Can explain their team's architecture and identify one concrete improvement, but hasn't driven it.",
    "level3Competent": "Scope: Team (proactive). Key Behavior: Co-authors tech strategy with TL/Staff; runs design reviews with clear criteria; makes build-vs-buy decisions with TCO analysis. Artifact: Tech strategy doc with current→target→migration path; searchable decision log. Distinguishing Test: Design review criteria exist as a written checklist; build-vs-buy decisions include documented TCO.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Architecture decisions account for cross-team system interactions; design review criteria refined from post-mortems; influences platform direction through written proposals. Artifact: Cross-team architecture proposals with data-backed justification; post-mortem-derived review criteria. Distinguishing Test: Peer EMs seek technical input on their architecture decisions; design review improvements trace to specific production learnings.",
    "level5Advanced": "Scope: Org. Key Behavior: Authors and maintains an org-wide technical strategy document updated quarterly with delivery reality checks; presents architectural direction at staff meetings and architecture forums to build cross-team alignment; runs quarterly architecture review across all teams using fitness function metrics (coupling scores, complexity trends, SLO compliance) to verify alignment with stated direction; classifies every major architectural decision as reversible/irreversible and assigns proportional review rigor (lightweight for reversible, full RFC for irreversible); identifies common platform patterns across 3+ teams and authors consolidation proposals with TCO analysis and adoption roadmaps. Artifact: Org-wide technical strategy doc with quarterly update history; architecture fitness function dashboard; decision classification log (reversible vs irreversible); platform consolidation proposals. Distinguishing Test: Peer EMs reference the technical strategy doc when making local architecture decisions; fitness function metrics catch architectural drift before it reaches production; platform consolidation proposals include multi-team TCO analysis; the architecture review process runs without this person — governance scales without bottlenecks. Company benchmark: Google's design doc culture requires written technical proposals with explicit alternatives analysis before implementation, and Amazon's one-way/two-way door framing applies appropriate rigor to architecture decisions based on reversibility.",
    "level3WorkedExample": "Wei co-authored a technical strategy document with his Staff Engineer mapping the team's current monolithic order service to a target event-driven architecture with a 3-phase migration path. He established a design review checklist derived from their last two production incidents (missing circuit breakers, untested failover paths) and ran weekly reviews using it. When the team evaluated a third-party payment gateway, Wei produced a TCO analysis comparing build vs. buy over 3 years that included maintenance burden and integration costs, which his Director used to approve the buy decision.",
    "level4WorkedExample": "Nadia noticed that three teams in her area were independently implementing caching layers with different patterns and inconsistent invalidation strategies. She authored a cross-team architecture proposal recommending a shared caching framework, backed by production incident data showing 4 cache-related outages in 6 months across the teams. Her design review criteria — updated after each post-mortem to include cache invalidation verification — were adopted by two peer EMs. The shared caching framework reduced cache-related incidents to zero over the following two quarters.",
    "level5WorkedExample": "Hiroshi maintained an org-wide technical strategy document covering 8 teams, updated quarterly with delivery reality checks showing which architectural migrations were on track and which had drifted. He deployed a fitness function dashboard tracking coupling scores (measured via import analysis), complexity trends (cyclomatic complexity per service), and SLO compliance. When the dashboard flagged rising coupling between the payments and notifications services, two peer EMs used the technical strategy doc to independently design a decoupling plan without Hiroshi's involvement. His platform consolidation proposal — merging three team-specific API gateways into one shared gateway — included a TCO analysis showing $400K annual savings and was approved at the VP level.",
    "level3CommonMisassessment": "Often rated L4 because the EM co-authors strong technical strategy docs and runs good design reviews. True L4 requires that architecture decisions account for cross-team system interactions and that peer EMs seek this person's technical input — depth within one team's architecture is L3, cross-team architectural influence is L4.",
    "level4CommonMisassessment": "Often rated L5 because the EM drives impressive cross-team architecture proposals. True L5 requires an org-wide technical strategy document that peer EMs reference for local decisions, fitness function metrics that catch drift automatically, and an architecture review process that runs without the leader — personal architectural influence is L4, institutionalized governance is L5.",
    "level5CommonMisassessment": "Often assessed based on the existence of architecture documents and dashboards. True L5 requires that the architecture review process runs without the leader present, that fitness function metrics catch drift before production incidents, and that platform consolidation proposals include multi-team TCO analysis with adoption roadmaps — artifacts without demonstrated autonomous governance are insufficient.",
    "rationale": "Anchor 1/2: Covers technical vision, design reviews, and system ownership. Maps to C3-O1, C3-O2, C3-O5 through C3-O8."
  },
  {
    "anchorId": "C3-2",
    "capabilityId": "C3",
    "sourceTopic": "Tech Debt & Platform Investment",
    "level1Developing": "Scope: Individual. Key Behavior: Acknowledges tech debt exists but doesn't track or prioritize it. Artifact: None — tech debt is undocumented. Distinguishing Test: Cannot quantify any tech debt item's impact on delivery velocity or incident risk.",
    "level2Emerging": "Scope: Team (informal). Key Behavior: Tracks tech debt informally; makes occasional build-vs-buy arguments with basic cost comparison. Artifact: Informal tech debt list; basic cost comparisons. Distinguishing Test: Can name the top 3 tech debt items but cannot quantify their business impact.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Manages 15-20% sprint capacity for debt; quantifies debt in business terms; makes build-vs-buy decisions with TCO analysis. Artifact: Tech debt registry with cost-of-delay and remediation estimates; documented build-vs-buy decisions. Distinguishing Test: Debt allocation is visible in sprint planning; each registry item has a business impact estimate.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Identifies systemic tech debt spanning team boundaries; proposes remediation with velocity impact measurement; evaluates platform ROI across consuming teams. Artifact: Cross-team debt remediation proposals with before/after velocity data; platform investment ROI analysis. Distinguishing Test: Tech debt allocation defended to leadership with delivery speed correlation data; platform decisions evaluated with multi-team adoption metrics.",
    "level5Advanced": "Scope: Org. Key Behavior: Presents org-level tech debt report to VP/finance quarterly, framing each item with cost-of-delay, remediation cost, and payback period; maintains a shared tech debt dashboard showing velocity impact of completed remediation across all teams; authors platform investment proposals with multi-team adoption targets, success SLOs, and projected ROI over 4-quarter horizon; reviews tech debt budget allocation quarterly as a first-class line item alongside feature investment; coaches peer EMs on tech debt quantification technique and business-case framing for remediation requests. Artifact: Org-level tech debt report with business-impact quantification; remediation ROI dashboard (before/after velocity data); platform investment proposals with adoption targets and success SLOs. Distinguishing Test: Tech debt investment is defended with cost-of-delay data at the same review cadence as feature investment; platform proposals include measurable adoption targets reviewed quarterly. Company benchmark: Stripe's engineering effectiveness team quantifies tech debt in developer-hours-lost and prioritizes remediation by ROI, while Google's infrastructure teams maintain explicit SLOs that create accountability for platform investment outcomes.",
    "level3WorkedExample": "Lena maintained a tech debt registry for her 10-person team where each item included a business impact estimate ('legacy auth library adds 3 hours/sprint in workarounds, costing ~$18K/quarter in engineer time'). She allocated 15% of sprint capacity to debt remediation and tracked velocity before and after each fix. When evaluating whether to build a custom feature-flag service or buy LaunchDarkly, she produced a 3-year TCO analysis showing the buy option saving $90K after accounting for maintenance burden, and her Director approved the purchase within a week.",
    "level4WorkedExample": "Omar identified that a shared database schema dating from the company's early days was causing cross-team migration failures — three teams independently lost 2-4 days per quarter to schema conflicts. He authored a cross-team remediation proposal with before/after velocity projections, presented it to all three EMs, and secured buy-in by quantifying the cumulative cost at $200K/year in lost productivity. After the 6-week remediation, he published a platform ROI analysis showing a 30% reduction in deployment failures across all three teams, which his Director referenced in the quarterly investment review.",
    "level5WorkedExample": "Yuki presented a quarterly org-level tech debt report to the VP and finance partner, with each of the top 10 items framed as cost-of-delay (e.g., 'legacy CI pipeline costs $45K/month in engineer wait time'), remediation cost, and payback period. She maintained a shared dashboard showing that completed remediations had recovered 1,200 engineer-hours in the past year. Her platform investment proposal for a shared observability stack included adoption targets (80% of teams within 2 quarters), success SLOs (p95 query latency <2s), and projected ROI ($600K/year in reduced incident resolution time). The VP allocated dedicated budget for tech debt as a line item in annual planning, and two peer EMs used Yuki's quantification templates to defend their own remediation requests.",
    "level3CommonMisassessment": "Often rated L4 because the EM maintains a thorough tech debt registry with business impact estimates. True L4 requires identifying systemic tech debt spanning team boundaries and proposing cross-team remediation with velocity impact measurement — managing debt well within one team is L3, driving cross-team debt strategy is L4.",
    "level4CommonMisassessment": "Often rated L5 because the EM has driven impressive cross-team debt remediation with measurable ROI. True L5 requires presenting org-level tech debt reports to VP/finance with cost-of-delay framing, and maintaining a shared dashboard across all teams — cross-team influence is L4, org-level financial stewardship of tech debt is L5.",
    "level5CommonMisassessment": "Often assessed based on the existence of an org-level tech debt report or dashboard. True L5 requires that tech debt investment is defended with cost-of-delay data at the same review cadence as feature investment, and that platform proposals include measurable adoption targets reviewed quarterly — reports without financial parity and adoption accountability are insufficient.",
    "rationale": "Anchor 2/2: Covers tech debt management, build-vs-buy, and platform investment decisions. Maps to C3-O3, C3-O4, C3-O9 through C3-O11. Intentionally separate from technical vision — investment prioritization is a distinct sub-skill."
  },
  {
    "anchorId": "C4-1",
    "capabilityId": "C4",
    "sourceTopic": "Operating Cadence & Process",
    "level1Developing": "Scope: Individual. Key Behavior: Follows existing processes without evaluating their effectiveness; interrupts handled reactively. Artifact: None — no documented cadence or operating norms. Distinguishing Test: Cannot describe the purpose of team ceremonies or how information flows.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: Establishing regular operating cadence; sprint predictability improving; interrupt management attempted but inconsistent. Artifact: Basic meeting schedule with stated purposes. Distinguishing Test: Can articulate why each ceremony exists but retro action items still incomplete >50% of the time.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Clear operating cadence with defined outputs; interrupt rotation implemented; retro action items tracked to >80% completion; engineers have 4+ hours of daily focus time. Artifact: Team operating manual; velocity bottleneck analysis using value stream mapping. Distinguishing Test: New hire understands the operating system within first week; focus time is structurally protected by calendar policy.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Operating cadence requires minimal intervention — team self-corrects when rituals drift; toil eliminated through quarterly reviews with measurable time reclaimed; process improvements shared with adjacent teams. Artifact: Documented process improvements with before/after metrics; async contribution rates measured. Distinguishing Test: Team operates effectively when EM is out for 2+ weeks; remote/hybrid participation is equitable by measured contribution.",
    "level5Advanced": "Scope: Org. Key Behavior: Designs and documents an operating rhythm framework (cadence templates, meeting charters, interrupt policies) that peer EMs adopt across the org without top-down mandate; runs quarterly operational health reviews across all teams using standardized metrics (retro completion rate, focus time hours, ceremony value scores); coaches EMs on adapting cadence to team maturity rather than imposing one-size-fits-all process; conducts systemic velocity analysis across teams using value stream mapping, identifying cross-team bottlenecks invisible at individual team level; audits org-wide focus time quarterly and removes structural obstacles (recurring meetings, approval bottlenecks) at the org level. Artifact: Org operating rhythm framework with adoption metrics; quarterly operational health reviews; cross-team velocity analysis reports; focus time audit results. Distinguishing Test: Teams operate effectively when this leader is unavailable for 2+ weeks — the operating system runs without its architect. Company benchmark: Amazon's weekly business review cadence creates systematic operational accountability, while Spotify's squad health checks make operating rhythm health visible and discussable at the team level.",
    "level3WorkedExample": "David established a clear operating cadence for his 9-person team: Monday planning, Wednesday async standup, Friday retro with a tracked action-item board. He implemented an interrupt rotation where one engineer per sprint handled all ad-hoc requests, protecting the rest of the team's focus time to 4.5 hours/day average. Retro action items hit 85% completion rate over 3 consecutive sprints. When a new hire joined, she found the team operating manual on day one and understood all ceremonies, escalation paths, and interrupt policies within her first week.",
    "level4WorkedExample": "After a 2-week vacation, Maria returned to find her team had self-corrected when the Wednesday retro drifted into a status meeting — the tech lead referenced the meeting charter and redirected. Maria had also eliminated 6 hours/week of toil through a quarterly review that identified manual deployment approvals as a bottleneck, automating them and measuring 30% faster lead times. Two adjacent teams adopted her interrupt rotation model after seeing her team's focus time metrics, and remote engineers' async contribution rates matched in-office peers within 5%.",
    "level5WorkedExample": "Chen designed an operating rhythm framework with cadence templates (planning, retro, async standup), meeting charters (purpose, expected outputs, attendee criteria), and interrupt policies (rotation schedule, escalation thresholds). Seven of nine EMs in the org adopted the framework without a top-down mandate. His quarterly operational health review surfaced that teams using value stream mapping identified a shared CI/CD bottleneck invisible at the individual team level — a 45-minute average build queue that was costing the org 200+ engineer-hours per month. He removed the structural obstacle by partnering with the platform team to parallelize the build pipeline, and his focus time audit led to canceling 12 recurring meetings org-wide that had outlived their purpose.",
    "level3CommonMisassessment": "Often rated L4 because the EM runs a well-oiled team with good ceremonies and high retro completion rates. True L4 requires that the operating cadence is self-correcting without the EM's presence and that process improvements are shared with adjacent teams — a smooth team operation is L3, a self-sustaining one with cross-team influence is L4.",
    "level4CommonMisassessment": "Often rated L5 because adjacent teams have adopted the EM's operating practices and the team runs well during the EM's absence. True L5 requires designing an operating rhythm framework adopted org-wide, running cross-team velocity analysis that surfaces systemic bottlenecks, and conducting org-level focus time audits — influence over a few teams' processes is L4, org-wide operational architecture is L5.",
    "level5CommonMisassessment": "Often assessed based on the existence of an operating framework document or high adoption numbers. True L5 requires that teams operate effectively when the leader is unavailable for 2+ weeks and that cross-team velocity analysis identifies bottlenecks invisible at the individual team level — framework existence without demonstrated autonomous operation and systemic insight is insufficient.",
    "rationale": "Anchor 1/2: Covers team operating system (cadence, process, interrupts, remote practices, velocity diagnosis, focus time protection). Maps to C4-O1, C4-O7 through C4-O12."
  },
  {
    "anchorId": "C4-2",
    "capabilityId": "C4",
    "sourceTopic": "Delivery Predictability & Execution",
    "level1Developing": "Scope: Individual. Key Behavior: Sprint velocity inconsistent; scope creep common; commitments aspirational not evidence-based; delivery risks surface at deadline. Artifact: None — no delivery tracking or capacity model. Distinguishing Test: Cannot state team's commitment accuracy rate or identify the top delivery bottleneck.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Beginning to track delivery metrics; risks identified mid-sprint but mitigation is reactive; starting to use data for capacity planning. Artifact: Basic delivery dashboard; some historical velocity data. Distinguishing Test: Can state last sprint's commitment accuracy but mitigation plans are after-the-fact.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Meeting 85%+ of committed scope for 3+ consecutive quarters; capacity model accounts for interrupts, on-call, and PTO; risks flagged early with mitigation plans. Artifact: Capacity model with interrupt/PTO buffers; delivery dashboard with trend data; documented scope negotiation with stakeholders. Distinguishing Test: Stakeholders trust delivery estimates; scope trade-offs are negotiated proactively, not at deadline.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Delivery predictability maintained through team transitions and scope changes; capacity model refined using actuals-vs-plan variance; teams self-manage scope trade-offs. Artifact: Variance analysis history; delivery practices documented for adoption. Distinguishing Test: Predictability sustained above 85% during organizational disruption; adjacent teams adopt delivery practices.",
    "level5Advanced": "Scope: Org. Key Behavior: Maintains elite-level DORA metrics (deployment frequency, lead time, change failure rate, MTTR) across all teams with dashboards reviewed weekly; sustains >85% delivery predictability through organizational disruptions (re-orgs, leadership transitions, headcount changes) by adjusting capacity models within one sprint of the disruption; coordinates cross-team delivery dependencies using a shared dependency tracker with weekly sync cadence; coaches EMs on capacity modeling, scope negotiation, and DORA metric interpretation through quarterly calibration sessions; runs post-quarter delivery retrospectives analyzing org-level patterns (estimation accuracy, common scope creep sources, interrupt distribution). Artifact: Org-wide DORA dashboard; cross-team dependency tracker; quarterly delivery retrospective outputs; EM capacity modeling calibration records. Distinguishing Test: Delivery predictability survives organizational disruption — the capacity model adapts within one sprint, not one quarter. Company benchmark: Google's DORA research program established that elite teams achieve on-demand deployment frequency with <1 hour lead time, demonstrating that delivery speed and reliability are complementary when supported by mature engineering practices.",
    "level3WorkedExample": "Amara's 7-person team hit 88% committed scope delivery for 4 consecutive quarters. Her capacity model explicitly accounted for 15% interrupt load, on-call rotation (1 engineer/week at 50% capacity), and PTO patterns by month. When a critical security patch consumed 3 engineer-days mid-sprint, she flagged the scope impact to stakeholders within 24 hours with a revised delivery date and two options: defer the analytics dashboard by one week or cut the filtering feature. The PM chose the deferral, and the sprint still closed at 85% of original scope.",
    "level4WorkedExample": "During a re-org that moved 2 engineers off his team and added 3 new hires, Ravi updated his capacity model within 48 hours, accounting for reduced velocity during ramp-up and adjusting committed scope for the quarter. Despite the disruption, his area maintained 86% delivery predictability. He also published a variance analysis showing that his teams consistently over-estimated backend work by 20% and under-estimated frontend integration by 35%, which two adjacent teams used to recalibrate their own estimates.",
    "level5WorkedExample": "Suki maintained an org-wide DORA dashboard reviewed weekly at the Director staff meeting, covering 11 teams across 3 Directors' orgs. When a company-wide headcount freeze hit mid-quarter, she adjusted all capacity models within one sprint and published revised delivery forecasts to each stakeholder team. Her post-quarter delivery retrospective identified that 60% of scope creep org-wide originated from untracked customer escalations, leading to a new escalation triage process that reduced unplanned work by 40% the following quarter. She also ran quarterly calibration sessions where EMs compared their estimation accuracy data and shared techniques, improving org-wide estimation accuracy from 72% to 89% over three quarters.",
    "level3CommonMisassessment": "Often rated L4 because the EM consistently hits delivery targets and has a solid capacity model. True L4 requires maintaining predictability through team transitions and organizational disruption, and that delivery practices are documented and adopted by adjacent teams — consistent delivery within a stable team is L3, resilient delivery across disruption with cross-team influence is L4.",
    "level4CommonMisassessment": "Often rated L5 because the EM sustains high predictability through disruption and adjacent teams adopt delivery practices. True L5 requires maintaining elite DORA metrics across all teams in an org, running post-quarter delivery retrospectives analyzing org-level patterns, and coaching EMs on capacity modeling — area-level delivery excellence is L4, org-wide delivery architecture is L5.",
    "level5CommonMisassessment": "Often assessed based on strong DORA metrics dashboards or high delivery predictability numbers alone. True L5 requires that delivery predictability survives organizational disruption with capacity models adapting within one sprint, and that post-quarter retrospectives produce systemic insights that reduce org-level failure patterns — metrics without resilience and systemic improvement are insufficient.",
    "rationale": "Anchor 2/2: Covers delivery outcomes (predictability, capacity planning, scope management). Maps to C4-O2 through C4-O6. Intentionally separate from cadence — process vs. outcomes are distinct sub-skills."
  },
  {
    "anchorId": "C5-1",
    "capabilityId": "C5",
    "sourceTopic": "Cross-Functional Partnership",
    "level1Developing": "Scope: Own team only. Key Behavior: Works with PM on features but relationship is transactional; design involvement is late; dependencies managed reactively. Artifact: None — no shared context docs or triad cadence. Distinguishing Test: Cannot describe PM's or Design's top priorities.",
    "level2Emerging": "Scope: Own team (emerging). Key Behavior: Building working relationship with PM beyond feature handoffs; design involvement improving; starting to manage dependencies more proactively. Artifact: Informal triad communication. Distinguishing Test: Can describe PM's priorities but triad syncs are inconsistent.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Healthy triad with PM/Design (regular syncs, shared ownership, constructive disagreements); technical input shapes product direction; dependencies negotiated proactively; TPM partnership has clear division of labor. Artifact: Weekly triad sync cadence; shared success metrics; dependency tracker. Distinguishing Test: PM includes engineering perspective as co-equal input in planning.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Technical insights shape product direction at the roadmap level; cross-functional disagreements resolved through structured trade-off analysis; TPM and DS partnerships produce joint deliverables with shared success criteria; partner feedback collected and acted upon. Artifact: Joint deliverables with cross-functional partners; partner feedback action plans. Distinguishing Test: Partners proactively seek engineering input on scope and strategy decisions.",
    "level5Advanced": "Scope: Org. Key Behavior: Triad is a model for the org; shared OKRs and joint retrospectives are standard practice; cross-org partnerships brokered at scale; legal/privacy partnerships proactive; partner feedback consistently strong. Artifact: Org-wide triad model with adoption metrics; cross-functional partnership framework. Distinguishing Test: Other teams adopt the triad practices without being asked; partner functions cite this leader as a model collaborator. Company benchmark: The cross-functional triad model (Eng/PM/Design as co-equal owners) is practiced at Meta, Stripe, and Airbnb — shared OKRs and joint retrospectives ensure alignment is structural, not dependent on individual relationships.",
    "level3WorkedExample": "Tanya established a weekly triad sync with her PM and Design lead, where they jointly reviewed upcoming features, surfaced technical constraints early, and shared ownership of success metrics. When the PM proposed a real-time collaboration feature, Tanya's technical input on WebSocket limitations reshaped the design into a more achievable polling-based approach that shipped 3 weeks earlier. She also set up a shared dependency tracker with the platform team, negotiating API delivery dates 6 weeks ahead of her team's integration sprint.",
    "level4WorkedExample": "When the PM and Design leads disagreed on whether to invest in accessibility improvements versus a new analytics dashboard, Jorge facilitated a structured trade-off session with quantified impact data for both options. The data science team participated as a co-equal partner, providing user segment analysis that resolved the debate. His cross-functional partnership model — shared OKRs, joint quarterly retros, and a monthly partner feedback survey — was adopted by two adjacent triads. The Head of Product cited Jorge's partnership as 'the template for how Eng/PM should work' in the all-hands.",
    "level5WorkedExample": "Lin established an org-wide triad partnership framework: shared OKRs between Eng/PM/Design for every product area, joint quarterly retrospectives reviewing partnership health, and a cross-functional partner feedback survey run twice per year. She brokered a cross-org partnership between engineering, legal, and privacy to proactively review all features touching user data before design spec, reducing last-minute compliance blockers from 8 per quarter to zero. When the General Counsel was asked which engineering leader best exemplified proactive partnership, he named Lin without hesitation. Four triads across two other Directors' orgs adopted her partnership framework unprompted.",
    "level3CommonMisassessment": "Often rated L4 because the EM has a strong PM relationship and runs smooth triad syncs. True L4 requires that technical insights shape product direction at the roadmap level and that cross-functional disagreements are resolved through structured trade-off analysis — a healthy team-level triad is L3, roadmap-level influence with structured conflict resolution is L4.",
    "level4CommonMisassessment": "Often rated L5 because the EM's cross-functional partnerships are excellent and other teams have noticed. True L5 requires that the triad model is adopted org-wide with shared OKRs and joint retrospectives as standard practice, and that cross-org partnerships (including legal/privacy) are proactive — strong partnerships within a few teams is L4, institutionalized partnership practice is L5.",
    "level5CommonMisassessment": "Often assessed based on partner feedback scores or the existence of a partnership framework document. True L5 requires that other teams adopt the triad practices without being asked, and that partner functions cite the leader as a model collaborator — high personal partner satisfaction is not the same as org-wide partnership culture transformation.",
    "rationale": "Anchor 1/2: Covers peer-level cross-functional relationships (PM/Design triad, TPM, DS, platform partnerships, triad alignment cadence). Maps to C5-O1 through C5-O5."
  },
  {
    "anchorId": "C5-2",
    "capabilityId": "C5",
    "sourceTopic": "Upward Management & Sponsor Building",
    "level1Developing": "Scope: Own team only. Key Behavior: Relationship with manager is status-update focused; no sponsor relationships; political dynamics invisible or confusing. Artifact: None — no strategic alignment with manager. Distinguishing Test: Cannot name manager's top 3 priorities.",
    "level2Emerging": "Scope: Own team (emerging). Key Behavior: Beginning to invest in manager relationship beyond status updates; recognizes importance of sponsors and political capital; starting to navigate org dynamics. Artifact: Occasional written updates to manager. Distinguishing Test: Can articulate manager's priorities but doesn't proactively align to them.",
    "level3Competent": "Scope: Team (strategic). Key Behavior: Manager relationship is strategic — proactive alignment, early risk signaling, mutual trust; building sponsor relationships through consistent delivery; navigating political dynamics constructively; expanding scope through demonstrated capability. Artifact: Weekly written updates aligned to manager's priorities; sponsor relationship with at least one senior leader. Distinguishing Test: Manager advocates for you in rooms you're not in.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Manager relationship produces sponsorship for stretch assignments; multiple sponsors across leadership built through cross-team delivery; political capital invested to unblock cross-team initiatives; scope expansion proposals grounded in demonstrated capability. Artifact: Documented scope expansion proposals with impact evidence; cross-team initiative delivery record. Distinguishing Test: Influence extends beyond direct reports; peers seek advice on org navigation.",
    "level5Advanced": "Scope: Org. Key Behavior: Maintains active sponsor relationships at VP+ level where sponsors proactively advocate and bring opportunities without being asked. Navigates every leadership transition within 2 weeks — prepares impact summaries, schedules intros, and identifies alignment opportunities before the first planning cycle. Pre-wires all cross-org decisions involving VP+ stakeholders through structured 1:1 preparation, ensuring consensus before formal meetings. Coaches peer Directors and senior EMs on managing-up technique, stakeholder mapping, and sponsor relationship building. Submits scope expansion proposals grounded in 2+ quarters of demonstrated delivery with documented cross-functional partner support. Artifact: Leadership transition playbooks; scope expansion proposals with partner endorsements; evidence of sponsor-initiated opportunities. Distinguishing Test: VP-level sponsors proactively bring opportunities and advocate in rooms without being asked — the leader's reputation generates inbound invitations to high-visibility initiatives. Company benchmark: Amazon's Leadership Principles expect leaders to actively build sponsor relationships and 'Earn Trust' through consistent delivery and transparent communication with senior leadership.",
    "level3WorkedExample": "Grace aligned her weekly written updates to her Director's top 3 priorities (platform reliability, Q4 launch timeline, and headcount efficiency), flagging risks 2 weeks before they became blockers. When a database migration risk emerged, she escalated with a written impact summary and two mitigation options before her Director's staff meeting. Her Director later told her he used her write-up verbatim in his VP update. She also built a sponsor relationship with the VP of Infrastructure by consistently delivering on shared reliability initiatives, and the VP mentioned her by name when advocating for her team's headcount request.",
    "level4WorkedExample": "After delivering a cross-team observability platform that reduced MTTR by 40%, Kwame used the impact data to submit a scope expansion proposal requesting ownership of the broader developer experience portfolio. His proposal included endorsements from two peer EMs and the Head of Platform Engineering. He invested political capital to unblock a cross-team migration that had stalled for 3 months by pre-wiring alignment with three stakeholders in individual 1:1s before the formal decision meeting. Two senior EMs sought his advice on navigating their own scope expansion conversations.",
    "level5WorkedExample": "When a new VP of Engineering joined, Anita had her leadership transition playbook ready: a 2-page impact summary of her org's top initiatives, scheduled intro meetings within 5 business days, and identified 3 alignment opportunities between the new VP's stated priorities and her team's roadmap. Her VP-level sponsor (the CTO) proactively brought her into the company's cloud cost reduction initiative without her requesting it, citing her FinOps track record. She coached 4 peer Directors and 6 senior EMs on stakeholder mapping and managing-up technique through monthly office hours, and her scope expansion proposal framework — which required 2+ quarters of demonstrated delivery with partner endorsements — was adopted as the standard format for Director-level scope requests.",
    "level3CommonMisassessment": "Often rated L4 because the EM has a strong relationship with their manager and gets good feedback. True L4 requires multiple sponsors across leadership built through cross-team delivery, and political capital actively invested to unblock cross-team initiatives — a trusting manager relationship is L3, multi-stakeholder influence with demonstrated scope expansion is L4.",
    "level4CommonMisassessment": "Often rated L5 because the EM has strong sponsor relationships and has successfully expanded scope. True L5 requires that VP-level sponsors proactively advocate and bring opportunities without being asked, and that the leader coaches peers on managing-up technique — actively cultivating sponsors is L4, having a reputation that generates inbound opportunities and teaching others the skill is L5.",
    "level5CommonMisassessment": "Often assessed based on the number or seniority of sponsor relationships alone. True L5 requires that sponsors proactively advocate without being asked, that leadership transitions are navigated within 2 weeks using a structured playbook, and that cross-org decisions are pre-wired through structured preparation — having senior sponsors is not the same as having sponsors who independently champion the leader's work.",
    "rationale": "Anchor 2/2: Covers upward influence (managing up, sponsor relationships, political capital, scope navigation). Maps to C5-O6 through C5-O15. Intentionally separate from peer partnerships — upward influence is a distinct sub-skill."
  },
  {
    "anchorId": "C6-1",
    "capabilityId": "C6",
    "sourceTopic": "Team Health & Execution",
    "level1Developing": "Scope: Own team only. Key Behavior: Holds 1:1s but they're mostly status updates; avoids difficult conversations; capacity planning is rough estimate; handles retention reactively (after notice given). Artifact: None — no structured 1:1 notes, no capacity model, no career development docs. Distinguishing Test: Cannot describe each report's career goals or current development areas.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: 1:1s mix status with some career discussion; beginning to address performance issues (though slowly); capacity model emerging; starting to notice flight risk signals. Artifact: 1:1 notes exist but inconsistent; basic capacity spreadsheet. Distinguishing Test: Can name career goals for most reports but has no written development plan for any.",
    "level3Competent": "Scope: Own team (systematic). Key Behavior: 1:1s are trust-building with career development mix; addresses underperformance within 2 weeks of identification; capacity model accounts for interrupts, on-call, PTO; retention managed proactively via stay interviews and flight risk identification. Artifact: Written development plans for each report; capacity model updated monthly; stay interview notes. Distinguishing Test: Can show written evidence of proactive retention actions taken before resignation notices.",
    "level4Distinguished": "Scope: Team + cross-team coaching influence. Key Behavior: Measures psychological safety with structured assessments (Edmondson scale or equivalent) and builds action plans for any dimension below threshold; addresses underperformance through skill/will diagnosis with differentiated intervention paths; proactive retention interventions triggered by psych safety assessment data or engagement survey trends — not just individual stay interview responses. Artifact: Quarterly psych safety assessment results with action plans; skill/will diagnosis docs; engagement survey data showing team members cite manager as reason for staying. Distinguishing Test: Can show a proactive retention intervention triggered by psych safety assessment data or engagement survey trends — not just individual stay interview responses.",
    "level5Advanced": "Scope: Org-wide coaching culture. Key Behavior: Runs quarterly psychological safety assessments and builds action plans for any dimension below 4.0/5.0 within one sprint; initiates every difficult conversation within 48 hours — zero avoidance pattern across 12+ months; calibrates workload against sustainable pace by reviewing utilization data bi-weekly; conducts quarterly retention reviews with structured stay interviews for all reports; coaches Staff+ engineers through organizational influence challenges, not just technical decisions. Artifact: 12-month track record of zero-avoidance difficult conversations; quarterly retention plans reviewed with leadership; Staff+ coaching artifacts. Distinguishing Test: Research identifies coaching as the #1 behavior distinguishing great managers — at this level, anonymous upward feedback consistently confirms coaching quality across all reports. Company benchmark: Google's Project Oxygen research identified coaching as the #1 behavior distinguishing great managers, measured through anonymous upward feedback surveys.",
    "rationale": "Anchor 1/2: Covers EM-level coaching (1:1s, underperformance, retention, stretch assignments, Staff+ management, career development). Maps to C6-O1 through C6-O5, C6-O8 (stay interviews), C6-O9 (Staff+ management), C6-O10 (career conversations), C6-O13 (career pathways)."
  },
  {
    "anchorId": "C6-2",
    "capabilityId": "C6",
    "sourceTopic": "Managing Managers (Director Track)",
    "level1Developing": "Scope: n/a at EM level. Early Director: still doing EM-level work. Key Behavior: Skip-levels infrequent or awkward; EMs operate independently without cross-calibration; delegation is uncomfortable. Artifact: None — no skip-level notes, no cross-calibration records. Distinguishing Test: Cannot describe each EM's management development areas or current coaching priorities.",
    "level2Emerging": "Scope: n/a at EM level. Early Director (reactive). Key Behavior: Beginning to establish skip-level rhythm; starting to coach EMs beyond project management; cross-calibration conversations starting; delegation improving. Artifact: Basic skip-level notes; some EM coaching notes. Distinguishing Test: Can describe EM strengths/weaknesses but has no written EM development plans.",
    "level3Competent": "Scope: Director managing multiple EMs. Key Behavior: Regular skip-level 1:1s providing org insight; EMs coached on management craft (not just project management); cross-EM calibration sessions before perf cycles; EM bench strength being developed. Artifact: Skip-level notes with trend tracking; EM development plans; cross-calibration session outputs; delegation boundaries documented. Distinguishing Test: Can show evidence of coaching an EM on a management skill (not a project) with measurable improvement.",
    "level4Distinguished": "Scope: Director with cross-org influence. Key Behavior: Skip-level insights drive proactive interventions before issues escalate; EMs coached on full management craft with specific behavioral feedback; cross-EM calibration produces consistent rating distributions across teams; EM bench development includes deliberate stretch assignments. Artifact: Proactive intervention examples triggered by skip-level data; EM behavioral feedback records; cross-team calibration alignment docs; stretch assignment tracking. Distinguishing Test: Can show an example where skip-level data triggered an intervention that prevented an escalation — reactive Directors only discover issues after they escalate.",
    "level5Advanced": "Scope: Org-wide leadership development. Key Behavior: Delegates full operational authority to EMs, intervening only on cross-team and strategic issues; converts skip-level insights into proactive interventions as patterns emerge; runs pre-calibration alignment sessions ensuring consistent standards across all EMs; maintains succession pipeline with at least one ready-now candidate per critical role; reviews org health dashboard weekly, investigating when any leading indicator crosses warning threshold. Artifact: Succession readiness reviews (quarterly); pre-calibration alignment session records; org health dashboard with intervention log. Distinguishing Test: EMs make independent decisions without Director approval for team-scope matters — Director-level heroics are a failure mode, not a strength. Company benchmark: Amazon evaluates Directors on how independently their EMs operate — Director-level heroics are a failure mode, not a strength.",
    "rationale": "Anchor 2/2: Covers Director-level EM development (coaching EMs, skip-levels, bench building, succession planning). Maps to C6-O6, C6-O7, C6-O11 (coaching struggling EMs), C6-O12 (succession plans). Intentionally separate — different altitude of practice."
  },
  {
    "anchorId": "C7-1",
    "capabilityId": "C7",
    "sourceTopic": "Stakeholder Management & Influence",
    "level1Developing": "Scope: Own team only. Key Behavior: Communicates status when asked; frames asks in engineering terms; delivers bad news late or sugar-coated; no executive presence. Artifact: None — no written status updates, no proposals. Distinguishing Test: Cannot explain what their VP cares about or how their team's work connects to business outcomes.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Starting to provide proactive updates without being asked; beginning to frame proposals with some business context; bad news delivery improving but still sometimes delayed. Artifact: Occasional written updates; proposals exist but lack business framing. Distinguishing Test: Can articulate business value of their team's work but defaults to engineering terms under pressure.",
    "level3Competent": "Scope: Team + immediate stakeholders. Key Behavior: Proactive weekly written updates; proposals framed in business terms with ROI; bad news delivered early with mitigation plan; political capital built through consistent delivery and helping others. Artifact: Weekly written status updates; proposals with ROI analysis; documented bad-news-plus-mitigation communications. Distinguishing Test: Stakeholders voluntarily seek out this EM's updates; bad news never surprises leadership.",
    "level4Distinguished": "Scope: Cross-team + exec audience. Key Behavior: Tailors communication altitude to audience — IC-level detail for engineers, business outcome framing for VPs; proposals include pre-built options with trade-off analysis; bad news delivered with root cause analysis and mitigation options before being asked; builds political capital by unblocking cross-team dependencies. Artifact: Multi-audience communication artifacts (same decision, different framings); trade-off option docs; unsolicited mitigation plans. Distinguishing Test: Can present the same decision to an IC audience and a VP audience with appropriate framing for each — not just volume adjustment but actual altitude change.",
    "level5Advanced": "Scope: Org-wide + executive leadership. Key Behavior: Produces written narrative documents (6-pager style) for every org-level decision, forcing structured thinking and eliminating slide-deck hand-waving; delivers bad news to VP+ audiences within 4 hours with root cause analysis, mitigation options, and next-update timeline; tailors the same decision to three audiences (IC, peer EM, VP) with distinct altitude and framing within the same day; surfaces cross-org alignment gaps proactively through written pre-reads and 1:1 pre-wiring before formal meetings; coaches peer leaders on communication altitude, trade-off framing, and bad-news delivery technique. Artifact: Written narrative documents for org-level decisions; multi-audience communication artifacts; cross-org alignment docs produced without being asked; peer coaching session records. Distinguishing Test: Drives cross-org decisions through written narratives and strategic framing without needing escalation — others cite this person's communication as a model. Company benchmark: Amazon's 6-pager narrative format eliminates slide-deck hand-waving and forces structured thinking, while Netflix's 'context, not control' model delegates decisions with clear strategic framing.",
    "rationale": "Anchor 1/2: Covers external-facing communication (status updates, bad news delivery, exec presentations, managing up, difficult conversations, communication scaling). Maps to C7-O2 through C7-O5, C7-O8 (difficult conversations), C7-O9 (communication scaling), C7-O11 (explicit trade-off communication)."
  },
  {
    "anchorId": "C7-2",
    "capabilityId": "C7",
    "sourceTopic": "Decision Making & Prioritization",
    "level1Developing": "Scope: Own team only. Key Behavior: Decisions made in meetings without documentation; difficulty saying no; most decisions escalated or deferred; first principles thinking is aspirational, not practiced. Artifact: None — no decision logs, no RFC process. Distinguishing Test: Cannot list the 3 most important decisions made last quarter or explain why they were made.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Some decisions documented but inconsistently; starting to differentiate reversible from irreversible decisions; beginning to push back on requests with basic trade-off framing. Artifact: Occasional decision docs; some RFCs. Distinguishing Test: Can explain the reversible/irreversible distinction but doesn't consistently apply it — treats most decisions as high-stakes.",
    "level3Competent": "Scope: Team + adjacent teams. Key Behavior: DACI/RFC culture established; decisions classified by reversibility and handled appropriately; says no with trade-off framing; first principles analysis applied to significant decisions; systems thinking catches second-order effects. Artifact: Decision log with DACI assignments; RFC archive; documented trade-off analyses. Distinguishing Test: Can show a decision where they explicitly chose speed (Type 2) and one where they chose rigor (Type 1) — with reasoning for each.",
    "level4Distinguished": "Scope: Cross-team decision architecture. Key Behavior: Decision framework shared with peers and influencing adoption on adjacent teams; makes high-stakes decisions under ambiguity with documented reasoning that holds up in retrospect; first principles analysis produces novel approaches; systems thinking anticipates cross-team second-order effects before they materialize. Artifact: Decision framework adopted by adjacent teams; post-decision reviews showing reasoning quality; novel reframing examples. Distinguishing Test: Can show a decision made under ambiguity where the documented reasoning held up 6+ months later — not just lucky outcomes but sound process.",
    "level5Advanced": "Scope: Org-wide decision standards. Key Behavior: Maintains and evolves an org-wide decision framework with Type 1/Type 2 classification, DACI templates, and RFC processes that peer teams adopt as their default; classifies decisions under ambiguity by explicitly documenting what is known, unknown, and unknowable — then selects the appropriate decision speed; applies first principles analysis to cross-org problems, reframing debates from surface-level options to underlying assumptions; maps second-order effects across teams before committing to org-level decisions using a structured impact analysis template; coaches peer leaders on decision framework adoption and trade-off framing through quarterly decision review sessions. Artifact: Org-wide decision framework documentation with adoption metrics; Type 1/Type 2 classification examples; disagree-and-commit culture operating in documented decision outcomes; cross-org impact analysis templates. Distinguishing Test: The decision framework has outlived any single decision — new leaders adopt it during onboarding without needing the original author. Company benchmark: Amazon's Type 1/Type 2 decision framework and 'disagree and commit' principle provide structural clarity for decision-making at scale.",
    "rationale": "Anchor 2/2: Covers decision architecture (DACI/RFC, trade-off framing, reversibility, authority distribution). Maps to C7-O1, C7-O6, C7-O7, C7-O10 (decision authority distribution). Intentionally separate — communication vs decision-making are distinct sub-skills."
  },
  {
    "anchorId": "C8-1",
    "capabilityId": "C8",
    "sourceTopic": "Incident Response & Command",
    "level1Developing": "Scope: Individual. Key Behavior: Reacts to incidents without defined process; post-mortems blame individuals or don't happen; on-call health unmonitored. Artifact: None. Distinguishing Test: Cannot describe incident command roles or the team's on-call page rate.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Participates in incident response; post-mortems attempted but action items incomplete; on-call load tracked but not actively managed. Artifact: Post-mortem documents exist but completion tracking is informal. Distinguishing Test: Post-mortems happen but action item completion rate is below 50%.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: ICS roles defined and practiced; blameless post-mortems with >90% action item completion; on-call health maintained at <2 off-hours pages/night. Artifact: ICS roster, post-mortem template with tracked action items, on-call health dashboard. Distinguishing Test: Repeat incidents from same root cause drop to near-zero; on-call is viewed as sustainable.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Incident response patterns shared across teams; post-mortem themes analyzed quarterly for systemic improvements; on-call excellence is a team differentiator. Artifact: Cross-team incident pattern analysis; on-call health benchmarks across teams. Distinguishing Test: Other teams adopt incident response practices; MTTR consistently below area average.",
    "level5Advanced": "Scope: Org. Key Behavior: Maintains and evolves an org-wide incident management framework with ICS role definitions, post-mortem templates, and escalation paths that peer EMs adopt as their standard process; reviews org-wide MTTR and repeat-incident rate monthly, initiating cross-team remediation when patterns emerge; implements error budget governance that automatically triggers reliability investment when any team exceeds 80% budget consumption; runs quarterly incident retrospectives across all teams identifying systemic patterns (recurring root causes, on-call load distribution, action item completion rates); coaches peer EMs on blameless post-mortem facilitation and incident command technique. Artifact: Org-wide incident management framework with adoption metrics; monthly MTTR and repeat-incident dashboard; error budget governance policy with automated triggers; quarterly cross-team incident retrospective reports. Distinguishing Test: Peer EMs use the incident framework without customization prompts; error budget triggers fire automatically and shift team focus within 24 hours; repeat incidents from known root causes are zero across the org. Company benchmark: Google SRE's blameless post-mortem culture (documented in the SRE book) institutionalizes learning from incidents without blame, while PagerDuty's open-source incident response framework provides a scalable ICS model adopted across the industry.",
    "rationale": "Anchor 1/2: Covers reactive operational excellence (incident command, post-mortems, on-call health). Maps to C8-O1 through C8-O3."
  },
  {
    "anchorId": "C8-2",
    "capabilityId": "C8",
    "sourceTopic": "Systemic Risk Reduction",
    "level1Developing": "Scope: Individual. Key Behavior: Risk management is purely reactive — learns only from production incidents; no pre-launch risk assessment. Artifact: None. Distinguishing Test: Cannot identify the top 3 failure modes in their system.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: Conducts ad hoc risk assessments before major launches; beginning to track vendor dependencies; awareness of compliance requirements. Artifact: Launch checklist exists but not consistently used. Distinguishing Test: Can name critical dependencies but has no tested fallback for any of them.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: FMEA before every major launch; vendor risk registry maintained; compliance requirements mapped to engineering practices; quarterly game days run. Artifact: FMEA documents, vendor risk registry, compliance matrix, game day results. Distinguishing Test: Zero Sev1 incidents on launches; game days reveal and fix gaps proactively.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Risk management practices shared across teams; game days include cross-team failure scenarios; vendor management coordinated at area level. Artifact: Cross-team risk register; shared game day exercises. Distinguishing Test: Cross-team incidents handled smoothly; vendor risk mitigation coordinated rather than duplicated.",
    "level5Advanced": "Scope: Org. Key Behavior: Runs an org-wide resilience testing program with quarterly game days coordinated across teams, rotating failure scenarios from a shared catalog, and publishing results with gap-closure tracking; maintains a cross-org risk register covering vendor dependencies, compliance requirements, and architectural single points of failure reviewed quarterly with engineering leadership; implements error budget policy across the org where budget consumption automatically governs feature/reliability trade-off decisions; audits launch readiness processes across all teams quarterly, calibrating FMEA standards and ensuring consistent coverage; coaches peer EMs on chaos engineering practice, vendor risk assessment, and regulatory compliance integration. Artifact: Org-wide resilience testing program with game day results dashboard; cross-org risk register; error budget policy documentation; quarterly launch readiness audit reports. Distinguishing Test: Game days run across teams without this leader facilitating; risk register reviewed at VP-level quarterly; launch readiness audits catch gaps before launches, not after incidents. Company benchmark: Netflix's Chaos Monkey and Simian Army pioneered chaos engineering as a proactive resilience practice, while Google's DiRT (Disaster Recovery Testing) program runs org-wide failure simulations that build resilience before incidents occur.",
    "rationale": "Anchor 2/2: Covers proactive risk management (failure mode analysis, launch readiness, dependency risk, vendor management, chaos engineering, error budgets). Maps to C8-O4 through C8-O6, C8-O7 (chaos engineering and game days), C8-O8 (error budget management). Intentionally separate from incident response — proactive vs. reactive are distinct sub-skills."
  },
  {
    "anchorId": "C9-1",
    "capabilityId": "C9",
    "sourceTopic": "Metric Selection & Dashboard Design",
    "level1Developing": "Scope: Individual. Key Behavior: No metrics tracked; delivery health is anecdotal; dashboards don't exist or are never consulted. Artifact: None. Distinguishing Test: Cannot state team's deployment frequency or lead time.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Basic DORA metrics tracked; dashboards exist but checked infrequently; metrics used to report status, not drive decisions. Artifact: Basic delivery dashboard. Distinguishing Test: Can state DORA numbers but cannot name a decision driven by metric data in the last quarter.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: DORA metrics tracked weekly with trend analysis; developer satisfaction measured quarterly; metric pairings prevent single-metric gaming; feature flag lifecycle governed. Artifact: Delivery dashboard with trend data; developer satisfaction survey; metric pair documentation. Distinguishing Test: Every active metric has informed a decision within 90 days; no surveillance culture complaints.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Metric frameworks adapted to team maturity; metrics benchmarked across teams; insights shared to improve adjacent teams; metric sophistication evolves based on decisions driven. Artifact: Cross-team metric benchmarks; maturity-appropriate metric recommendations for each team. Distinguishing Test: Teams at different maturity levels have appropriately different metric sets; no death-by-metrics.",
    "level5Advanced": "Scope: Org. Key Behavior: Maintains an org-wide metrics framework selecting from DORA, SPACE, and business-outcome metrics appropriate to each team's maturity level; reviews metric dashboards with engineering leadership monthly, identifying anomalies and recommending interventions; presents engineering metrics alongside business metrics in quarterly business reviews showing the causal chain from engineering investment to business outcomes; audits metric hygiene across all teams quarterly (data freshness, dashboard usage, decision traceability); coaches peer EMs on metric selection, dashboard design, and avoiding Goodhart's Law traps. Artifact: Org-wide metrics framework document with maturity-based metric recommendations; monthly dashboard review notes; quarterly metric hygiene audit results. Distinguishing Test: Business leadership references engineering metrics in planning conversations; metric hygiene audits show >80% of dashboards actively used for decisions, not just reporting. Company benchmark: Google's DORA metrics framework (deployment frequency, lead time, change failure rate, MTTR) provides a validated, industry-standard measurement model, while the SPACE framework adds developer experience dimensions that prevent over-indexing on throughput alone.",
    "rationale": "Anchor 1/2: Covers metric infrastructure (DORA tracking, developer experience, delivery metrics, dashboards, metric pairings, maturity adaptation). Maps to C9-O1 through C9-O3, C9-O7, C9-O9 (metric pairings), C9-O10 (metric maturity adaptation)."
  },
  {
    "anchorId": "C9-2",
    "capabilityId": "C9",
    "sourceTopic": "Data-Driven Decision Making",
    "level1Developing": "Scope: Individual. Key Behavior: OKRs vague or absent; engineering outcomes not translated to business terms; attrition not classified. Artifact: None. Distinguishing Test: Cannot state team's OKRs or explain how engineering work connects to business outcomes.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: OKRs defined but not scored quarterly; some engineering outcomes translated to business terms; attrition tracked but not classified. Artifact: OKR document (not actively scored). Distinguishing Test: OKRs exist on paper but team cannot describe how they influenced a prioritization decision.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Outcome-oriented OKRs scored quarterly with 0.6-0.8 average; engineering outcomes translated to business metrics; attrition classified and patterns addressed; A/B testing with statistical rigor. Artifact: Scored OKRs with quarterly review; business metric translation in investment proposals; attrition classification. Distinguishing Test: >80% of engineering investment proposals include business ROI; regrettable attrition below benchmark.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: OKR quality improved through coaching; business metric translation is standard across teams; experimentation infrastructure supports multiple teams. Artifact: OKR quality rubric; cross-team experimentation platform. Distinguishing Test: Engineering leaders invited to business planning conversations based on metric credibility.",
    "level5Advanced": "Scope: Org. Key Behavior: Runs an org-wide experimentation program with standardized A/B testing protocols, power analysis requirements, and result interpretation guidelines; requires every major engineering investment proposal to include baseline metrics, success criteria, and measurement plan before approval; reviews experiment results with product and business leadership monthly, connecting findings to strategy adjustments; maintains an org-level experiment knowledge base cataloging past experiments, results, and lessons learned; coaches peer EMs on experiment design, metric pairing, and statistical rigor. Artifact: Org-wide experimentation protocol document; investment proposal template with required measurement plan; monthly experiment review reports; experiment knowledge base. Distinguishing Test: Engineering investment proposals are rejected without measurement plans; experiment results inform strategy adjustments within one quarter of completion. Company benchmark: Netflix's experimentation platform enables thousands of concurrent A/B tests with rigorous statistical methodology, while Amazon's 6-pager culture requires investment proposals to include specific metrics, baseline data, and projected outcomes — proposals without quantified success criteria are rejected in review.",
    "rationale": "Anchor 2/2: Covers decision-making with data (OKR measurement, business impact articulation, experimentation, ROI framing). Maps to C9-O4 through C9-O6, C9-O8. Intentionally separate from metric infrastructure — collecting data vs. using data are distinct sub-skills."
  },
  {
    "anchorId": "C10-1",
    "capabilityId": "C10",
    "sourceTopic": "Budget, Headcount & Resource Planning",
    "level1Developing": "Scope: Own team only. Key Behavior: Requests headcount without financial framing; cloud costs not tracked at team level; build-vs-buy decisions are gut feel; budget is someone else's problem. Artifact: None — no cost dashboards, no ROI analyses. Distinguishing Test: Cannot state team's cost-per-engineer-month or cloud spend trend.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Beginning to think about headcount in terms of ROI; starting to track cloud costs at team level; some awareness of build-vs-buy trade-offs. Artifact: Basic cloud cost tracking; initial headcount justification docs. Distinguishing Test: Can state team cloud costs but cannot explain cost-per-feature or ROI of recent hires.",
    "level3Competent": "Scope: Team + finance partnership. Key Behavior: Headcount justified with ROI analysis; cloud costs attributed and optimized per service; build-vs-buy uses TCO framework; contractor strategy is principled; budget defense prepared for cuts. Artifact: Headcount ROI docs; per-service cost dashboards; TCO analyses; contractor strategy doc. Distinguishing Test: Can show a build-vs-buy decision with documented TCO analysis and a headcount request that survived finance review.",
    "level4Distinguished": "Scope: Cross-team financial influence. Key Behavior: Headcount proposals connect to business outcomes with multi-scenario ROI modeling; cloud cost attribution drives team-level accountability; build-vs-buy decisions documented with reusable TCO analysis; finance partners proactively consult on engineering investment questions. Artifact: Multi-scenario ROI models; team-owned cost dashboards; reusable TCO templates; evidence of finance partnership. Distinguishing Test: Finance partners seek out this leader for investment framing advice — not just approve requests but proactively consult.",
    "level5Advanced": "Scope: Org-wide financial stewardship. Key Behavior: Presents org-level headcount narrative to VP/finance with multi-scenario ROI modeling; runs quarterly FinOps reviews across all teams with per-service cost dashboards and optimization targets; frames every engineering investment as a business case with payback period and opportunity cost; coaches peer leaders on budget defense and ROI framing. Artifact: Org-wide FinOps review outputs; per-team cost dashboards with anomaly detection; documented savings reinvested into highest-ROI initiatives with finance sign-off. Distinguishing Test: Teams self-allocate within strategic guardrails and present their own cost curves rather than receiving top-down mandates — cost discipline is distributed, not centralized. Company benchmark: Netflix's 'highly aligned, loosely coupled' model lets teams self-allocate within strategic guardrails, and Amazon's FinOps culture treats cost as a first-class engineering constraint.",
    "rationale": "Anchor 1/2: Covers financial stewardship (headcount justification, cloud costs, build-vs-buy, contractor strategy). Maps to C10-O1, C10-O3, C10-O4, C10-O5."
  },
  {
    "anchorId": "C10-2",
    "capabilityId": "C10",
    "sourceTopic": "Strategic Reallocation & ROI Framing",
    "level1Developing": "Scope: Own team only. Key Behavior: Resources stay where assigned regardless of changing priorities; no framework for comparing ROI across investments; reacts to cuts rather than proactively optimizing. Artifact: None — no tiered proposals, no cost-per-outcome tracking. Distinguishing Test: Cannot explain the ROI difference between their top and bottom investment areas.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Beginning to evaluate resource allocation against impact; some awareness that reallocation is a tool; starting to frame engineering investments in outcome terms. Artifact: Basic investment tracking; initial outcome-framed proposals. Distinguishing Test: Can identify which investment area has lowest ROI but hasn't acted on it yet.",
    "level3Competent": "Scope: Team + leadership interface. Key Behavior: Presents resource requests as tiered options with quantified trade-offs; tracks cost-per-outcome; proactively reallocates capacity to highest-impact work; during cuts, communicates explicitly what stops. Artifact: Tiered proposals with quantified trade-offs; cost-per-outcome dashboards; documented reallocation decisions. Distinguishing Test: Can show a proactive reallocation decision made before leadership requested it — with outcome data justifying the shift.",
    "level4Distinguished": "Scope: Cross-team allocation influence. Key Behavior: Tiered resource proposals adopted as default format; cost-per-outcome data actively informs quarterly reallocation decisions; reallocation happens within boundaries without exec approval; adjacent teams adopt the framework. Artifact: Quarterly reallocation reports; tiered proposal templates used by peers; cost-per-outcome trend data. Distinguishing Test: Adjacent teams have adopted the tiered proposal framework — influence extends beyond own scope.",
    "level5Advanced": "Scope: Org-wide resource optimization. Key Behavior: Presents resource allocation proposals to VP/finance with multi-scenario projections and quantified opportunity costs; runs monthly cost-per-outcome reviews across all teams, initiating reallocation within one week of diminishing returns; designs tiered proposal templates adopted by peer leaders; identifies reallocation opportunities proactively by comparing planned-vs-actual impact quarterly. Artifact: Org-wide cost-per-outcome reviews; reallocation decision log with outcome tracking; peer-adopted templates; documented savings reinvested into highest-ROI initiatives. Distinguishing Test: Monthly reviews compare cost-per-outcome across teams and savings are reinvested into highest-ROI initiatives — resource optimization is a continuous process, not an annual exercise. Company benchmark: Amazon's FinOps culture treats cost as a first-class engineering constraint, with monthly business reviews comparing cost-per-outcome across teams and reinvesting savings into highest-ROI initiatives.",
    "rationale": "Anchor 2/2: Covers dynamic allocation (tiered proposals, ROI tracking, proactive reallocation, cut communication). Maps to C10-O2, C10-O6, C10-O7, C10-O8. Intentionally separate from financial stewardship — static budgeting vs. dynamic allocation are distinct sub-skills."
  },
  {
    "anchorId": "C11-1",
    "capabilityId": "C11",
    "sourceTopic": "Hiring Process & Bar Raising",
    "level1Developing": "Scope: Participates in hiring. Key Behavior: Joins hiring loops without training; onboarding is informal; headcount requests are 'we need more people'; no engineering brand presence. Artifact: None — no rubrics, no onboarding docs, no capacity model. Distinguishing Test: Cannot describe the interview rubric or what 'bar' means for their team.",
    "level2Emerging": "Scope: Own team hiring (improving). Key Behavior: Hiring loops becoming more structured; some onboarding documentation exists; starting to justify headcount with basic data; referral network emerging. Artifact: Basic interview guides; initial onboarding docs. Distinguishing Test: Has interview guides but interviewers interpret them differently — no calibration.",
    "level3Competent": "Scope: Team hiring with process rigor. Key Behavior: Calibrated hiring loops with trained interviewers and rubrics; structured interviews evaluate demonstrated competencies over pedigree; headcount justified with capacity model; bar raiser discipline maintained under pressure; hiring funnel conversion tracked. Artifact: Interview rubrics with behavioral anchors; calibrated scoring; capacity model linking headcount to deliverables; funnel metrics. Distinguishing Test: Two interviewers evaluating the same candidate produce consistent scores — inter-rater reliability is tracked.",
    "level4Distinguished": "Scope: Cross-team hiring influence. Key Behavior: Hiring bar maintained under volume pressure with quarterly interview calibration sessions; structured process produces consistent signal across interviewers; headcount narrative connects capacity to business strategy with revenue/risk impact; diverse candidate slates through structured sourcing. Artifact: Quarterly calibration session records; inter-rater reliability data; business-framed headcount narratives; diversity pipeline metrics. Distinguishing Test: Can show calibration data proving interviewer consistency improved over time — not just claimed but measured.",
    "level5Advanced": "Scope: Org-wide hiring standards. Key Behavior: Runs an org-wide interviewer calibration program with quarterly calibration sessions, inter-rater reliability scoring, and shadow-interview requirements for new interviewers; appoints independent bar raisers with veto power on every hiring loop above a defined seniority threshold; presents headcount proposals to VP with role-impact justification, pipeline data, and market compensation analysis; maintains and publishes hiring funnel conversion metrics (source-to-screen, screen-to-onsite, onsite-to-offer, offer-to-accept) monthly with bottleneck analysis; coaches peer EMs on structured interviewing technique and bias reduction practices. Artifact: Org-wide interviewer calibration program with quarterly session records and inter-rater reliability data; bar raiser veto log; monthly hiring funnel conversion dashboard with bottleneck analysis. Distinguishing Test: Bar raisers exercise veto power independently; inter-rater reliability scores improve quarter over quarter; hiring funnel bottlenecks are identified and addressed within one hiring cycle. Company benchmark: Amazon's Bar Raiser program gives independent interviewers veto power to protect hiring quality, and Google's structured hiring committees use calibrated rubrics to ensure consistent signal across interviewers.",
    "rationale": "Anchor 1/2: Covers hiring mechanics (interview loops, bar raising, headcount justification, interviewer calibration, structured interviews, competitive hiring strategy, funnel optimization). Maps to C11-O1, C11-O2, C11-O7 through C11-O11."
  },
  {
    "anchorId": "C11-2",
    "capabilityId": "C11",
    "sourceTopic": "Onboarding Excellence & Talent Brand",
    "level1Developing": "Scope: Own team only. Key Behavior: Onboarding is informal — new hires figure it out; no engineering brand presence; time-to-productivity not tracked; no referral pipeline. Artifact: None — no onboarding docs, no brand content. Distinguishing Test: New hires take 3+ months to be productive and cannot articulate what their first 30 days should have looked like.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Basic onboarding documentation exists; some buddy pairing; beginning to track new hire experience; occasional blog post or meetup participation. Artifact: Basic onboarding checklist; buddy assignments. Distinguishing Test: Onboarding exists but is inconsistent between hires — no measurable milestones.",
    "level3Competent": "Scope: Team onboarding + emerging brand. Key Behavior: Structured 30/60/90 onboarding with measurable milestones; time-to-productivity tracked and improving; buddy/mentor program formalized; active blog or conference presence emerging; referral pipeline maintained. Artifact: 30/60/90 plan template; time-to-productivity data; blog posts or talks; referral tracking. Distinguishing Test: Can show time-to-productivity improving quarter-over-quarter with specific bottlenecks identified and addressed.",
    "level4Distinguished": "Scope: Cross-team brand influence. Key Behavior: Onboarding continuously refined using new hire feedback surveys at 30/60/90 days; time-to-productivity benchmarked against org averages; engineering brand built through conference talks, blog posts, or open source; referral rate among highest on team. Artifact: New hire feedback surveys with action items; benchmark comparison data; published engineering content; referral metrics. Distinguishing Test: Engineering brand generates inbound senior talent interest — candidates cite team content in interviews.",
    "level5Advanced": "Scope: Org-wide onboarding model. Key Behavior: Measures time-to-productivity for every new hire against role-specific milestones and publishes cohort data quarterly with improvement actions; runs a structured mentorship program pairing new hires with experienced engineers for the first 60 days with weekly check-ins; maintains an engineering brand program (tech blog cadence, conference talk pipeline, open source contribution guidelines) with tracked inbound application rates; collects onboarding feedback at 30/60/90 days and iterates the program each cohort based on quantitative satisfaction scores; coaches peer EMs on onboarding design and talent brand building. Artifact: Quarterly time-to-productivity cohort reports with improvement actions; structured mentorship program with pairing records and check-in cadence; engineering brand metrics dashboard (blog posts, talks, open source contributions, inbound application rates). Distinguishing Test: Time-to-productivity improves cohort over cohort with documented program changes driving the improvement; onboarding feedback scores trend upward each quarter. Company benchmark: Meta's Bootcamp onboarding program gets new engineers productive within 6 weeks through structured mentorship, and Google's engineering brand (publications, open source, tech talks) drives inbound senior talent.",
    "rationale": "Anchor 2/2: Covers talent brand and onboarding (30/60/90 programs, time-to-productivity, engineering brand, referrals). Maps to C11-O3 through C11-O6. Intentionally separate from hiring mechanics — acquisition vs. integration are distinct sub-skills."
  },
  {
    "anchorId": "C12-1",
    "capabilityId": "C12",
    "sourceTopic": "Team Charter & Engineering Principles",
    "level1Developing": "Scope: Own team only. Key Behavior: Team culture exists by accident; recognition is sporadic; inclusion is 'we treat everyone the same.' Artifact: None — no team charter, no written norms. Distinguishing Test: Cannot articulate the team's operating norms or explain why the team behaves the way it does.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Starting to be intentional about team norms; some recognition happening but not systematized; awareness of inclusion beyond 'same treatment'; beginning to document engineering practices. Artifact: Draft team norms doc; occasional recognition. Distinguishing Test: Can name desired team norms but they aren't written or consistently reinforced.",
    "level3Competent": "Scope: Team with documented culture. Key Behavior: Team charter written and referenced; engineering principles documented and used in design reviews; regular recognition rituals; active inclusion practices; explicit feedback channels; cultural continuity maintained through growth; toxic behaviors addressed within 48 hours. Artifact: Team charter; engineering principles doc; recognition log; inclusion metrics. Distinguishing Test: New hires can find the team charter in their first week and reference it when navigating team norms.",
    "level4Distinguished": "Scope: Cross-team cultural influence. Key Behavior: Team charter reviewed and evolved quarterly based on feedback; engineering principles referenced in code review and architecture decisions; recognition operates peer-to-peer without manager initiation; inclusion practices produce measurable engagement survey results with action plans. Artifact: Quarterly charter review records; peer recognition log; engagement survey results with action items; gap analysis reports. Distinguishing Test: Adjacent teams have adopted cultural practices (recognition rituals, engineering principles, feedback norms) initiated by this leader — cultural influence extends beyond own team.",
    "level5Advanced": "Scope: Org-wide culture model. Key Behavior: Runs quarterly team health checks using structured surveys (psychological safety, inclusion, feedback quality) and publishes results with action plans; codifies engineering principles in a written culture document updated annually with team input and referenced in hiring, onboarding, and performance conversations; addresses toxic behaviors within one week of identification using a structured intervention protocol (1:1 conversation, documented expectations, follow-up timeline); conducts skip-level listening sessions quarterly to surface cultural drift early; coaches peer EMs on psychological safety practices, inclusive facilitation, and norm reinforcement technique. Artifact: Quarterly team health check results with action plans; written engineering principles document with annual review history; toxic behavior intervention log; skip-level listening session notes. Distinguishing Test: Team health check scores improve or remain stable through leadership transitions and team growth; toxic behaviors are addressed within one week of identification, not allowed to persist. Company benchmark: Netflix's Freedom and Responsibility culture doc is the canonical example of intentional culture at scale, and Spotify's team Health Checks make culture measurable and discussable rather than abstract.",
    "rationale": "Anchor 1/2: Covers culture foundations (team charter, engineering principles, recognition, inclusion, cultural continuity, inclusive feedback channels, toxic behavior intervention). Maps to C12-O1, C12-O2, C12-O4 through C12-O8."
  },
  {
    "anchorId": "C12-2",
    "capabilityId": "C12",
    "sourceTopic": "Knowledge Sharing & Documentation Culture",
    "level1Developing": "Scope: Own team only. Key Behavior: Knowledge sharing informal — tribal knowledge dominates; documentation sparse and outdated; no structured forums for cross-team learning. Artifact: None — no documentation standards, no tech talks. Distinguishing Test: Ask 3 team members 'where is X documented?' and get 3 different answers (or 'ask person Y').",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Some documentation emerging but inconsistent; occasional knowledge-sharing sessions; beginning to notice bus-factor risk from tribal knowledge. Artifact: Scattered docs; occasional sessions. Distinguishing Test: Documentation exists but team members don't know where to find it or whether it's current.",
    "level3Competent": "Scope: Team with documentation culture. Key Behavior: Documentation standards maintained and enforced; regular tech talks or knowledge-sharing rituals; design doc culture established; onboarding documentation reduces ramp time measurably. Artifact: Documentation standards; regular tech talk schedule; design doc archive; measurable onboarding ramp improvement. Distinguishing Test: New hires can answer most common questions from docs without asking a person — tribal knowledge converted to institutional knowledge.",
    "level4Distinguished": "Scope: Cross-team knowledge influence. Key Behavior: Documentation updated alongside code changes; design doc quality validated through structured peer review; knowledge-sharing sessions run with rotating ownership; documentation standards shared with adjacent teams. Artifact: PR-linked doc updates; peer review records for design docs; rotating presentation schedule; adjacent team adoption. Distinguishing Test: Documentation updates happen in the same PR as code changes — not as a separate task or afterthought.",
    "level5Advanced": "Scope: Org-wide knowledge culture. Key Behavior: Runs an org-wide knowledge sharing program with scheduled tech talks (bi-weekly cadence), design doc requirements for all major decisions, and a searchable wiki with freshness SLOs (>90% of critical docs updated within past quarter); tracks bus factor per team quarterly using a structured assessment and initiates cross-training for any area with bus factor of 1; maintains documentation coverage metrics per team and publishes quarterly with improvement targets; requires design docs for all decisions above a defined threshold (>1 eng-month or cross-team scope); coaches peer EMs on writing culture, design doc standards, and knowledge transfer practices. Artifact: Org-wide tech talk schedule with bi-weekly cadence; documentation freshness dashboard with SLO tracking; quarterly bus factor assessment per team; design doc coverage metrics. Distinguishing Test: Design docs exist for all major decisions without this leader requesting them; bus factor of 1 is zero across the org; documentation freshness SLOs are met consistently. Company benchmark: Stripe's writing culture and Amazon's 6-pager requirement institutionalize knowledge sharing as an organizational practice — decisions are documented by default, not by individual initiative.",
    "rationale": "Anchor 2/2: Covers knowledge management (documentation, tech talks, design docs, cross-team learning). Maps to C12-O3. Intentionally separate from culture foundations — knowledge sharing is a distinct operational practice."
  },
  {
    "anchorId": "C13-1",
    "capabilityId": "C13",
    "sourceTopic": "Security Practices & Vulnerability Management",
    "level1Developing": "Scope: Own team only. Key Behavior: Ensures team follows security practices when reminded; patches vulnerabilities reactively; change management informal; compliance handled last-minute. Artifact: None — no threat models, no automated scanning, no vulnerability SLA tracking. Distinguishing Test: Cannot state the team's vulnerability backlog count or time-to-patch for the last critical CVE.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Security practices becoming more consistent without reminders; vulnerability patching improving but SLA sometimes missed; change management process emerging. Artifact: Basic scanning in CI; initial vulnerability tracking. Distinguishing Test: Vulnerability scanning exists but SLA compliance is inconsistent — some criticals take weeks to patch.",
    "level3Competent": "Scope: Team with security workflow. Key Behavior: Security embedded in development workflow (threat modeling, automated scanning, champion rotation); vulnerability SLAs met consistently (P0 <48hrs, P1 <7 days); threat models required for auth/payments/PII. Artifact: Security champion roster; CI/CD scanning config; vulnerability SLA dashboard; threat model archive. Distinguishing Test: Can show vulnerability SLA compliance data for the last quarter and a threat model for the most recent sensitive feature.",
    "level4Distinguished": "Scope: Cross-team security influence. Key Behavior: Engineers initiate threat models without prompting; vulnerability SLAs met with mean resolution time trending downward; security champion program produces engineers who improve practices independently; change management process influencing adjacent teams. Artifact: Mean resolution time trend data; champion-initiated improvements; adjacent team adoption evidence. Distinguishing Test: Engineers proactively start threat models for sensitive features — security is a cultural norm, not a manager-enforced requirement.",
    "level5Advanced": "Scope: Org-wide security culture. Key Behavior: Runs an org-wide security champion program with champions in every team attending a central security guild bi-weekly; sets org-level security standards (mandatory SAST/DAST in all CI pipelines, threat modeling required for any feature touching auth/payments/PII) and audits compliance quarterly; publishes org-wide vulnerability metrics monthly showing posture trends per team; conducts annual threat landscape review across the org identifying emerging risk categories and updating security standards accordingly; coaches peer EMs on security workflow integration and threat modeling facilitation. Artifact: Org-wide security standards document with quarterly audit results; security champion guild roster and meeting cadence; monthly vulnerability posture dashboard per team; annual threat landscape review. Distinguishing Test: Security champions in every team catch design-level vulnerabilities without escalation; org-level vulnerability posture trends downward quarter over quarter; threat modeling happens without this leader prompting it. Company benchmark: Amazon's mandatory security review process requires threat models for every service handling sensitive data, and Google's Project Zero demonstrates that proactive vulnerability research creates security culture beyond compliance.",
    "rationale": "Anchor 1/2: Covers security operations (threat modeling, vulnerability management, change management, security champions, automated scanning, threat modeling for sensitive features). Maps to C13-O1 through C13-O3, C13-O5 through C13-O7."
  },
  {
    "anchorId": "C13-2",
    "capabilityId": "C13",
    "sourceTopic": "Compliance & Governance Automation",
    "level1Developing": "Scope: Own team only. Key Behavior: Compliance handled last-minute before audits; evidence collection manual; access controls inconsistent; no automated compliance checks. Artifact: None — no automated evidence, no access governance process. Distinguishing Test: Audit preparation takes weeks and involves scrambling for evidence that should already exist.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Beginning to embed compliance into workflows; some automated scanning; starting to track access control hygiene; audit preparation moving earlier. Artifact: Basic automated scanning; initial access tracking. Distinguishing Test: Some evidence is automated but audit prep still takes days — manual gaps remain.",
    "level3Competent": "Scope: Team with compliance automation. Key Behavior: Continuous compliance posture with automated evidence collection; access governance automated (quarterly reviews, auto-expiring elevated permissions); audit prep reduced from weeks to days. Artifact: Automated evidence collection pipeline; access governance system with auto-expiry; audit prep checklist. Distinguishing Test: Audit prep takes days, not weeks — evidence is collected continuously rather than assembled retroactively.",
    "level4Distinguished": "Scope: Cross-team compliance influence. Key Behavior: Compliance evidence collected automatically with real-time dashboard — audit preparation requires hours, not days; access governance runs exception-based reviews only; zero repeat findings across consecutive audit cycles; governance automation patterns shared with adjacent teams. Artifact: Real-time compliance dashboard; exception-based access review records; zero repeat findings evidence. Distinguishing Test: Zero repeat findings across consecutive audits — every finding gets a systemic fix, not just a point correction.",
    "level5Advanced": "Scope: Org-wide compliance model. Key Behavior: Maintains automated compliance evidence collection pipelines for all certification controls across the org, generating evidence as a byproduct of engineering workflow; runs monthly internal mini-audits spot-checking 3-5 controls to catch drift before external audits; presents compliance posture to VP/CISO monthly using a live dashboard showing control coverage, evidence freshness, and access governance metrics; designs and deploys governance automation templates that peer teams adopt as their standard compliance infrastructure; coaches engineering leadership on continuous compliance methodology and access governance automation. Artifact: Automated evidence collection pipelines with coverage metrics; monthly mini-audit results; live compliance posture dashboard; governance automation templates with adoption tracking. Distinguishing Test: Compliance evidence is generated continuously without manual effort; mini-audits catch drift before external auditors do; governance automation templates are adopted by peer teams without mandate. Company benchmark: Netflix automates compliance evidence collection continuously, making audit readiness a dashboard metric rather than a quarterly project.",
    "rationale": "Anchor 2/2: Covers compliance and governance (audit readiness, evidence collection, access governance, compliance automation). Maps to C13-O3, C13-O4. Intentionally separate from security operations — compliance is a distinct discipline from vulnerability management."
  },
  {
    "anchorId": "C14-1",
    "capabilityId": "C14",
    "sourceTopic": "Performance Reviews & Calibration",
    "level1Developing": "Scope: Own team only. Key Behavior: Writes vague reviews without specific evidence; calibration prep is last-minute; avoids PIPs entirely; career conversations are infrequent. Artifact: None — reviews lack specific examples or rubric alignment. Distinguishing Test: Cannot cite specific evidence for any rating given.",
    "level2Emerging": "Scope: Own team with HR support. Key Behavior: Writes reviews with improving specificity; prepares for calibration earlier; begins addressing performance issues with HR guidance; career conversations becoming regular. Artifact: Reviews include some specific examples but lack consistent rubric alignment. Distinguishing Test: Can cite examples but cannot defend ratings against calibration committee questioning.",
    "level3Competent": "Scope: Own team with cross-team calibration awareness. Key Behavior: Writes evidence-based reviews with rubric language; maintains weekly running notes; prepares calibration cases with cross-team data; executes PIPs with HR partnership; differentiates high performer development; conducts managed exits with dignity. Artifact: Weekly performance notes per report; calibration prep docs with cross-team comparisons; quarterly career conversation records. Distinguishing Test: Reviews are evidence-based with rubric alignment — calibration prep is routine, not heroic.",
    "level4Distinguished": "Scope: Cross-team calibration influence. Key Behavior: Calibration cases withstand committee scrutiny with ratings rarely revised; manages full performance spectrum — stretch assignments for high performers, structured PIPs for underperformers, dignified exits with knowledge transfer; career conversations produce written development plans. Artifact: Calibration prep with pre-built counter-arguments for contested ratings; written development plans referenced in subsequent reviews. Distinguishing Test: Ratings rarely require revision in calibration — committee trusts the evidence quality.",
    "level5Advanced": "Scope: Org-wide calibration leadership. Key Behavior: Reviews are referenced as examples by peer managers; presents calibration cases with pre-built counter-arguments for every contested rating; runs pre-calibration alignment sessions to surface bias; creates differentiated retention plans with quarterly evidence portfolios; executes managed exits with 24-hour team communication; coaches peer EMs on calibration technique. Artifact: Review templates adopted by peer managers; pre-calibration alignment session records; retention plans with quarterly evidence portfolio reviews; managed exit checklists with knowledge transfer documentation. Distinguishing Test: Peer managers seek out calibration coaching — review preparation takes days not weeks because continuous documentation is habitual. Company benchmark: Google's calibration committee model separates promotion decisions from direct managers, ensuring cross-team consistency and reducing individual bias in performance assessment.",
    "rationale": "Anchor 1/2: Covers performance operations (reviews, calibration, feedback, PIPs, continuous documentation, high performer management, managed exits). Maps to C14-O1 through C14-O4, C14-O6 through C14-O9."
  },
  {
    "anchorId": "C14-2",
    "capabilityId": "C14",
    "sourceTopic": "Promotion Readiness & Career Architecture",
    "level1Developing": "Scope: Own team only. Key Behavior: Writes reactive promotion packets without systematic evidence building; no gap-filling stretch assignments planned; promotion cases lack rubric alignment. Artifact: None — promotion packets assembled last-minute without deliberate evidence collection. Distinguishing Test: Cannot explain what specific evidence gaps exist for any promotion candidate.",
    "level2Emerging": "Scope: Own team with emerging committee awareness. Key Behavior: Plans promotions 1-2 cycles ahead; starts identifying evidence gaps and assigning stretch work; developing awareness of committee expectations. Artifact: Basic promotion timeline for candidates; some stretch assignments mapped to evidence gaps. Distinguishing Test: Can name specific evidence gaps for promotion candidates but gap-filling is ad hoc rather than systematic.",
    "level3Competent": "Scope: Cross-team evidence gathering. Key Behavior: Builds promotion cases over 2-3 cycles with deliberate gap-filling stretch assignments; gathers cross-team evidence systematically; structures cases with specific impact matching rubric criteria. Artifact: Multi-cycle promotion tracking documents; stretch assignment plans mapped to rubric gaps; structured promotion packets with cross-team evidence. Distinguishing Test: Promotion packets include evidence from multiple cycles and cross-team sources — not a single-cycle snapshot.",
    "level4Distinguished": "Scope: Multi-team influence on career architecture. Key Behavior: Promotion success rate demonstrates calibration skill — committees approve without extensive debate; stretch assignments deliberately designed for next-level evidence; career development reputation attracts internal transfers; coaches peer EMs on promotion readiness. Artifact: Evidence portfolios with multi-cycle tracking; stretch assignment designs mapped to specific rubric criteria; peer coaching frameworks for promotion readiness. Distinguishing Test: Internal transfers cite career development as a reason for joining — promotion success rate is consistently high without committee pushback.",
    "level5Advanced": "Scope: Org-wide career architecture ownership. Key Behavior: Builds promotion cases 3+ cycles ahead with monthly evidence tracking; maps candidates against every rubric criterion with deliberate gap-filling; gets pre-reads from committee members one cycle before submission; maintains career architecture documents for IC and management tracks; coaches peer EMs on evidence quality and packet construction. Artifact: Shared evidence tracking documents updated monthly per candidate; career architecture documents reviewed annually with Staff+ engineers; promotion packet templates and evidence quality rubrics used by peer managers. Distinguishing Test: Promotion cases are pre-validated through committee pre-reads — packets require minimal debate because evidence gaps were filled systematically over multiple cycles. Company benchmark: Amazon's promotion process requires documented evidence of sustained next-level impact over multiple review cycles, and Google's promo committees evaluate packets independently of manager advocacy.",
    "rationale": "Anchor 2/2: Covers career growth architecture (promotion planning, stretch assignments, evidence building, career paths). Maps to C14-O5. Intentionally separate from performance operations — evaluation vs. development are distinct sub-skills."
  }
]