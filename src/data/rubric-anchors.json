[
  {
    "anchorId": "C1-1",
    "capabilityId": "C1",
    "sourceTopic": "Org Design & Team Topologies",
    "level1Developing": "Scope: Individual. Key Behavior: Operates within their team's boundaries without understanding broader org context; cannot articulate how their work connects to other teams or strategic outcomes. Artifact: None. Distinguishing Test: Cannot name the top 3 dependencies their team has on other teams.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Understands basic team topology and some dependencies; participates in cross-team meetings but doesn't drive decisions. Artifact: Team dependency diagram exists but not actively maintained. Distinguishing Test: Can list their team's key dependencies but cannot describe why the org is structured this way.",
    "level3Competent": "Scope: Team/Area (2-3 teams). Key Behavior: Owns team structure and topology for their area; recognizes re-org signals early; executes team transitions with clear communication. Artifact: Updated team topology registry for area; re-org communication plans. Distinguishing Test: Team can explain org structure rationale; re-org execution produces zero regrettable attrition; coordination overhead <10%.",
    "level4Distinguished": "Scope: Area (3-5 teams). Key Behavior: Designs org structure for multiple teams using stream-aligned principles; applies inverse Conway to align org and architecture; coaches peer managers on topology decisions. Artifact: Cross-team topology strategy; architectural alignment proposals. Distinguishing Test: Org decisions drive architecture improvements; peer managers seek input on topology changes; cross-team coordination overhead measurably decreases.",
    "level5Advanced": "Scope: Org. Key Behavior: Org structure is a strategic asset aligned with business outcomes and technical architecture; senior leadership uses org structure as a primary lever for change; teams can scale without creating dysfunction. Artifact: Org-wide topology strategy integrated with technology roadmap; documented org design principles. Distinguishing Test: Multiple re-orgs executed flawlessly with zero attrition; org structure enables scaling without adding management layers; architecture and org reinforce each other.",
    "rationale": "Anchor 1/2: Covers structural design (team splits/merges, ownership, Conway's Law). Maps to C1-O1 through C1-O5."
  },
  {
    "anchorId": "C1-2",
    "capabilityId": "C1",
    "sourceTopic": "Cross-Team Strategy & Long-Horizon Planning",
    "level1Developing": "Scope: Individual. Key Behavior: Ownership is ambiguous or shared; 'who decides for this service' is a frequent question; escalations happen frequently because authority is unclear. Artifact: None. Distinguishing Test: Cannot state who owns 50% of their team's key dependencies.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: Team ownership is clear but cross-team ownership disputes take >24 hours to resolve; some DRIs named for key decisions. Artifact: Team owns their services clearly; broader ownership has gaps. Distinguishing Test: Most people know who owns their team's services, but disputes with other teams about shared areas take meetings to resolve.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Single-threaded ownership registry maintained and consulted; zero ambiguity about who decides for any service or domain; disputes resolved through clear escalation path within hours. Artifact: Service→team→contact registry; explicitly documented DRIs for cross-team domains. Distinguishing Test: Any engineer can answer 'who decides about X?' within 30 seconds for 100% of critical services.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Ownership model extends across multiple teams with rare disputes; shared services have clear escalation procedures and named arbiters. Artifact: Cross-team ownership agreements with decision rights documented; escalation flowcharts. Distinguishing Test: Ownership disputes across teams resolved in <4 hours; teams coordinate without explicit permission because authority is transparent.",
    "level5Advanced": "Scope: Org. Key Behavior: Org-wide single-threaded ownership culture; ambiguity is rare; decisions made by clear DRIs even in complex, cross-cutting scenarios. Artifact: Org-wide ownership registry used as ground truth; ownership model taught to new leaders as standard. Distinguishing Test: Org-wide decisions that span multiple teams happen without coordination overhead; escalation is rare because ownership is universally understood.",
    "rationale": "Anchor 2/2: Covers strategic leadership altitude (multi-quarter planning, cross-team alignment, org-level influence, strategy-to-team translation). Maps to C1-O6 through C1-O16. Intentionally separate from structural design — strategy vs. structure are distinct sub-skills."
  },
  {
    "anchorId": "C2-1",
    "capabilityId": "C2",
    "sourceTopic": "Strategic Alignment & Roadmapping",
    "level1Developing": "Scope: Individual. Key Behavior: Participates in planning but doesn't lead or influence priorities; treats roadmap as handed down from above. Artifact: None — no documented planning. Distinguishing Test: Cannot articulate the rationale behind current quarter's priorities.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Documents roadmap and gets stakeholder sign-off, but planning is ad hoc; priorities shift mid-quarter without formal re-evaluation; technical debt needs aren't surfaced in planning. Artifact: Basic roadmap document. Distinguishing Test: Can describe current roadmap but cannot articulate trade-offs that were made.",
    "level3Competent": "Scope: Team (proactive). Key Behavior: Structured quarterly planning with documented inputs and outputs; monthly re-prioritization gates; tech debt allocated and tracked; roadmap delivery 80%+. Artifact: Planning process doc; roadmap with business outcome mapping; monthly re-prioritization meeting notes. Distinguishing Test: Stakeholders can articulate the prioritization criteria; engineers understand why they're allocated as they are; roadmap delivery consistent.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Planning discipline scaled across teams with consistent prioritization framework; strategic alignment measurable quarterly; technical feasibility gates prevent mid-sprint surprises. Artifact: Cross-team planning framework; strategic theme mapping; pre-execution design review documentation. Distinguishing Test: Planning process shared as best practice; other EMs adopt the framework; roadmap delivery maintained >80% during organizational change.",
    "level5Advanced": "Scope: Org. Key Behavior: Planning culture institutionalized org-wide; roadmap metrics drive continuous planning improvement; strategic and quarterly rhythms well-defined and executed at scale. Artifact: Org-wide planning framework with measured outcomes; strategic roadmap updated annually with quarterly adjustments. Distinguishing Test: Business leadership cites engineering's planning credibility when discussing investment decisions; planning artifacts shape board-level conversations.",
    "rationale": "Anchor 1/2: Covers strategic planning mechanics (planning process, OKRs, roadmapping, capacity models). Maps to C2-O1, C2-O5, C2-O6."
  },
  {
    "anchorId": "C2-2",
    "capabilityId": "C2",
    "sourceTopic": "Trade-Off Discipline & Decision Rigor",
    "level1Developing": "Scope: Individual. Key Behavior: Trade-offs made arbitrarily; declined requests feel personal to stakeholders; rationale not documented; same requests recur quarterly. Artifact: None. Distinguishing Test: Stakeholder cannot explain why their request was deferred; feels like EM said 'no' without hearing them.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Some prioritization scoring attempted; trade-offs explained informally; stakeholder satisfaction with trade-off process still low; scoring criteria not consistent. Artifact: Basic prioritization scorecard (inconsistently applied). Distinguishing Test: Scoring exists but stakeholders experienced different criteria for different requests.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Explicit prioritization scorecard with consistent criteria; trade-offs presented with impact/effort/ROI analysis; stakeholders involved in trade-off discussions; same request not resurfaces repeatedly. Artifact: Documented prioritization scorecard; decision log with rationale for each deferred initiative. Distinguishing Test: Stakeholders can articulate the scoring criteria; deferred requests stay deferred unless context changes.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Prioritization framework refined from delivery experience; trade-off patterns analyzed quarterly for systemic insights; prioritization consistency ensured across teams. Artifact: Prioritization framework with calibration notes; cross-team decision consistency analysis. Distinguishing Test: Different teams use same prioritization framework and get consistent outcomes; stakeholders benchmark their requests against other teams' initiatives.",
    "level5Advanced": "Scope: Org. Key Behavior: Prioritization discipline becomes organizational norm; business and engineering share prioritization framework; strategic decisions made visibly with trade-off data. Artifact: Org-wide prioritization methodology with decision impact tracking. Distinguishing Test: Board-level decisions reference engineering prioritization trade-offs; prioritization framework shapes exec-level conversations about resource allocation.",
    "rationale": "Anchor 2/2: Covers prioritization discipline (saying no, trade-off framing, decision rigor, first principles thinking). Maps to C2-O2, C2-O3, C2-O4. Intentionally separate from planning mechanics — strategic discipline vs. process execution are distinct sub-skills."
  },
  {
    "anchorId": "C3-1",
    "capabilityId": "C3",
    "sourceTopic": "Technical Strategy & System Ownership",
    "level1Developing": "Scope: Individual. Key Behavior: Follows technical direction set by TL/Staff without contributing architectural input. Artifact: None — relies on others' design docs. Distinguishing Test: Cannot identify technical trade-offs in their team's system without prompting.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Participates in design reviews and forms technical opinions; begins advocating for system health based on data. Artifact: Written comments on design docs with specific technical concerns. Distinguishing Test: Can explain their team's architecture and identify one concrete improvement, but hasn't driven it.",
    "level3Competent": "Scope: Team (proactive). Key Behavior: Co-authors tech strategy with TL/Staff; runs design reviews with clear criteria; makes build-vs-buy decisions with TCO analysis. Artifact: Tech strategy doc with current→target→migration path; searchable decision log. Distinguishing Test: Design review criteria exist as a written checklist; build-vs-buy decisions include documented TCO.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Architecture decisions account for cross-team system interactions; design review criteria refined from post-mortems; influences platform direction through written proposals. Artifact: Cross-team architecture proposals with data-backed justification; post-mortem-derived review criteria. Distinguishing Test: Peer EMs seek technical input on their architecture decisions; design review improvements trace to specific production learnings.",
    "level5Advanced": "Scope: Org. Key Behavior: Sets multi-quarter technical vision adopted across teams; architecture review process institutionalized; drives platform thinking. Artifact: Org-wide technical strategy doc referenced by multiple teams; institutionalized review process with measured outcomes. Distinguishing Test: Technical governance scales across teams without creating bottlenecks; vision doc updated quarterly with delivery reality checks.",
    "rationale": "Anchor 1/2: Covers technical vision, design reviews, and system ownership. Maps to C3-O1, C3-O2, C3-O5 through C3-O8."
  },
  {
    "anchorId": "C3-2",
    "capabilityId": "C3",
    "sourceTopic": "Tech Debt & Platform Investment",
    "level1Developing": "Scope: Individual. Key Behavior: Acknowledges tech debt exists but doesn't track or prioritize it. Artifact: None — tech debt is undocumented. Distinguishing Test: Cannot quantify any tech debt item's impact on delivery velocity or incident risk.",
    "level2Emerging": "Scope: Team (informal). Key Behavior: Tracks tech debt informally; makes occasional build-vs-buy arguments with basic cost comparison. Artifact: Informal tech debt list; basic cost comparisons. Distinguishing Test: Can name the top 3 tech debt items but cannot quantify their business impact.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Manages 15-20% sprint capacity for debt; quantifies debt in business terms; makes build-vs-buy decisions with TCO analysis. Artifact: Tech debt registry with cost-of-delay and remediation estimates; documented build-vs-buy decisions. Distinguishing Test: Debt allocation is visible in sprint planning; each registry item has a business impact estimate.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Identifies systemic tech debt spanning team boundaries; proposes remediation with velocity impact measurement; evaluates platform ROI across consuming teams. Artifact: Cross-team debt remediation proposals with before/after velocity data; platform investment ROI analysis. Distinguishing Test: Tech debt allocation defended to leadership with delivery speed correlation data; platform decisions evaluated with multi-team adoption metrics.",
    "level5Advanced": "Scope: Org. Key Behavior: Drives strategic tech debt paydown with measurable velocity impact; platform investments create org-wide leverage with documented ROI. Artifact: Org-level tech debt report in business terms; platform adoption dashboard with multi-team ROI. Distinguishing Test: Tech debt investment is a first-class budget line item reviewed alongside feature delivery; platform teams have measurable internal adoption targets.",
    "rationale": "Anchor 2/2: Covers tech debt management, build-vs-buy, and platform investment decisions. Maps to C3-O3, C3-O4, C3-O9 through C3-O11. Intentionally separate from technical vision — investment prioritization is a distinct sub-skill."
  },
  {
    "anchorId": "C4-1",
    "capabilityId": "C4",
    "sourceTopic": "Operating Cadence & Process",
    "level1Developing": "Scope: Individual. Key Behavior: Follows existing processes without evaluating their effectiveness; interrupts handled reactively. Artifact: None — no documented cadence or operating norms. Distinguishing Test: Cannot describe the purpose of team ceremonies or how information flows.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: Establishing regular operating cadence; sprint predictability improving; interrupt management attempted but inconsistent. Artifact: Basic meeting schedule with stated purposes. Distinguishing Test: Can articulate why each ceremony exists but retro action items still incomplete >50% of the time.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Clear operating cadence with defined outputs; interrupt rotation implemented; retro action items tracked to >80% completion; engineers have 4+ hours of daily focus time. Artifact: Team operating manual; velocity bottleneck analysis using value stream mapping. Distinguishing Test: New hire understands the operating system within first week; focus time is structurally protected by calendar policy.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Operating cadence requires minimal intervention — team self-corrects when rituals drift; toil eliminated through quarterly reviews with measurable time reclaimed; process improvements shared with adjacent teams. Artifact: Documented process improvements with before/after metrics; async contribution rates measured. Distinguishing Test: Team operates effectively when EM is out for 2+ weeks; remote/hybrid participation is equitable by measured contribution.",
    "level5Advanced": "Scope: Org. Key Behavior: Operating rhythm is the org standard; teams self-manage within cadence; systemic velocity analysis and focus time protection are organizational norms. Artifact: Org-wide operating framework adopted by multiple teams with measured outcomes. Distinguishing Test: Other EMs adopt this operating model; remote/hybrid practices produce equal engagement measured across locations.",
    "rationale": "Anchor 1/2: Covers team operating system (cadence, process, interrupts, remote practices, velocity diagnosis, focus time protection). Maps to C4-O1, C4-O7 through C4-O13."
  },
  {
    "anchorId": "C4-2",
    "capabilityId": "C4",
    "sourceTopic": "Delivery Predictability & Execution",
    "level1Developing": "Scope: Individual. Key Behavior: Sprint velocity inconsistent; scope creep common; commitments aspirational not evidence-based; delivery risks surface at deadline. Artifact: None — no delivery tracking or capacity model. Distinguishing Test: Cannot state team's commitment accuracy rate or identify the top delivery bottleneck.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Beginning to track delivery metrics; risks identified mid-sprint but mitigation is reactive; starting to use data for capacity planning. Artifact: Basic delivery dashboard; some historical velocity data. Distinguishing Test: Can state last sprint's commitment accuracy but mitigation plans are after-the-fact.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Meeting 85%+ of committed scope for 3+ consecutive quarters; capacity model accounts for interrupts, on-call, and PTO; risks flagged early with mitigation plans. Artifact: Capacity model with interrupt/PTO buffers; delivery dashboard with trend data; documented scope negotiation with stakeholders. Distinguishing Test: Stakeholders trust delivery estimates; scope trade-offs are negotiated proactively, not at deadline.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Delivery predictability maintained through team transitions and scope changes; capacity model refined using actuals-vs-plan variance; teams self-manage scope trade-offs. Artifact: Variance analysis history; delivery practices documented for adoption. Distinguishing Test: Predictability sustained above 85% during organizational disruption; adjacent teams adopt delivery practices.",
    "level5Advanced": "Scope: Org. Key Behavior: Delivery practices are the org standard; predictability maintained through organizational change; cross-team delivery coordination is seamless. Artifact: Org-wide delivery framework with measured adoption and outcomes. Distinguishing Test: Elite-level DORA metrics maintained alongside high predictability — velocity and reliability are complementary, not competing.",
    "rationale": "Anchor 2/2: Covers delivery outcomes (predictability, capacity planning, scope management). Maps to C4-O2 through C4-O6. Intentionally separate from cadence — process vs. outcomes are distinct sub-skills."
  },
  {
    "anchorId": "C5-1",
    "capabilityId": "C5",
    "sourceTopic": "Cross-Functional Partnership",
    "level1Developing": "Scope: Individual. Key Behavior: Works primarily within engineering; limited visibility into product/design strategy; reactive to product asks. Artifact: None — collaboration is ad hoc. Distinguishing Test: Cannot articulate the product roadmap or explain why product prioritized the current quarter's top 3 items.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Attends some cross-functional meetings; beginning to understand product constraints; responds to product/design input but doesn't proactively seek it. Artifact: Attendance at cross-functional forums. Distinguishing Test: Can describe product goals but hasn't influenced a major planning decision; waits for product to ask rather than volunteering perspective.",
    "level3Competent": "Scope: Team (proactive). Key Behavior: Active participant in triad; shares engineering constraints transparently; influences decisions through data and structured frameworks; scope surprises rare. Artifact: Weekly triad sync notes with shared decision rationale; documented constraints and trade-offs. Distinguishing Test: Triad partners seek engineering input on scope/feasibility before finalizing plans; product/design explicitly reference engineering constraints in decisions.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Cross-functional practices shared across teams; influences product strategy through early involvement in planning; mediates conflicts between Product and Design from engineering perspective. Artifact: Cross-team triad frameworks; documented planning involvement in major initiatives. Distinguishing Test: Other EMs adopt cross-functional practices; leadership invites engineering input on strategic decisions before commitments are made.",
    "level5Advanced": "Scope: Org. Key Behavior: Engineering voice integrated into org-level strategy; cross-functional collaboration is a cultural norm; influences long-horizon roadmap decisions. Artifact: Org-wide cross-functional governance with engineering representation. Distinguishing Test: Engineering perspective shapes business strategy; major initiatives reflect technical feasibility analysis from the planning phase.",
    "rationale": "Anchor 1/2: Covers peer-level cross-functional relationships (PM/Design triad, TPM, DS, platform partnerships, triad alignment cadence). Maps to C5-O1 through C5-O5, C5-O16."
  },
  {
    "anchorId": "C5-2",
    "capabilityId": "C5",
    "sourceTopic": "Upward Management & Sponsor Building",
    "level1Developing": "Scope: Individual. Key Behavior: Conflicts with product/design often escalate to leadership; communication is unclear or politicized. Artifact: None. Distinguishing Test: Leadership frequently resolves cross-functional disagreements; EM describes conflicts in blame language ('product always changes their mind').",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: Attempts to resolve disagreements with product/design; communication improving but still occasional misalignment; escalations less frequent but still happen quarterly. Artifact: Informal conversation attempts. Distinguishing Test: Can resolve 1:1 disagreements but struggles with complex multi-party conflicts.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Uses structured frameworks (trade-off analysis, DACI) to resolve disagreements; escalations rare (<1 per quarter); stakeholders describe collaboration as professional and data-driven. Artifact: Documented decision frameworks and resolved disagreement examples. Distinguishing Test: Triad members describe EM as collaborative and respectful of different perspectives; disagreements resolved at team level without escalation.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Helps adjacent teams resolve cross-functional conflicts; frameworks adopted across teams; relationship-building skills evident in ease of working with stakeholders across organization. Artifact: Cross-team collaboration norms and documented conflict resolution patterns. Distinguishing Test: Other teams seek this EM's mediation help; reputation for fair-mindedness and understanding of multiple perspectives.",
    "level5Advanced": "Scope: Org. Key Behavior: Org-level cross-functional health improved; stakeholder satisfaction with engineering collaboration high; influences how the org handles cross-functional complexity. Artifact: Org-wide collaboration frameworks; cross-functional satisfaction metrics trending upward. Distinguishing Test: Other functions cite this leader's influence as a major reason for better engineering-business alignment.",
    "rationale": "Anchor 2/2: Covers upward influence (managing up, sponsor relationships, political capital, scope navigation). Maps to C5-O6 through C5-O15. Intentionally separate from peer partnerships — upward influence is a distinct sub-skill."
  },
  {
    "anchorId": "C6-1",
    "capabilityId": "C6",
    "sourceTopic": "Team Health & Execution",
    "level1Developing": "Scope: Individual. Key Behavior: 1:1s are reactive status meetings; no development conversations; team career goals unknown to EM. Artifact: None — no development plans or career frameworks. Distinguishing Test: EM cannot name a single development goal for any direct report.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Some 1:1s include development conversations; annual performance reviews completed but feedback is generic; promotion decisions reactive to business need. Artifact: Performance reviews exist but lack behavioral specificity. Distinguishing Test: Development conversations are occasional, not structural; team cannot describe clear progression path.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Structured 1:1s with development focus; quarterly skip-level conversations; performance feedback calibrated; promotion framework with transparent criteria. Artifact: Structured 1:1 template; skip-level conversation notes; promotion register; progression framework visible to team. Distinguishing Test: Every team member can describe their development goals and how they'll be measured; high-potential people know path to promotion.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Development practices scaled across teams; weak managers coached or transitioned; retention of high performers above 90%; underperformance addressed within 60 days. Artifact: Development coaching program for managers; cross-team progression framework; high-performer retention data. Distinguishing Test: Peer EMs adopt this team's development practices; promotion success rate (people promoted are later high performers) is above 80%.",
    "level5Advanced": "Scope: Org. Key Behavior: Talent development is a cultural norm; managers are skilled developers of people; organization creates internal leaders instead of hiring from outside. Artifact: Org-wide talent development program; leadership pipeline visible with succession plans. Distinguishing Test: Majority of leadership positions filled from internal candidates; development practices drive organizational growth.",
    "rationale": "Anchor 1/2: Covers EM-level coaching (1:1s, underperformance, retention, stretch assignments, Staff+ management, career development). Maps to C6-O1 through C6-O5, C6-O8 (stay interviews), C6-O9 (Staff+ management), C6-O10 (career conversations), C6-O13 (career pathways)."
  },
  {
    "anchorId": "C6-2",
    "capabilityId": "C6",
    "sourceTopic": "Managing Managers (Director Track)",
    "level1Developing": "Scope: Individual. Key Behavior: Performance reviews are annual checkbox; feedback primarily negative or absent; underperformance not addressed; team diversity not tracked. Artifact: None — reviews are generic form-filling. Distinguishing Test: Cannot name three specific behavioral observations from last performance review of any team member.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Feedback more specific but based on recency bias; calibration across team inconsistent; underperformance addressed only when it becomes critical. Artifact: Performance review documents with some behavioral content; informal calibration. Distinguishing Test: Review ratings diverge >1 level for similar performance; some underperformance feedback is personality-based rather than behavioral.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Feedback specific and behavioral with examples; calibration across team ensures consistency; underperformance addressed within 60 days with clear improvement plan; diversity hiring intentional. Artifact: Feedback templates grounded in situation-behavior-impact; calibration session notes; underperformance action plans. Distinguishing Test: Team perceives reviews as fair; rating distribution consistent across team; underperformance issues resolved or exited cleanly.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Calibration practices scaled across teams; feedback quality improves measurably; high-potential talent retention >90%; underperformance handled with coaching then clean exit if needed. Artifact: Cross-team feedback guidelines; calibration oversight function; underperformance resolution tracking. Distinguishing Test: Peer managers improve their feedback quality by adopting this team's practices; team diversity approaching org targets.",
    "level5Advanced": "Scope: Org. Key Behavior: Feedback culture is developmental; calibration is org-wide standard; performance management drives clear consequences and development; diverse hiring is systematic. Artifact: Org-wide calibration process; performance management system that tracks coaching and outcomes. Distinguishing Test: Performance reviews drive clear career outcomes and development without fear; diversity metrics move toward representation goals.",
    "rationale": "Anchor 2/2: Covers Director-level EM development (coaching EMs, skip-levels, bench building, succession planning). Maps to C6-O6, C6-O7, C6-O11 (coaching struggling EMs), C6-O12 (succession plans). Intentionally separate — different altitude of practice."
  },
  {
    "anchorId": "C7-1",
    "capabilityId": "C7",
    "sourceTopic": "Stakeholder Management & Influence",
    "level1Developing": "Scope: Individual. Key Behavior: Proposals lack structure; options presented without trade-offs; no written decision artifacts. Artifact: None — decisions made verbally. Distinguishing Test: Cannot produce a written decision memo; stakeholders report ambiguity about what was decided.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Beginning to write decision memos; options presented but trade-offs quantified incompletely; decisions sometimes revisited. Artifact: Occasional decision memos, inconsistent format. Distinguishing Test: Proposals sometimes require rework before approval; can articulate rationale but decision memo wasn't sufficient first time.",
    "level3Competent": "Scope: Team (proactive). Key Behavior: Decision memos template followed consistently; options with quantified trade-offs; first-meeting approval rate >80%; roadmap includes 'what we are NOT doing.' Artifact: Searchable decision log; consistent memo template; roadmap with trade-off section. Distinguishing Test: Decisions don't get revisited; stakeholders understand and accept trade-offs; new hires can read decision log to understand system rationale.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Decision processes improved based on past errors; DACI framework adopted across teams; exec communication shapes organizational strategy. Artifact: Cross-team decision framework; DACI role assignments for org-level decisions; roadmap coordination across teams. Distinguishing Test: Decision revisitation <5% across teams; other EMs adopt this decision process; execs reference EMs' proposals in strategy discussions.",
    "level5Advanced": "Scope: Org. Key Behavior: Org-wide decision discipline established; decision culture embedded in hiring/onboarding; historical decisions used as reference material; strategy visibility across org. Artifact: Org-wide decision framework with measured adoption; decision log referenced in all organizational communications. Distinguishing Test: New hires understand org rationale by reading decisions; no decision revisitation; org strategy traces back to transparent decisions.",
    "rationale": "Anchor 1/2: Covers external-facing communication (status updates, bad news delivery, exec presentations, managing up, difficult conversations, communication scaling). Maps to C7-O2 through C7-O5, C7-O8 (difficult conversations), C7-O9 (communication scaling), C7-O11 (explicit trade-off communication)."
  },
  {
    "anchorId": "C7-2",
    "capabilityId": "C7",
    "sourceTopic": "Decision Making & Prioritization",
    "level1Developing": "Scope: Individual. Key Behavior: Communication altitude not adapted; detail level same whether speaking to execs or ICs; bad news delivered late or not at all. Artifact: None — no structured communication. Distinguishing Test: Execs complain about too much detail; ICs complain about missing context; manager surprised by bad news.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: Attempting to adapt communication; sometimes delivers bad news early; weekly updates exist but inconsistent quality. Artifact: Weekly status update template (used inconsistently). Distinguishing Test: Some altitude adaptation attempted but execution uneven; bad news flag timing inconsistent.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Three-altitude communication (exec summary, peer detail, IC implementation); weekly proactive status with outcomes/risks/asks; bad news flagged within 24 hours with mitigation plan. Artifact: Three-version communication template; consistent weekly status update; bad-news flag log. Distinguishing Test: Execs get 1-slide summary; peers get trade-offs; ICs get implementation context; manager never surprised; zero instances per quarter where leadership learns about problem from someone other than EM.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Exec communication drives org strategy; communication quality evaluated across teams; bad news culture normalized. Artifact: Org communication standards; cross-team communication templates; exec presentation best practices documented. Distinguishing Test: Other teams adopt this communication approach; execs invite EM to strategy conversations based on communication quality; bad news surfaces systematically across org.",
    "level5Advanced": "Scope: Org. Key Behavior: Communication culture is organizational norm; altitude adaptation second-nature; transparent escalation practiced at all levels. Artifact: Org-wide communication framework with measured outcomes. Distinguishing Test: Communication quality is a hiring criterion; transparency is described as org characteristic; bad news surfaces early across org.",
    "rationale": "Anchor 2/2: Covers decision architecture (DACI/RFC, trade-off framing, reversibility, authority distribution). Maps to C7-O1, C7-O6, C7-O7, C7-O10 (decision authority distribution). Intentionally separate — communication vs decision-making are distinct sub-skills."
  },
  {
    "anchorId": "C8-1",
    "capabilityId": "C8",
    "sourceTopic": "Incident Response & Command",
    "level1Developing": "Scope: Individual. Key Behavior: Reacts to incidents without defined process; post-mortems blame individuals or don't happen; on-call health unmonitored. Artifact: None. Distinguishing Test: Cannot describe incident command roles or the team's on-call page rate.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Participates in incident response; post-mortems attempted but action items incomplete; on-call load tracked but not actively managed. Artifact: Post-mortem documents exist but completion tracking is informal. Distinguishing Test: Post-mortems happen but action item completion rate is below 50%.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: ICS roles defined and practiced; blameless post-mortems with >90% action item completion; on-call health maintained at <2 off-hours pages/night. Artifact: ICS roster, post-mortem template with tracked action items, on-call health dashboard. Distinguishing Test: Repeat incidents from same root cause drop to near-zero; on-call is viewed as sustainable.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Incident response patterns shared across teams; post-mortem themes analyzed quarterly for systemic improvements; on-call excellence is a team differentiator. Artifact: Cross-team incident pattern analysis; on-call health benchmarks across teams. Distinguishing Test: Other teams adopt incident response practices; MTTR consistently below area average.",
    "level5Advanced": "Scope: Org. Key Behavior: Incident management culture institutionalized; error budget model drives reliability investment; org-wide MTTR improving year-over-year. Artifact: Org-wide incident management framework with measured outcomes. Distinguishing Test: Reliability metrics used as first-class business KPIs; error budget policy automatically governs feature/reliability trade-offs.",
    "rationale": "Anchor 1/2: Covers reactive operational excellence (incident command, post-mortems, on-call health). Maps to C8-O1 through C8-O3."
  },
  {
    "anchorId": "C8-2",
    "capabilityId": "C8",
    "sourceTopic": "Systemic Risk Reduction",
    "level1Developing": "Scope: Individual. Key Behavior: Risk management is purely reactive — learns only from production incidents; no pre-launch risk assessment. Artifact: None. Distinguishing Test: Cannot identify the top 3 failure modes in their system.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: Conducts ad hoc risk assessments before major launches; beginning to track vendor dependencies; awareness of compliance requirements. Artifact: Launch checklist exists but not consistently used. Distinguishing Test: Can name critical dependencies but has no tested fallback for any of them.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: FMEA before every major launch; vendor risk registry maintained; compliance requirements mapped to engineering practices; quarterly game days run. Artifact: FMEA documents, vendor risk registry, compliance matrix, game day results. Distinguishing Test: Zero Sev1 incidents on launches; game days reveal and fix gaps proactively.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Risk management practices shared across teams; game days include cross-team failure scenarios; vendor management coordinated at area level. Artifact: Cross-team risk register; shared game day exercises. Distinguishing Test: Cross-team incidents handled smoothly; vendor risk mitigation coordinated rather than duplicated.",
    "level5Advanced": "Scope: Org. Key Behavior: Proactive resilience is an organizational norm; chaos engineering culture established; error budgets govern feature/reliability trade-offs. Artifact: Org-wide resilience framework with chaos engineering program. Distinguishing Test: Resilience investment justified with data; org handles major incidents without heroics.",
    "rationale": "Anchor 2/2: Covers proactive risk management (failure mode analysis, launch readiness, dependency risk, vendor management, chaos engineering, error budgets). Maps to C8-O4 through C8-O6, C8-O7 (chaos engineering and game days), C8-O8 (error budget management). Intentionally separate from incident response — proactive vs. reactive are distinct sub-skills."
  },
  {
    "anchorId": "C9-1",
    "capabilityId": "C9",
    "sourceTopic": "Metric Selection & Dashboard Design",
    "level1Developing": "Scope: Individual. Key Behavior: No metrics tracked; delivery health is anecdotal; dashboards don't exist or are never consulted. Artifact: None. Distinguishing Test: Cannot state team's deployment frequency or lead time.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Basic DORA metrics tracked; dashboards exist but checked infrequently; metrics used to report status, not drive decisions. Artifact: Basic delivery dashboard. Distinguishing Test: Can state DORA numbers but cannot name a decision driven by metric data in the last quarter.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: DORA metrics tracked weekly with trend analysis; developer satisfaction measured quarterly; metric pairings prevent single-metric gaming; feature flag lifecycle governed. Artifact: Delivery dashboard with trend data; developer satisfaction survey; metric pair documentation. Distinguishing Test: Every active metric has informed a decision within 90 days; no surveillance culture complaints.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Metric frameworks adapted to team maturity; metrics benchmarked across teams; insights shared to improve adjacent teams; metric sophistication evolves based on decisions driven. Artifact: Cross-team metric benchmarks; maturity-appropriate metric recommendations for each team. Distinguishing Test: Teams at different maturity levels have appropriately different metric sets; no death-by-metrics.",
    "level5Advanced": "Scope: Org. Key Behavior: Metrics culture established org-wide; measurement frameworks (DORA, SPACE) applied appropriately; engineering metrics integrated with business metrics. Artifact: Org-wide metrics framework with documented decision impact. Distinguishing Test: Business leadership references engineering metrics in planning; engineering metrics drive budget allocation.",
    "rationale": "Anchor 1/2: Covers metric infrastructure (DORA tracking, developer experience, delivery metrics, dashboards, metric pairings, maturity adaptation). Maps to C9-O1 through C9-O3, C9-O7, C9-O9 (metric pairings), C9-O10 (metric maturity adaptation)."
  },
  {
    "anchorId": "C9-2",
    "capabilityId": "C9",
    "sourceTopic": "Data-Driven Decision Making",
    "level1Developing": "Scope: Individual. Key Behavior: OKRs vague or absent; engineering outcomes not translated to business terms; attrition not classified. Artifact: None. Distinguishing Test: Cannot state team's OKRs or explain how engineering work connects to business outcomes.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: OKRs defined but not scored quarterly; some engineering outcomes translated to business terms; attrition tracked but not classified. Artifact: OKR document (not actively scored). Distinguishing Test: OKRs exist on paper but team cannot describe how they influenced a prioritization decision.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Outcome-oriented OKRs scored quarterly with 0.6-0.8 average; engineering outcomes translated to business metrics; attrition classified and patterns addressed; A/B testing with statistical rigor. Artifact: Scored OKRs with quarterly review; business metric translation in investment proposals; attrition classification. Distinguishing Test: >80% of engineering investment proposals include business ROI; regrettable attrition below benchmark.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: OKR quality improved through coaching; business metric translation is standard across teams; experimentation infrastructure supports multiple teams. Artifact: OKR quality rubric; cross-team experimentation platform. Distinguishing Test: Engineering leaders invited to business planning conversations based on metric credibility.",
    "level5Advanced": "Scope: Org. Key Behavior: Data-driven decision culture established; engineering metrics integrated into business strategy; experimentation at scale with organizational learning. Artifact: Org-wide experimentation platform; engineering-business metric integration. Distinguishing Test: Engineering investment decisions driven by data at board level; experimentation velocity is a competitive advantage.",
    "rationale": "Anchor 2/2: Covers decision-making with data (OKR measurement, business impact articulation, experimentation, ROI framing). Maps to C9-O4 through C9-O6, C9-O8. Intentionally separate from metric infrastructure — collecting data vs. using data are distinct sub-skills."
  },
  {
    "anchorId": "C10-1",
    "capabilityId": "C10",
    "sourceTopic": "Budget, Headcount & Resource Planning",
    "level1Developing": "Requests headcount without financial framing. Cloud costs not tracked at team level. Build-vs-buy decisions are gut feel. Budget is someone else's problem.",
    "level2Emerging": "Beginning to think about headcount in terms of ROI. Starting to track cloud costs at team level. Some awareness of build-vs-buy trade-offs. Budget awareness growing.",
    "level3Competent": "Headcount justified with ROI analysis. Cloud costs attributed and optimized per service. Build-vs-buy uses TCO framework. Contractor strategy is principled. Budget defense prepared for cuts.",
    "level4Distinguished": "Headcount proposals connect to business outcomes with multi-scenario ROI modeling (best/base/worst case). Cloud cost attribution drives team-level accountability — engineers understand and optimize their service costs. Build-vs-buy decisions documented with TCO analysis reusable for future decisions. Finance partners proactively consult on engineering investment questions.",
    "level5Advanced": "Org-level headcount narrative wins at VP/finance level. FinOps culture established across teams. Engineering investment framed as business asset with measurable returns. Proactive cost optimization (savings reinvested). Trusted steward of company resources. Company benchmark: Netflix's 'highly aligned, loosely coupled' model lets teams self-allocate within strategic guardrails, and Amazon's FinOps culture treats cost as a first-class engineering constraint.",
    "rationale": "Anchor 1/2: Covers financial stewardship (headcount justification, cloud costs, build-vs-buy, contractor strategy). Maps to C10-O1, C10-O3, C10-O4, C10-O5."
  },
  {
    "anchorId": "C10-2",
    "capabilityId": "C10",
    "sourceTopic": "Strategic Reallocation & ROI Framing",
    "level1Developing": "Resources stay where they were assigned regardless of changing priorities. No framework for comparing ROI across investments. Reacts to cuts rather than proactively optimizing allocation.",
    "level2Emerging": "Beginning to evaluate resource allocation against impact. Some awareness that reallocation is a tool, not just a response to cuts. Starting to frame engineering investments in outcome terms.",
    "level3Competent": "Presents resource requests as tiered options with quantified trade-offs at each level. Tracks cost-per-outcome to demonstrate engineering ROI. Proactively reallocates capacity to highest-impact work without waiting for top-down direction. During cuts, communicates explicitly what stops — not just what continues.",
    "level4Distinguished": "Tiered resource proposals adopted as the default format for engineering requests to leadership. Cost-per-outcome data actively informs quarterly reallocation decisions — not just tracked but acted upon. Reallocation happens within established boundaries without requiring executive approval for each shift. Adjacent teams begin adopting the tiered proposal framework.",
    "level5Advanced": "Resource allocation narrative presented at VP/finance level with rigorous ROI analysis. Proactive cost optimization generates savings that are reinvested in high-impact areas. Engineering investment framing is a recognized organizational capability. Teams across org adopt similar ROI tracking practices. Company benchmark: Amazon's FinOps culture treats cost as a first-class engineering constraint, with weekly business reviews comparing cost-per-outcome across teams and reinvesting savings into highest-ROI initiatives.",
    "rationale": "Anchor 2/2: Covers dynamic allocation (tiered proposals, ROI tracking, proactive reallocation, cut communication). Maps to C10-O2, C10-O6, C10-O7, C10-O8. Intentionally separate from financial stewardship — static budgeting vs. dynamic allocation are distinct sub-skills."
  },
  {
    "anchorId": "C11-1",
    "capabilityId": "C11",
    "sourceTopic": "Hiring Process & Bar Raising",
    "level1Developing": "Participates in hiring loops. Onboarding is informal (buddy system if lucky). Headcount requests are 'we need more people.' No engineering brand presence.",
    "level2Emerging": "Hiring loops becoming more structured. Some onboarding documentation exists. Starting to justify headcount with basic data. Referral network emerging.",
    "level3Competent": "Calibrated hiring loops with trained interviewers and rubrics. Structured interview processes evaluate demonstrated competencies over pedigree. Headcount justified with capacity model and business impact. Referral pipeline active. Bar raiser discipline maintained under hiring pressure. Hiring funnel conversion tracked at each stage.",
    "level4Distinguished": "Hiring bar maintained under volume pressure — interview calibration sessions run quarterly with inter-rater reliability tracked. Structured interview process produces consistent signal across interviewers. Headcount narrative connects team capacity to business strategy with specific revenue or risk impact. Diverse candidate slates achieved through structured sourcing, not just pipeline volume.",
    "level5Advanced": "Bar raiser discipline maintained under pressure. Interviewer calibration is org model. Headcount cases win at VP level. Hiring process produces consistently strong signal with high inter-rater reliability. Competitive hiring strategy wins talent beyond compensation through differentiated value proposition. Hiring funnel instrumented and optimized with data-driven pipeline management. Company benchmark: Amazon's Bar Raiser program gives independent interviewers veto power to protect hiring quality, and Google's structured hiring committees use calibrated rubrics to eliminate bias.",
    "rationale": "Anchor 1/2: Covers hiring mechanics (interview loops, bar raising, headcount justification, interviewer calibration, structured interviews, competitive hiring strategy, funnel optimization). Maps to C11-O1, C11-O2, C11-O7 through C11-O11."
  },
  {
    "anchorId": "C11-2",
    "capabilityId": "C11",
    "sourceTopic": "Onboarding Excellence & Talent Brand",
    "level1Developing": "Onboarding is informal — new hires figure it out. No engineering brand presence. Time-to-productivity not tracked. No referral pipeline.",
    "level2Emerging": "Basic onboarding documentation exists. Some buddy pairing. Beginning to track new hire experience. Occasional blog post or meetup participation.",
    "level3Competent": "Structured 30/60/90 onboarding with measurable milestones. Time-to-productivity tracked and improving. Buddy/mentor program formalized. Active blog or conference presence emerging. Referral pipeline maintained.",
    "level4Distinguished": "Onboarding continuously refined using new hire feedback surveys at 30/60/90 days. Time-to-productivity benchmarked against org averages with diagnosed bottlenecks. Engineering brand built through conference talks, blog posts, or open source contributions that generate inbound senior talent interest. Referral rate among highest on team — engineers actively recruit from their networks.",
    "level5Advanced": "Time-to-productivity measured and optimized to org-best levels. Engineering brand is a recruiting advantage — candidates cite it in interviews. Conference talks and open source presence attract senior talent. Onboarding process is a model other teams adopt. Company benchmark: Meta's Bootcamp onboarding gets new engineers productive within 6 weeks through structured mentorship, and Google's engineering brand (publications, open source) drives inbound senior talent.",
    "rationale": "Anchor 2/2: Covers talent brand and onboarding (30/60/90 programs, time-to-productivity, engineering brand, referrals). Maps to C11-O3 through C11-O6. Intentionally separate from hiring mechanics — acquisition vs. integration are distinct sub-skills."
  },
  {
    "anchorId": "C12-1",
    "capabilityId": "C12",
    "sourceTopic": "Team Charter & Engineering Principles",
    "level1Developing": "Team culture exists by accident (whatever norms emerged). Recognition is sporadic. Inclusion is 'we treat everyone the same.'",
    "level2Emerging": "Starting to be intentional about team norms. Some recognition happening but not systematized. Awareness of inclusion beyond 'same treatment.' Beginning to document engineering practices.",
    "level3Competent": "Team charter written and referenced. Engineering principles documented and used in design reviews. Regular recognition rituals. Active inclusion practices (meeting facilitation, async options, equitable opportunity distribution). Explicit feedback channels ensuring every voice is heard. Cultural continuity maintained through growth and leadership transitions. Toxic behaviors addressed within 48 hours.",
    "level4Distinguished": "Team charter reviewed and evolved quarterly based on team feedback and growth. Engineering principles referenced in code review and architecture decision feedback, not just design reviews. Recognition operates peer-to-peer without requiring manager initiation. Inclusion practices produce measurable engagement survey results with action plans for identified gaps.",
    "level5Advanced": "Culture is intentional, measurable, and self-reinforcing. Engineering principles adopted beyond own team. Cultural continuity maintained through rapid growth and leadership transitions without degradation. Inclusive environment measurable in surveys and independently confirmed. Toxic behaviors caught and addressed proactively — no 'missing stair' dynamics. Candidates cite team culture as top-3 reason for accepting offer in post-hire surveys. Company benchmark: Netflix's Freedom and Responsibility culture doc is the canonical example of intentional culture at scale, and Spotify's explicit team Health Checks make culture measurable and discussable.",
    "rationale": "Anchor 1/2: Covers culture foundations (team charter, engineering principles, recognition, inclusion, cultural continuity, inclusive feedback channels, toxic behavior intervention). Maps to C12-O1, C12-O2, C12-O4 through C12-O8."
  },
  {
    "anchorId": "C12-2",
    "capabilityId": "C12",
    "sourceTopic": "Knowledge Sharing & Documentation Culture",
    "level1Developing": "Knowledge sharing is informal — tribal knowledge dominates. Documentation sparse and outdated. No structured forums for cross-team learning.",
    "level2Emerging": "Some documentation emerging but inconsistent. Occasional knowledge-sharing sessions. Beginning to notice that tribal knowledge creates bottlenecks and bus-factor risk.",
    "level3Competent": "Documentation standards maintained and enforced. Regular tech talks or knowledge-sharing rituals. Design doc culture established. Onboarding documentation reduces ramp time measurably.",
    "level4Distinguished": "Team members contribute documentation as part of their workflow — updates happen alongside code changes, not as separate tasks. Design doc quality validated through structured peer review with clear approval criteria. Knowledge-sharing sessions run with rotating ownership — engineers volunteer to present. Documentation standards shared with at least one adjacent team.",
    "level5Advanced": "Knowledge sharing institutionalized across org (tech talks, wikis, design doc culture). Documentation coverage comprehensive and maintained. Knowledge-sharing practices are a model other teams adopt. Reduces organizational bus-factor risk at scale. Company benchmark: Stripe's writing culture and Amazon's design doc requirements institutionalize knowledge sharing as an organizational practice, not an individual habit.",
    "rationale": "Anchor 2/2: Covers knowledge management (documentation, tech talks, design docs, cross-team learning). Maps to C12-O3. Intentionally separate from culture foundations — knowledge sharing is a distinct operational practice."
  },
  {
    "anchorId": "C13-1",
    "capabilityId": "C13",
    "sourceTopic": "Security Practices & Vulnerability Management",
    "level1Developing": "Ensures team follows security practices when reminded. Patches vulnerabilities reactively. Change management is informal. Compliance is handled last-minute before audits.",
    "level2Emerging": "Security practices becoming more consistent without reminders. Vulnerability patching improving but SLA sometimes missed. Change management process emerging. Starting to prepare for audits proactively.",
    "level3Competent": "Security embedded in development workflow (threat modeling, automated scanning, champion rotation). Security champions rotate quarterly and attend security guild. Automated security scanning integrated into CI/CD with severity-based deployment gates. Vulnerability SLAs met consistently (P0 <48hrs, P1 <7 days). Threat models required for features touching auth, payments, or PII. Tiered change management process in place.",
    "level4Distinguished": "Security practices embedded in development culture — engineers initiate threat models without prompting for sensitive features. Vulnerability SLAs met consistently with mean resolution time trending downward. Security champion program produces engineers who improve team security practices independently. Change management process documented and influencing adoption by adjacent teams.",
    "level5Advanced": "Security culture driven across org. Change management standards adopted org-wide. Security champion program scaled beyond own team. Automated scanning and threat modeling are organizational standards. Teams proactively identify and mitigate security risks before they become vulnerabilities. Company benchmark: Google's BeyondCorp zero-trust model and Amazon's mandatory security review process demonstrate security as a proactive engineering discipline, not a compliance checkbox.",
    "rationale": "Anchor 1/2: Covers security operations (threat modeling, vulnerability management, change management, security champions, automated scanning, threat modeling for sensitive features). Maps to C13-O1 through C13-O3, C13-O5 through C13-O7."
  },
  {
    "anchorId": "C13-2",
    "capabilityId": "C13",
    "sourceTopic": "Compliance & Governance Automation",
    "level1Developing": "Compliance is handled last-minute before audits. Evidence collection is manual. Access controls inconsistent. No automated compliance checks.",
    "level2Emerging": "Beginning to embed compliance into workflows. Some automated scanning in place. Starting to track access control hygiene. Audit preparation moving earlier in the cycle.",
    "level3Competent": "Continuous compliance posture with automated evidence collection. Access governance automated (quarterly reviews, auto-expiring elevated permissions). Audit prep reduced from weeks to days.",
    "level4Distinguished": "Compliance evidence collected automatically with real-time dashboard — audit preparation requires hours, not days. Access governance runs exception-based reviews only — routine changes fully automated. Zero repeat findings across consecutive audit cycles. Governance automation patterns documented and shared with adjacent teams.",
    "level5Advanced": "Zero P1/P2 findings in 3+ consecutive audit cycles. Compliance is a competitive advantage, not a burden. Governance automation is an org model. Continuous compliance frameworks adopted beyond own team. Company benchmark: Netflix automates compliance evidence collection continuously, making audit readiness a dashboard metric rather than a quarterly project.",
    "rationale": "Anchor 2/2: Covers compliance and governance (audit readiness, evidence collection, access governance, compliance automation). Maps to C13-O3, C13-O4. Intentionally separate from security operations — compliance is a distinct discipline from vulnerability management."
  },
  {
    "anchorId": "C14-1",
    "capabilityId": "C14",
    "sourceTopic": "Performance Reviews & Calibration",
    "level1Developing": "Writes reviews but they're vague. Calibration prep is last-minute. Avoids PIPs. Career conversations are infrequent.",
    "level2Emerging": "Reviews improving in specificity. Starting to prepare for calibration earlier. Beginning to address performance issues with HR guidance. Career conversations starting to happen regularly.",
    "level3Competent": "Evidence-based reviews with specific impact and rubric language. Continuous performance documentation — running notes updated weekly, not reconstructed at review time. Calibration cases prepared with cross-team comparison data. PIPs executed when needed with HR partnership. High performers managed with differentiated development plans and retention-conscious conversations. Managed exits executed with dignity and clear process. Quarterly career conversations for all reports.",
    "level4Distinguished": "Calibration cases include cross-team comparison data that withstands committee scrutiny — ratings rarely require revision. Performance management covers the full spectrum without avoidance: high performers receive differentiated stretch assignments, underperformers receive structured improvement plans with clear milestones, managed exits execute with dignity and complete knowledge transfer. Career conversations produce written development plans referenced in subsequent reviews.",
    "level5Advanced": "Reviews set the standard for the org. Continuous performance documentation produces evidence-rich reviews with zero surprises. Calibration presence is authoritative and credible. High performer retention above 90% through differentiated investment. Performance management (including exits) handled cleanly — managed exits executed with dignity, full knowledge transfer, and zero legal escalations. Career development is the reason people join your team. Calibrates other EMs' performance standards. Company benchmark: Google's calibration committee model separates promotion decisions from direct managers, ensuring cross-team consistency and reducing individual bias in performance assessment.",
    "rationale": "Anchor 1/2: Covers performance operations (reviews, calibration, feedback, PIPs, continuous documentation, high performer management, managed exits). Maps to C14-O1 through C14-O4, C14-O6 through C14-O9."
  },
  {
    "anchorId": "C14-2",
    "capabilityId": "C14",
    "sourceTopic": "Promotion Readiness & Career Architecture",
    "level1Developing": "Promotion packets are reactive ('they've been doing this for a while'). No systematic approach to building promotion cases over time. Gap-filling stretch assignments not planned.",
    "level2Emerging": "Beginning to plan promotions 1-2 cycles ahead. Starting to identify and fill gaps through stretch assignments. Some awareness of what calibration committees look for.",
    "level3Competent": "Promo packets built over 2-3 cycles with deliberate gap-filling stretch assignments. Cross-team evidence gathering is systematic. Promotion cases well-structured with specific impact examples matching rubric criteria.",
    "level4Distinguished": "Promotion success rate demonstrates calibration skill — candidates prepared over multiple cycles with evidence portfolios that committees approve without extensive debate. Gap-filling stretch assignments deliberately designed to build specific next-level evidence. Career development reputation attracts internal transfers to the team. Coaches at least one peer EM on promotion readiness with structured frameworks.",
    "level5Advanced": "Promotion track record is among the best in the org. Promo packet construction is a model others follow. Career architecture creates clear, achievable growth paths. Development reputation is a recruiting advantage. Company benchmark: Amazon's promotion process requires documented evidence of sustained next-level impact over multiple review cycles, and Google's promo committees evaluate packets independently of manager advocacy.",
    "rationale": "Anchor 2/2: Covers career growth architecture (promotion planning, stretch assignments, evidence building, career paths). Maps to C14-O5. Intentionally separate from performance operations — evaluation vs. development are distinct sub-skills."
  }
]
