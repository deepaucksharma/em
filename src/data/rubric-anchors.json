[
  {
    "anchorId": "C1-1",
    "capabilityId": "C1",
    "sourceTopic": "Org Design & Team Topologies",
    "level1Developing": "Scope: Own team only. Key Behavior: Inherits team structure and works within it; recognizes ownership confusion when it causes problems. Artifact: None — relies on existing org charts. Distinguishing Test: Cannot articulate why the team is structured this way.",
    "level2Emerging": "Scope: Own team + adjacent. Key Behavior: Recognizes when team structure causes friction; beginning to track ownership gaps; identifies Conway's Law effects but doesn't initiate structural changes. Artifact: Ownership gap list or shared-responsibility inventory. Distinguishing Test: Can name structural friction points but hasn't proposed fixes.",
    "level3Competent": "Scope: Area (2-4 teams). Key Behavior: Assesses team cognitive load and advocates for structural changes; maintains clear ownership registry; proposes and executes team splits/merges when needed; applies Conway's Law to align architecture and org structure. Artifact: Team topology registry, cognitive load assessment, re-org proposal with rationale. Distinguishing Test: Has executed at least one intentional structural change based on data.",
    "level4Distinguished": "Scope: Org (5+ teams). Key Behavior: Proactively proposes team topology changes based on cognitive load analysis and value-stream mapping, influencing org-level design before bottlenecks surface; executes team changes with <4 weeks to full productivity. Artifact: Org-wide topology map with quarterly reviews, delegation charter, cross-team dependency board. Distinguishing Test: Org structure decisions are proactive (data-driven proposals) rather than reactive (responding to crises).",
    "level5Advanced": "Scope: Company / multi-org. Key Behavior: Designs and evolves organizational models at company scale; builds self-correcting org structures that adapt to strategic shifts without top-down re-orgs; institutions (platform teams, ownership registries, health dashboards) outlive any individual leader. Artifact: Org design principles doc adopted company-wide, self-service org health tooling, documented org-design playbook used by other Directors. Distinguishing Test: The org design practices they built continue operating effectively after they move to a new role. Company benchmark: Spotify's tribe/squad/chapter model demonstrates self-correcting org design at scale, while Amazon's two-pizza team principle enforces cognitive load limits through structural constraints rather than management oversight.",
    "rationale": "Anchor 1/2: Covers structural design (team splits/merges, ownership, Conway's Law). Maps to C1-O1 through C1-O5."
  },
  {
    "anchorId": "C1-2",
    "capabilityId": "C1",
    "sourceTopic": "Cross-Team Strategy & Long-Horizon Planning",
    "level1Developing": "Scope: Own team, current quarter. Key Behavior: Focuses on own team's goals without considering cross-team impact; planning horizon limited to current quarter; relies on manager for org-level context. Artifact: Team-level sprint plans. Distinguishing Test: Cannot describe how their team's work connects to org strategy.",
    "level2Emerging": "Scope: Own team + adjacent, 2 quarters. Key Behavior: Beginning to track dependencies across adjacent teams; starting to consider 2-quarter planning horizons; occasionally contributes to org-level discussions when invited. Artifact: Dependency list for adjacent teams, multi-quarter roadmap draft. Distinguishing Test: Aware of cross-team dependencies but manages them reactively.",
    "level3Competent": "Scope: Area, 3-4 quarters. Key Behavior: Proactively manages cross-team dependencies and alignment; translates business strategy into team-level goals with explicit success criteria; contributes to org-level strategy discussions; identifies systemic issues crossing team boundaries. Artifact: Cascading OKRs with business outcome links, cross-team dependency board, strategy translation doc. Distinguishing Test: Can articulate how every team goal connects to a business outcome.",
    "level4Distinguished": "Scope: Org, 4+ quarters. Key Behavior: Drives cross-team initiatives spanning multiple quarters with explicit dependency maps and milestone tracking; anticipates org-level shifts and positions teams proactively; shapes org strategy as a peer contributor. Artifact: Multi-quarter strategic roadmap with dependency mapping, org-level OKR framework, crisis playbook. Distinguishing Test: Org-level strategy reflects their input; other Directors reference their strategic frameworks.",
    "level5Advanced": "Scope: Company, multi-year. Key Behavior: Sets strategic direction for the engineering org; builds strategy frameworks adopted by peers; navigates M&A, divestitures, and company pivots while maintaining organizational coherence; creates institutions that sustain strategic alignment beyond individual leaders. Artifact: Company-level engineering strategy doc, M&A integration playbook, strategy decomposition framework used by other orgs. Distinguishing Test: Strategic frameworks they created are used by leaders they don't manage. Company benchmark: Amazon's 'working backwards' process forces strategic alignment from customer outcome to team-level execution, while Microsoft's multi-year platform bets under Nadella demonstrate how engineering strategy can drive company-level transformation.",
    "rationale": "Anchor 2/2: Covers strategic leadership altitude (multi-quarter planning, cross-team alignment, org-level influence, strategy-to-team translation). Maps to C1-O6 through C1-O16. Intentionally separate from structural design — strategy vs. structure are distinct sub-skills."
  },
  {
    "anchorId": "C2-1",
    "capabilityId": "C2",
    "sourceTopic": "Strategic Alignment & Roadmapping",
    "level1Developing": "Scope: Own team only. Key Behavior: Plans reactively — PM tells you what to build; OKRs are task lists; engineering impact described in technical terms only. Artifact: None — no capacity model or planning doc. Distinguishing Test: Cannot explain trade-offs made during planning or why specific work was prioritized.",
    "level2Emerging": "Scope: Own team (emerging). Key Behavior: Participates in planning beyond task execution; writes OKRs with some outcome focus; acknowledges trade-offs but documents them inconsistently. Artifact: Draft OKRs with partial outcome language. Distinguishing Test: Can describe what was deprioritized but rationale is verbal, not written.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Runs structured quarterly planning with capacity model; writes outcome-oriented OKRs (max 3 per team); translates engineering impact to business metrics; maintains explicit 'not doing' list. Artifact: Capacity model, planning doc with trade-offs, outcome-oriented OKRs. Distinguishing Test: Stakeholders can find the 'not doing' list and reference it independently.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Planning process documented and adopted by adjacent teams; OKRs produce measurable outcomes reviewed mid-quarter with course corrections; PM partnership operates as strategic co-ownership. Artifact: Shared planning template adopted by peers; mid-quarter OKR review with documented course corrections. Distinguishing Test: Engineering impact framed in terms leadership uses in their own communications (revenue, retention, risk reduction).",
    "level5Advanced": "Scope: Org. Key Behavior: Org-level planning drives cross-team alignment; OKRs directly tied to company strategy with measurable outcomes; planning process is the standard others adopt. Artifact: Org-wide planning framework with adoption metrics; board-level engineering ROI narrative. Distinguishing Test: Strategic partner to PM/business leadership; planning framework used by teams the leader doesn't manage. Company benchmark: Google's OKR system (adapted from Intel's Andy Grove) aligns team-level planning to company strategy through quarterly scoring and transparent cross-team visibility into priorities and trade-offs.",
    "rationale": "Anchor 1/2: Covers strategic planning mechanics (planning process, OKRs, roadmapping, capacity models). Maps to C2-O1, C2-O5, C2-O6."
  },
  {
    "anchorId": "C2-2",
    "capabilityId": "C2",
    "sourceTopic": "Trade-Off Discipline & Decision Rigor",
    "level1Developing": "Scope: Own team only. Key Behavior: Difficulty saying no; trade-offs implicit or avoided; decisions escalated or deferred rather than framed. Artifact: None — no trade-off documentation. Distinguishing Test: Cannot name what was deprioritized to make room for current work.",
    "level2Emerging": "Scope: Own team (emerging). Key Behavior: Beginning to push back on requests with basic trade-off framing; starting to differentiate reversible from irreversible decisions. Artifact: Informal awareness of trade-offs but not documented. Distinguishing Test: Can explain trade-offs verbally but stakeholders don't have written reference.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Declines requests by making trade-offs visible ('yes, if we deprioritize X'); matches decision rigor to reversibility; challenges assumptions with first principles when situation demands it. Artifact: Written trade-off briefs; explicit 'not doing' list shared with stakeholders. Distinguishing Test: Stakeholders own the trade-off decision rather than feeling told no.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Trade-off framing applied proactively — stakeholders receive options analysis before escalating; decision rigor calibrated to stakes with written alternatives for irreversible choices; first principles thinking reframes problems to unlock novel solutions. Artifact: Living 'not doing' artifact stakeholders reference independently; alternatives analysis for irreversible decisions. Distinguishing Test: Reversible decisions ship within days; irreversible decisions include documented alternatives.",
    "level5Advanced": "Scope: Org. Key Behavior: Trade-off discipline is an org model — other teams adopt the practice; strategic 'not doing' decisions create organizational clarity; decision framework handles ambiguity at scale; first principles thinking produces novel solutions influencing org strategy. Artifact: Decision framework adopted across teams; documented track record of assumption challenges that saved significant investment. Distinguishing Test: Other leaders reference the decision framework without being asked to use it. Company benchmark: Amazon's Type 1/Type 2 decision framework and 'disagree and commit' principle provide structural clarity for balancing speed with rigor at scale — reversible decisions ship fast, irreversible decisions get written analysis.",
    "rationale": "Anchor 2/2: Covers prioritization discipline (saying no, trade-off framing, decision rigor, first principles thinking). Maps to C2-O2, C2-O3, C2-O4. Intentionally separate from planning mechanics — strategic discipline vs. process execution are distinct sub-skills."
  },
  {
    "anchorId": "C3-1",
    "capabilityId": "C3",
    "sourceTopic": "Technical Strategy & System Ownership",
    "level1Developing": "Scope: Individual. Key Behavior: Follows technical direction set by TL/Staff without contributing architectural input. Artifact: None — relies on others' design docs. Distinguishing Test: Cannot identify technical trade-offs in their team's system without prompting.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Participates in design reviews and forms technical opinions; begins advocating for system health based on data. Artifact: Written comments on design docs with specific technical concerns. Distinguishing Test: Can explain their team's architecture and identify one concrete improvement, but hasn't driven it.",
    "level3Competent": "Scope: Team (proactive). Key Behavior: Co-authors tech strategy with TL/Staff; runs design reviews with clear criteria; makes build-vs-buy decisions with TCO analysis. Artifact: Tech strategy doc with current→target→migration path; searchable decision log. Distinguishing Test: Design review criteria exist as a written checklist; build-vs-buy decisions include documented TCO.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Architecture decisions account for cross-team system interactions; design review criteria refined from post-mortems; influences platform direction through written proposals. Artifact: Cross-team architecture proposals with data-backed justification; post-mortem-derived review criteria. Distinguishing Test: Peer EMs seek technical input on their architecture decisions; design review improvements trace to specific production learnings.",
    "level5Advanced": "Scope: Org. Key Behavior: Sets multi-quarter technical vision adopted across teams; architecture review process institutionalized; drives platform thinking. Artifact: Org-wide technical strategy doc referenced by multiple teams; institutionalized review process with measured outcomes. Distinguishing Test: Technical governance scales across teams without creating bottlenecks; vision doc updated quarterly with delivery reality checks. Company benchmark: Google's design doc culture requires written technical proposals with explicit alternatives analysis before implementation, and Amazon's one-way/two-way door framing applies appropriate rigor to architecture decisions based on reversibility.",
    "rationale": "Anchor 1/2: Covers technical vision, design reviews, and system ownership. Maps to C3-O1, C3-O2, C3-O5 through C3-O8."
  },
  {
    "anchorId": "C3-2",
    "capabilityId": "C3",
    "sourceTopic": "Tech Debt & Platform Investment",
    "level1Developing": "Scope: Individual. Key Behavior: Acknowledges tech debt exists but doesn't track or prioritize it. Artifact: None — tech debt is undocumented. Distinguishing Test: Cannot quantify any tech debt item's impact on delivery velocity or incident risk.",
    "level2Emerging": "Scope: Team (informal). Key Behavior: Tracks tech debt informally; makes occasional build-vs-buy arguments with basic cost comparison. Artifact: Informal tech debt list; basic cost comparisons. Distinguishing Test: Can name the top 3 tech debt items but cannot quantify their business impact.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Manages 15-20% sprint capacity for debt; quantifies debt in business terms; makes build-vs-buy decisions with TCO analysis. Artifact: Tech debt registry with cost-of-delay and remediation estimates; documented build-vs-buy decisions. Distinguishing Test: Debt allocation is visible in sprint planning; each registry item has a business impact estimate.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Identifies systemic tech debt spanning team boundaries; proposes remediation with velocity impact measurement; evaluates platform ROI across consuming teams. Artifact: Cross-team debt remediation proposals with before/after velocity data; platform investment ROI analysis. Distinguishing Test: Tech debt allocation defended to leadership with delivery speed correlation data; platform decisions evaluated with multi-team adoption metrics.",
    "level5Advanced": "Scope: Org. Key Behavior: Drives strategic tech debt paydown with measurable velocity impact; platform investments create org-wide leverage with documented ROI. Artifact: Org-level tech debt report in business terms; platform adoption dashboard with multi-team ROI. Distinguishing Test: Tech debt investment is a first-class budget line item reviewed alongside feature delivery; platform teams have measurable internal adoption targets. Company benchmark: Stripe's engineering effectiveness team quantifies tech debt in developer-hours-lost and prioritizes remediation by ROI, while Google's infrastructure teams maintain explicit SLOs that create accountability for platform investment outcomes.",
    "rationale": "Anchor 2/2: Covers tech debt management, build-vs-buy, and platform investment decisions. Maps to C3-O3, C3-O4, C3-O9 through C3-O11. Intentionally separate from technical vision — investment prioritization is a distinct sub-skill."
  },
  {
    "anchorId": "C4-1",
    "capabilityId": "C4",
    "sourceTopic": "Operating Cadence & Process",
    "level1Developing": "Scope: Individual. Key Behavior: Follows existing processes without evaluating their effectiveness; interrupts handled reactively. Artifact: None — no documented cadence or operating norms. Distinguishing Test: Cannot describe the purpose of team ceremonies or how information flows.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: Establishing regular operating cadence; sprint predictability improving; interrupt management attempted but inconsistent. Artifact: Basic meeting schedule with stated purposes. Distinguishing Test: Can articulate why each ceremony exists but retro action items still incomplete >50% of the time.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Clear operating cadence with defined outputs; interrupt rotation implemented; retro action items tracked to >80% completion; engineers have 4+ hours of daily focus time. Artifact: Team operating manual; velocity bottleneck analysis using value stream mapping. Distinguishing Test: New hire understands the operating system within first week; focus time is structurally protected by calendar policy.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Operating cadence requires minimal intervention — team self-corrects when rituals drift; toil eliminated through quarterly reviews with measurable time reclaimed; process improvements shared with adjacent teams. Artifact: Documented process improvements with before/after metrics; async contribution rates measured. Distinguishing Test: Team operates effectively when EM is out for 2+ weeks; remote/hybrid participation is equitable by measured contribution.",
    "level5Advanced": "Scope: Org. Key Behavior: Designs and documents an operating rhythm framework (cadence templates, meeting charters, interrupt policies) that peer EMs adopt across the org without top-down mandate; runs quarterly operational health reviews across all teams using standardized metrics (retro completion rate, focus time hours, ceremony value scores); coaches EMs on adapting cadence to team maturity rather than imposing one-size-fits-all process; conducts systemic velocity analysis across teams using value stream mapping, identifying cross-team bottlenecks invisible at individual team level; audits org-wide focus time quarterly and removes structural obstacles (recurring meetings, approval bottlenecks) at the org level. Artifact: Org operating rhythm framework with adoption metrics; quarterly operational health reviews; cross-team velocity analysis reports; focus time audit results. Distinguishing Test: Teams operate effectively when this leader is unavailable for 2+ weeks — the operating system runs without its architect. Company benchmark: Amazon's weekly business review cadence creates organizational muscle memory; Spotify's squad health checks provide team-level operational self-assessment.",
    "rationale": "Anchor 1/2: Covers team operating system (cadence, process, interrupts, remote practices, velocity diagnosis, focus time protection). Maps to C4-O1, C4-O7 through C4-O13."
  },
  {
    "anchorId": "C4-2",
    "capabilityId": "C4",
    "sourceTopic": "Delivery Predictability & Execution",
    "level1Developing": "Scope: Individual. Key Behavior: Sprint velocity inconsistent; scope creep common; commitments aspirational not evidence-based; delivery risks surface at deadline. Artifact: None — no delivery tracking or capacity model. Distinguishing Test: Cannot state team's commitment accuracy rate or identify the top delivery bottleneck.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Beginning to track delivery metrics; risks identified mid-sprint but mitigation is reactive; starting to use data for capacity planning. Artifact: Basic delivery dashboard; some historical velocity data. Distinguishing Test: Can state last sprint's commitment accuracy but mitigation plans are after-the-fact.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Meeting 85%+ of committed scope for 3+ consecutive quarters; capacity model accounts for interrupts, on-call, and PTO; risks flagged early with mitigation plans. Artifact: Capacity model with interrupt/PTO buffers; delivery dashboard with trend data; documented scope negotiation with stakeholders. Distinguishing Test: Stakeholders trust delivery estimates; scope trade-offs are negotiated proactively, not at deadline.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Delivery predictability maintained through team transitions and scope changes; capacity model refined using actuals-vs-plan variance; teams self-manage scope trade-offs. Artifact: Variance analysis history; delivery practices documented for adoption. Distinguishing Test: Predictability sustained above 85% during organizational disruption; adjacent teams adopt delivery practices.",
    "level5Advanced": "Scope: Org. Key Behavior: Maintains elite-level DORA metrics (deployment frequency, lead time, change failure rate, MTTR) across all teams with dashboards reviewed weekly; sustains >85% delivery predictability through organizational disruptions (re-orgs, leadership transitions, headcount changes) by adjusting capacity models within one sprint of the disruption; coordinates cross-team delivery dependencies using a shared dependency tracker with weekly sync cadence; coaches EMs on capacity modeling, scope negotiation, and DORA metric interpretation through quarterly calibration sessions; runs post-quarter delivery retrospectives analyzing org-level patterns (estimation accuracy, common scope creep sources, interrupt distribution). Artifact: Org-wide DORA dashboard; cross-team dependency tracker; quarterly delivery retrospective outputs; EM capacity modeling calibration records. Distinguishing Test: Delivery predictability survives organizational disruption — the capacity model adapts within one sprint, not one quarter. Company benchmark: Google's DORA research program demonstrates that elite performers maintain all four key metrics simultaneously.",
    "rationale": "Anchor 2/2: Covers delivery outcomes (predictability, capacity planning, scope management). Maps to C4-O2 through C4-O6. Intentionally separate from cadence — process vs. outcomes are distinct sub-skills."
  },
  {
    "anchorId": "C5-1",
    "capabilityId": "C5",
    "sourceTopic": "Cross-Functional Partnership",
    "level1Developing": "Scope: Own team only. Key Behavior: Works with PM on features but relationship is transactional; design involvement is late; dependencies managed reactively. Artifact: None — no shared context docs or triad cadence. Distinguishing Test: Cannot describe PM's or Design's top priorities.",
    "level2Emerging": "Scope: Own team (emerging). Key Behavior: Building working relationship with PM beyond feature handoffs; design involvement improving; starting to manage dependencies more proactively. Artifact: Informal triad communication. Distinguishing Test: Can describe PM's priorities but triad syncs are inconsistent.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Healthy triad with PM/Design (regular syncs, shared ownership, constructive disagreements); technical input shapes product direction; dependencies negotiated proactively; TPM partnership has clear division of labor. Artifact: Weekly triad sync cadence; shared success metrics; dependency tracker. Distinguishing Test: PM includes engineering perspective as co-equal input in planning.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Technical insights shape product direction at the roadmap level; cross-functional disagreements resolved through structured trade-off analysis; TPM and DS partnerships produce joint deliverables with shared success criteria; partner feedback collected and acted upon. Artifact: Joint deliverables with cross-functional partners; partner feedback action plans. Distinguishing Test: Partners proactively seek engineering input on scope and strategy decisions.",
    "level5Advanced": "Scope: Org. Key Behavior: Triad is a model for the org; shared OKRs and joint retrospectives are standard practice; cross-org partnerships brokered at scale; legal/privacy partnerships proactive; partner feedback consistently strong. Artifact: Org-wide triad model with adoption metrics; cross-functional partnership framework. Distinguishing Test: Other teams adopt the triad practices without being asked; partner functions cite this leader as a model collaborator. Company benchmark: Meta's cross-functional triad model with shared OKRs and joint retrospectives treats Eng/PM/Design as co-equal owners of outcomes, not separate reporting chains.",
    "rationale": "Anchor 1/2: Covers peer-level cross-functional relationships (PM/Design triad, TPM, DS, platform partnerships, triad alignment cadence). Maps to C5-O1 through C5-O5, C5-O16."
  },
  {
    "anchorId": "C5-2",
    "capabilityId": "C5",
    "sourceTopic": "Upward Management & Sponsor Building",
    "level1Developing": "Scope: Own team only. Key Behavior: Relationship with manager is status-update focused; no sponsor relationships; political dynamics invisible or confusing. Artifact: None — no strategic alignment with manager. Distinguishing Test: Cannot name manager's top 3 priorities.",
    "level2Emerging": "Scope: Own team (emerging). Key Behavior: Beginning to invest in manager relationship beyond status updates; recognizes importance of sponsors and political capital; starting to navigate org dynamics. Artifact: Occasional written updates to manager. Distinguishing Test: Can articulate manager's priorities but doesn't proactively align to them.",
    "level3Competent": "Scope: Team (strategic). Key Behavior: Manager relationship is strategic — proactive alignment, early risk signaling, mutual trust; building sponsor relationships through consistent delivery; navigating political dynamics constructively; expanding scope through demonstrated capability. Artifact: Weekly written updates aligned to manager's priorities; sponsor relationship with at least one senior leader. Distinguishing Test: Manager advocates for you in rooms you're not in.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Manager relationship produces sponsorship for stretch assignments; multiple sponsors across leadership built through cross-team delivery; political capital invested to unblock cross-team initiatives; scope expansion proposals grounded in demonstrated capability. Artifact: Documented scope expansion proposals with impact evidence; cross-team initiative delivery record. Distinguishing Test: Influence extends beyond direct reports; peers seek advice on org navigation.",
    "level5Advanced": "Scope: Org. Key Behavior: Maintains quarterly 1:1 cadence with 3+ VP-level sponsors, providing strategic engineering briefings and receiving proactive career advocacy. Navigates every leadership transition within 2 weeks — prepares impact summaries, schedules intros, and identifies alignment opportunities before the first planning cycle. Pre-wires all cross-org decisions involving VP+ stakeholders through structured 1:1 preparation, ensuring consensus before formal meetings. Coaches peer Directors and senior EMs on managing-up technique, stakeholder mapping, and sponsor relationship building. Submits scope expansion proposals grounded in 2+ quarters of demonstrated delivery with documented cross-functional partner support. Artifact: Quarterly sponsor briefing records; leadership transition playbooks; scope expansion proposals with partner endorsements; peer coaching session logs. Distinguishing Test: VP-level sponsors proactively bring opportunities and advocate in rooms without being asked — the leader's reputation generates inbound invitations to high-visibility initiatives. Company benchmark: Amazon's Leadership Principles expect leaders to actively build sponsor relationships and 'Earn Trust' through consistent delivery and transparent communication with senior leadership.",
    "rationale": "Anchor 2/2: Covers upward influence (managing up, sponsor relationships, political capital, scope navigation). Maps to C5-O6 through C5-O15. Intentionally separate from peer partnerships — upward influence is a distinct sub-skill."
  },
  {
    "anchorId": "C6-1",
    "capabilityId": "C6",
    "sourceTopic": "Team Health & Execution",
    "level1Developing": "Scope: Own team only. Key Behavior: Holds 1:1s but they're mostly status updates; avoids difficult conversations; capacity planning is rough estimate; handles retention reactively (after notice given). Artifact: None — no structured 1:1 notes, no capacity model, no career development docs. Distinguishing Test: Cannot describe each report's career goals or current development areas.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: 1:1s mix status with some career discussion; beginning to address performance issues (though slowly); capacity model emerging; starting to notice flight risk signals. Artifact: 1:1 notes exist but inconsistent; basic capacity spreadsheet. Distinguishing Test: Can name career goals for most reports but has no written development plan for any.",
    "level3Competent": "Scope: Own team (systematic). Key Behavior: 1:1s are trust-building with career development mix; addresses underperformance within 2 weeks of identification; capacity model accounts for interrupts, on-call, PTO; retention managed proactively via stay interviews and flight risk identification. Artifact: Written development plans for each report; capacity model updated monthly; stay interview notes. Distinguishing Test: Can show written evidence of proactive retention actions taken before resignation notices.",
    "level4Distinguished": "Scope: Team + cross-team coaching influence. Key Behavior: Measures psychological safety with structured assessments (Edmondson scale or equivalent) and builds action plans for any dimension below threshold; addresses underperformance through skill/will diagnosis with differentiated intervention paths; stay interviews surface retention risks before resignations. Artifact: Quarterly psych safety assessment results with action plans; skill/will diagnosis docs; engagement survey data showing team members cite manager as reason for staying. Distinguishing Test: Can show differentiated intervention examples — a skill gap addressed differently from a will gap — with tracked outcomes.",
    "level5Advanced": "Scope: Org-wide coaching culture. Key Behavior: Runs quarterly psychological safety assessments and builds action plans for any dimension below 4.0/5.0 within one sprint; initiates every difficult conversation within 48 hours — zero avoidance pattern across 12+ months; calibrates workload against sustainable pace by reviewing utilization data bi-weekly; conducts quarterly retention reviews with structured stay interviews for all reports; coaches Staff+ engineers through organizational influence challenges, not just technical decisions. Artifact: 12-month track record of zero-avoidance difficult conversations; quarterly retention plans reviewed with leadership; Staff+ coaching artifacts. Distinguishing Test: Research identifies coaching as the #1 behavior distinguishing great managers — at this level, anonymous upward feedback consistently confirms coaching quality across all reports. Company benchmark: Google's Project Oxygen research identified coaching as the #1 behavior distinguishing great managers, measured through anonymous upward feedback surveys.",
    "rationale": "Anchor 1/2: Covers EM-level coaching (1:1s, underperformance, retention, stretch assignments, Staff+ management, career development). Maps to C6-O1 through C6-O5, C6-O8 (stay interviews), C6-O9 (Staff+ management), C6-O10 (career conversations), C6-O13 (career pathways)."
  },
  {
    "anchorId": "C6-2",
    "capabilityId": "C6",
    "sourceTopic": "Managing Managers (Director Track)",
    "level1Developing": "Scope: n/a at EM level. Early Director: still doing EM-level work. Key Behavior: Skip-levels infrequent or awkward; EMs operate independently without cross-calibration; delegation is uncomfortable. Artifact: None — no skip-level notes, no cross-calibration records. Distinguishing Test: Cannot describe each EM's management development areas or current coaching priorities.",
    "level2Emerging": "Scope: n/a at EM level. Early Director (reactive). Key Behavior: Beginning to establish skip-level rhythm; starting to coach EMs beyond project management; cross-calibration conversations starting; delegation improving. Artifact: Basic skip-level notes; some EM coaching notes. Distinguishing Test: Can describe EM strengths/weaknesses but has no written EM development plans.",
    "level3Competent": "Scope: Director managing multiple EMs. Key Behavior: Regular skip-level 1:1s providing org insight; EMs coached on management craft (not just project management); cross-EM calibration sessions before perf cycles; EM bench strength being developed. Artifact: Skip-level notes with trend tracking; EM development plans; cross-calibration session outputs; delegation boundaries documented. Distinguishing Test: Can show evidence of coaching an EM on a management skill (not a project) with measurable improvement.",
    "level4Distinguished": "Scope: Director with cross-org influence. Key Behavior: Skip-level insights drive proactive interventions before issues escalate; EMs coached on full management craft with specific behavioral feedback; cross-EM calibration produces consistent rating distributions across teams; EM bench development includes deliberate stretch assignments. Artifact: Proactive intervention examples triggered by skip-level data; EM behavioral feedback records; cross-team calibration alignment docs; stretch assignment tracking. Distinguishing Test: Can show an example where skip-level data triggered an intervention that prevented an escalation — reactive Directors only discover issues after they escalate.",
    "level5Advanced": "Scope: Org-wide leadership development. Key Behavior: Delegates full operational authority to EMs, intervening only on cross-team and strategic issues; converts skip-level insights into proactive interventions within one week of pattern detection; runs pre-calibration alignment sessions ensuring consistent standards across all EMs; maintains succession pipeline with at least one ready-now candidate per critical role; reviews org health dashboard weekly, investigating when any leading indicator crosses warning threshold. Artifact: Succession readiness reviews (quarterly); pre-calibration alignment session records; org health dashboard with intervention log. Distinguishing Test: EMs make independent decisions without Director approval for team-scope matters — Director-level heroics are a failure mode, not a strength. Company benchmark: Amazon evaluates Directors on how independently their EMs operate — Director-level heroics are a failure mode, not a strength.",
    "rationale": "Anchor 2/2: Covers Director-level EM development (coaching EMs, skip-levels, bench building, succession planning). Maps to C6-O6, C6-O7, C6-O11 (coaching struggling EMs), C6-O12 (succession plans). Intentionally separate — different altitude of practice."
  },
  {
    "anchorId": "C7-1",
    "capabilityId": "C7",
    "sourceTopic": "Stakeholder Management & Influence",
    "level1Developing": "Scope: Own team only. Key Behavior: Communicates status when asked; frames asks in engineering terms; delivers bad news late or sugar-coated; no executive presence. Artifact: None — no written status updates, no proposals. Distinguishing Test: Cannot explain what their VP cares about or how their team's work connects to business outcomes.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Starting to provide proactive updates without being asked; beginning to frame proposals with some business context; bad news delivery improving but still sometimes delayed. Artifact: Occasional written updates; proposals exist but lack business framing. Distinguishing Test: Can articulate business value of their team's work but defaults to engineering terms under pressure.",
    "level3Competent": "Scope: Team + immediate stakeholders. Key Behavior: Proactive weekly written updates; proposals framed in business terms with ROI; bad news delivered early with mitigation plan; political capital built through consistent delivery and helping others. Artifact: Weekly written status updates; proposals with ROI analysis; documented bad-news-plus-mitigation communications. Distinguishing Test: Stakeholders voluntarily seek out this EM's updates; bad news never surprises leadership.",
    "level4Distinguished": "Scope: Cross-team + exec audience. Key Behavior: Tailors communication altitude to audience — IC-level detail for engineers, business outcome framing for VPs; proposals include pre-built options with trade-off analysis; bad news delivered with root cause analysis and mitigation options before being asked; builds political capital by unblocking cross-team dependencies. Artifact: Multi-audience communication artifacts (same decision, different framings); trade-off option docs; unsolicited mitigation plans. Distinguishing Test: Can present the same decision to an IC audience and a VP audience with appropriate framing for each — not just volume adjustment but actual altitude change.",
    "level5Advanced": "Scope: Org-wide + executive leadership. Key Behavior: Produces written narrative documents (Amazon 6-pager style) for every org-level decision, forcing structured thinking and eliminating slide-deck hand-waving; delivers bad news to VP+ audiences within 4 hours with root cause analysis, mitigation options, and next-update timeline; tailors the same decision to three audiences (IC, peer EM, VP) with distinct altitude and framing within the same day; surfaces cross-org alignment gaps proactively through written pre-reads and 1:1 pre-wiring before formal meetings; coaches peer leaders on communication altitude, trade-off framing, and bad-news delivery technique. Artifact: Written narrative documents for org-level decisions; multi-audience communication artifacts; cross-org alignment docs produced without being asked; peer coaching session records. Distinguishing Test: Drives cross-org decisions through written narratives and strategic framing without needing escalation — others cite this person's communication as a model. Company benchmark: Amazon's 6-pager narrative format eliminates slide-deck hand-waving and forces structured thinking, while Netflix's 'context, not control' model delegates decisions with clear strategic framing.",
    "rationale": "Anchor 1/2: Covers external-facing communication (status updates, bad news delivery, exec presentations, managing up, difficult conversations, communication scaling). Maps to C7-O2 through C7-O5, C7-O8 (difficult conversations), C7-O9 (communication scaling), C7-O11 (explicit trade-off communication)."
  },
  {
    "anchorId": "C7-2",
    "capabilityId": "C7",
    "sourceTopic": "Decision Making & Prioritization",
    "level1Developing": "Scope: Own team only. Key Behavior: Decisions made in meetings without documentation; difficulty saying no; most decisions escalated or deferred; first principles thinking is aspirational, not practiced. Artifact: None — no decision logs, no RFC process. Distinguishing Test: Cannot list the 3 most important decisions made last quarter or explain why they were made.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Some decisions documented but inconsistently; starting to differentiate reversible from irreversible decisions; beginning to push back on requests with basic trade-off framing. Artifact: Occasional decision docs; some RFCs. Distinguishing Test: Can explain the reversible/irreversible distinction but doesn't consistently apply it — treats most decisions as high-stakes.",
    "level3Competent": "Scope: Team + adjacent teams. Key Behavior: DACI/RFC culture established; decisions classified by reversibility and handled appropriately; says no with trade-off framing; first principles analysis applied to significant decisions; systems thinking catches second-order effects. Artifact: Decision log with DACI assignments; RFC archive; documented trade-off analyses. Distinguishing Test: Can show a decision where they explicitly chose speed (Type 2) and one where they chose rigor (Type 1) — with reasoning for each.",
    "level4Distinguished": "Scope: Cross-team decision architecture. Key Behavior: Decision framework shared with peers and influencing adoption on adjacent teams; makes high-stakes decisions under ambiguity with documented reasoning that holds up in retrospect; first principles analysis produces novel approaches; systems thinking anticipates cross-team second-order effects before they materialize. Artifact: Decision framework adopted by adjacent teams; post-decision reviews showing reasoning quality; novel reframing examples. Distinguishing Test: Can show a decision made under ambiguity where the documented reasoning held up 6+ months later — not just lucky outcomes but sound process.",
    "level5Advanced": "Scope: Org-wide decision standards. Key Behavior: Maintains and evolves an org-wide decision framework with Type 1/Type 2 classification, DACI templates, and RFC processes that peer teams adopt as their default; classifies decisions under ambiguity by explicitly documenting what is known, unknown, and unknowable — then selects the appropriate decision speed; applies first principles analysis to cross-org problems, reframing debates from surface-level options to underlying assumptions; maps second-order effects across teams before committing to org-level decisions using a structured impact analysis template; coaches peer leaders on decision framework adoption and trade-off framing through quarterly decision review sessions. Artifact: Org-wide decision framework documentation with adoption metrics; Type 1/Type 2 classification examples; disagree-and-commit culture operating in documented decision outcomes; cross-org impact analysis templates. Distinguishing Test: The decision framework has outlived any single decision — new leaders adopt it during onboarding without needing the original author. Company benchmark: Amazon's Type 1/Type 2 decision framework and 'disagree and commit' principle provide structural clarity for decision-making at scale.",
    "rationale": "Anchor 2/2: Covers decision architecture (DACI/RFC, trade-off framing, reversibility, authority distribution). Maps to C7-O1, C7-O6, C7-O7, C7-O10 (decision authority distribution). Intentionally separate — communication vs decision-making are distinct sub-skills."
  },
  {
    "anchorId": "C8-1",
    "capabilityId": "C8",
    "sourceTopic": "Incident Response & Command",
    "level1Developing": "Scope: Individual. Key Behavior: Reacts to incidents without defined process; post-mortems blame individuals or don't happen; on-call health unmonitored. Artifact: None. Distinguishing Test: Cannot describe incident command roles or the team's on-call page rate.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Participates in incident response; post-mortems attempted but action items incomplete; on-call load tracked but not actively managed. Artifact: Post-mortem documents exist but completion tracking is informal. Distinguishing Test: Post-mortems happen but action item completion rate is below 50%.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: ICS roles defined and practiced; blameless post-mortems with >90% action item completion; on-call health maintained at <2 off-hours pages/night. Artifact: ICS roster, post-mortem template with tracked action items, on-call health dashboard. Distinguishing Test: Repeat incidents from same root cause drop to near-zero; on-call is viewed as sustainable.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Incident response patterns shared across teams; post-mortem themes analyzed quarterly for systemic improvements; on-call excellence is a team differentiator. Artifact: Cross-team incident pattern analysis; on-call health benchmarks across teams. Distinguishing Test: Other teams adopt incident response practices; MTTR consistently below area average.",
    "level5Advanced": "Scope: Org. Key Behavior: Incident management culture institutionalized; error budget model drives reliability investment; org-wide MTTR improving year-over-year. Artifact: Org-wide incident management framework with measured outcomes. Distinguishing Test: Reliability metrics used as first-class business KPIs; error budget policy automatically governs feature/reliability trade-offs. Company benchmark: Google SRE's blameless postmortem culture (documented in the SRE book) institutionalizes learning from incidents without blame, while PagerDuty's open-source incident response framework provides a scalable ICS model adopted across the industry.",
    "rationale": "Anchor 1/2: Covers reactive operational excellence (incident command, post-mortems, on-call health). Maps to C8-O1 through C8-O3."
  },
  {
    "anchorId": "C8-2",
    "capabilityId": "C8",
    "sourceTopic": "Systemic Risk Reduction",
    "level1Developing": "Scope: Individual. Key Behavior: Risk management is purely reactive — learns only from production incidents; no pre-launch risk assessment. Artifact: None. Distinguishing Test: Cannot identify the top 3 failure modes in their system.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: Conducts ad hoc risk assessments before major launches; beginning to track vendor dependencies; awareness of compliance requirements. Artifact: Launch checklist exists but not consistently used. Distinguishing Test: Can name critical dependencies but has no tested fallback for any of them.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: FMEA before every major launch; vendor risk registry maintained; compliance requirements mapped to engineering practices; quarterly game days run. Artifact: FMEA documents, vendor risk registry, compliance matrix, game day results. Distinguishing Test: Zero Sev1 incidents on launches; game days reveal and fix gaps proactively.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Risk management practices shared across teams; game days include cross-team failure scenarios; vendor management coordinated at area level. Artifact: Cross-team risk register; shared game day exercises. Distinguishing Test: Cross-team incidents handled smoothly; vendor risk mitigation coordinated rather than duplicated.",
    "level5Advanced": "Scope: Org. Key Behavior: Proactive resilience is an organizational norm; chaos engineering culture established; error budgets govern feature/reliability trade-offs. Artifact: Org-wide resilience framework with chaos engineering program. Distinguishing Test: Resilience investment justified with data; org handles major incidents without heroics. Company benchmark: Netflix's Chaos Monkey and Simian Army pioneered chaos engineering as a proactive resilience practice, while Google's DiRT (Disaster Recovery Testing) program runs org-wide failure simulations that build resilience before incidents occur.",
    "rationale": "Anchor 2/2: Covers proactive risk management (failure mode analysis, launch readiness, dependency risk, vendor management, chaos engineering, error budgets). Maps to C8-O4 through C8-O6, C8-O7 (chaos engineering and game days), C8-O8 (error budget management). Intentionally separate from incident response — proactive vs. reactive are distinct sub-skills."
  },
  {
    "anchorId": "C9-1",
    "capabilityId": "C9",
    "sourceTopic": "Metric Selection & Dashboard Design",
    "level1Developing": "Scope: Individual. Key Behavior: No metrics tracked; delivery health is anecdotal; dashboards don't exist or are never consulted. Artifact: None. Distinguishing Test: Cannot state team's deployment frequency or lead time.",
    "level2Emerging": "Scope: Team (reactive). Key Behavior: Basic DORA metrics tracked; dashboards exist but checked infrequently; metrics used to report status, not drive decisions. Artifact: Basic delivery dashboard. Distinguishing Test: Can state DORA numbers but cannot name a decision driven by metric data in the last quarter.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: DORA metrics tracked weekly with trend analysis; developer satisfaction measured quarterly; metric pairings prevent single-metric gaming; feature flag lifecycle governed. Artifact: Delivery dashboard with trend data; developer satisfaction survey; metric pair documentation. Distinguishing Test: Every active metric has informed a decision within 90 days; no surveillance culture complaints.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: Metric frameworks adapted to team maturity; metrics benchmarked across teams; insights shared to improve adjacent teams; metric sophistication evolves based on decisions driven. Artifact: Cross-team metric benchmarks; maturity-appropriate metric recommendations for each team. Distinguishing Test: Teams at different maturity levels have appropriately different metric sets; no death-by-metrics.",
    "level5Advanced": "Scope: Org. Key Behavior: Metrics culture established org-wide; measurement frameworks (DORA, SPACE) applied appropriately; engineering metrics integrated with business metrics. Artifact: Org-wide metrics framework with documented decision impact. Distinguishing Test: Business leadership references engineering metrics in planning; engineering metrics drive budget allocation. Company benchmark: Google's DORA metrics framework (deployment frequency, lead time, change failure rate, MTTR) provides a validated, industry-standard measurement model, while the SPACE framework adds developer experience dimensions that prevent over-indexing on throughput alone.",
    "rationale": "Anchor 1/2: Covers metric infrastructure (DORA tracking, developer experience, delivery metrics, dashboards, metric pairings, maturity adaptation). Maps to C9-O1 through C9-O3, C9-O7, C9-O9 (metric pairings), C9-O10 (metric maturity adaptation)."
  },
  {
    "anchorId": "C9-2",
    "capabilityId": "C9",
    "sourceTopic": "Data-Driven Decision Making",
    "level1Developing": "Scope: Individual. Key Behavior: OKRs vague or absent; engineering outcomes not translated to business terms; attrition not classified. Artifact: None. Distinguishing Test: Cannot state team's OKRs or explain how engineering work connects to business outcomes.",
    "level2Emerging": "Scope: Team (emerging). Key Behavior: OKRs defined but not scored quarterly; some engineering outcomes translated to business terms; attrition tracked but not classified. Artifact: OKR document (not actively scored). Distinguishing Test: OKRs exist on paper but team cannot describe how they influenced a prioritization decision.",
    "level3Competent": "Scope: Team (systematic). Key Behavior: Outcome-oriented OKRs scored quarterly with 0.6-0.8 average; engineering outcomes translated to business metrics; attrition classified and patterns addressed; A/B testing with statistical rigor. Artifact: Scored OKRs with quarterly review; business metric translation in investment proposals; attrition classification. Distinguishing Test: >80% of engineering investment proposals include business ROI; regrettable attrition below benchmark.",
    "level4Distinguished": "Scope: Area (2-3 teams). Key Behavior: OKR quality improved through coaching; business metric translation is standard across teams; experimentation infrastructure supports multiple teams. Artifact: OKR quality rubric; cross-team experimentation platform. Distinguishing Test: Engineering leaders invited to business planning conversations based on metric credibility.",
    "level5Advanced": "Scope: Org. Key Behavior: Data-driven decision culture established; engineering metrics integrated into business strategy; experimentation at scale with organizational learning. Artifact: Org-wide experimentation platform; engineering-business metric integration. Distinguishing Test: Engineering investment decisions driven by data at board level; experimentation velocity is a competitive advantage. Company benchmark: Netflix's experimentation platform enables thousands of concurrent A/B tests with rigorous statistical methodology, while Amazon's data-driven culture requires written metrics in every investment proposal — intuition-only arguments are explicitly rejected.",
    "rationale": "Anchor 2/2: Covers decision-making with data (OKR measurement, business impact articulation, experimentation, ROI framing). Maps to C9-O4 through C9-O6, C9-O8. Intentionally separate from metric infrastructure — collecting data vs. using data are distinct sub-skills."
  },
  {
    "anchorId": "C10-1",
    "capabilityId": "C10",
    "sourceTopic": "Budget, Headcount & Resource Planning",
    "level1Developing": "Scope: Own team only. Key Behavior: Requests headcount without financial framing; cloud costs not tracked at team level; build-vs-buy decisions are gut feel; budget is someone else's problem. Artifact: None — no cost dashboards, no ROI analyses. Distinguishing Test: Cannot state team's cost-per-engineer-month or cloud spend trend.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Beginning to think about headcount in terms of ROI; starting to track cloud costs at team level; some awareness of build-vs-buy trade-offs. Artifact: Basic cloud cost tracking; initial headcount justification docs. Distinguishing Test: Can state team cloud costs but cannot explain cost-per-feature or ROI of recent hires.",
    "level3Competent": "Scope: Team + finance partnership. Key Behavior: Headcount justified with ROI analysis; cloud costs attributed and optimized per service; build-vs-buy uses TCO framework; contractor strategy is principled; budget defense prepared for cuts. Artifact: Headcount ROI docs; per-service cost dashboards; TCO analyses; contractor strategy doc. Distinguishing Test: Can show a build-vs-buy decision with documented TCO analysis and a headcount request that survived finance review.",
    "level4Distinguished": "Scope: Cross-team financial influence. Key Behavior: Headcount proposals connect to business outcomes with multi-scenario ROI modeling; cloud cost attribution drives team-level accountability; build-vs-buy decisions documented with reusable TCO analysis; finance partners proactively consult on engineering investment questions. Artifact: Multi-scenario ROI models; team-owned cost dashboards; reusable TCO templates; evidence of finance partnership. Distinguishing Test: Finance partners seek out this leader for investment framing advice — not just approve requests but proactively consult.",
    "level5Advanced": "Scope: Org-wide financial stewardship. Key Behavior: Presents org-level headcount narrative to VP/finance with multi-scenario ROI modeling; runs quarterly FinOps reviews across all teams with per-service cost dashboards and optimization targets; frames every engineering investment as a business case with payback period and opportunity cost; coaches peer leaders on budget defense and ROI framing. Artifact: Org-wide FinOps review outputs; per-team cost dashboards with anomaly detection; documented savings reinvested into highest-ROI initiatives with finance sign-off. Distinguishing Test: Teams self-allocate within strategic guardrails and present their own cost curves rather than receiving top-down mandates — cost discipline is distributed, not centralized.",
    "rationale": "Anchor 1/2: Covers financial stewardship (headcount justification, cloud costs, build-vs-buy, contractor strategy). Maps to C10-O1, C10-O3, C10-O4, C10-O5."
  },
  {
    "anchorId": "C10-2",
    "capabilityId": "C10",
    "sourceTopic": "Strategic Reallocation & ROI Framing",
    "level1Developing": "Scope: Own team only. Key Behavior: Resources stay where assigned regardless of changing priorities; no framework for comparing ROI across investments; reacts to cuts rather than proactively optimizing. Artifact: None — no tiered proposals, no cost-per-outcome tracking. Distinguishing Test: Cannot explain the ROI difference between their top and bottom investment areas.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Beginning to evaluate resource allocation against impact; some awareness that reallocation is a tool; starting to frame engineering investments in outcome terms. Artifact: Basic investment tracking; initial outcome-framed proposals. Distinguishing Test: Can identify which investment area has lowest ROI but hasn't acted on it yet.",
    "level3Competent": "Scope: Team + leadership interface. Key Behavior: Presents resource requests as tiered options with quantified trade-offs; tracks cost-per-outcome; proactively reallocates capacity to highest-impact work; during cuts, communicates explicitly what stops. Artifact: Tiered proposals with quantified trade-offs; cost-per-outcome dashboards; documented reallocation decisions. Distinguishing Test: Can show a proactive reallocation decision made before leadership requested it — with outcome data justifying the shift.",
    "level4Distinguished": "Scope: Cross-team allocation influence. Key Behavior: Tiered resource proposals adopted as default format; cost-per-outcome data actively informs quarterly reallocation decisions; reallocation happens within boundaries without exec approval; adjacent teams adopt the framework. Artifact: Quarterly reallocation reports; tiered proposal templates used by peers; cost-per-outcome trend data. Distinguishing Test: Adjacent teams have adopted the tiered proposal framework — influence extends beyond own scope.",
    "level5Advanced": "Scope: Org-wide resource optimization. Key Behavior: Presents resource allocation proposals to VP/finance with multi-scenario projections and quantified opportunity costs; runs monthly cost-per-outcome reviews across all teams, initiating reallocation within one week of diminishing returns; designs tiered proposal templates adopted by peer leaders; identifies reallocation opportunities proactively by comparing planned-vs-actual impact quarterly. Artifact: Org-wide cost-per-outcome reviews; reallocation decision log with outcome tracking; peer-adopted templates; documented savings reinvested into highest-ROI initiatives. Distinguishing Test: Weekly reviews compare cost-per-outcome across teams and savings are reinvested into highest-ROI initiatives — resource optimization is a continuous process, not an annual exercise.",
    "rationale": "Anchor 2/2: Covers dynamic allocation (tiered proposals, ROI tracking, proactive reallocation, cut communication). Maps to C10-O2, C10-O6, C10-O7, C10-O8. Intentionally separate from financial stewardship — static budgeting vs. dynamic allocation are distinct sub-skills."
  },
  {
    "anchorId": "C11-1",
    "capabilityId": "C11",
    "sourceTopic": "Hiring Process & Bar Raising",
    "level1Developing": "Scope: Participates in hiring. Key Behavior: Joins hiring loops without training; onboarding is informal; headcount requests are 'we need more people'; no engineering brand presence. Artifact: None — no rubrics, no onboarding docs, no capacity model. Distinguishing Test: Cannot describe the interview rubric or what 'bar' means for their team.",
    "level2Emerging": "Scope: Own team hiring (improving). Key Behavior: Hiring loops becoming more structured; some onboarding documentation exists; starting to justify headcount with basic data; referral network emerging. Artifact: Basic interview guides; initial onboarding docs. Distinguishing Test: Has interview guides but interviewers interpret them differently — no calibration.",
    "level3Competent": "Scope: Team hiring with process rigor. Key Behavior: Calibrated hiring loops with trained interviewers and rubrics; structured interviews evaluate demonstrated competencies over pedigree; headcount justified with capacity model; bar raiser discipline maintained under pressure; hiring funnel conversion tracked. Artifact: Interview rubrics with behavioral anchors; calibrated scoring; capacity model linking headcount to deliverables; funnel metrics. Distinguishing Test: Two interviewers evaluating the same candidate produce consistent scores — inter-rater reliability is tracked.",
    "level4Distinguished": "Scope: Cross-team hiring influence. Key Behavior: Hiring bar maintained under volume pressure with quarterly interview calibration sessions; structured process produces consistent signal across interviewers; headcount narrative connects capacity to business strategy with revenue/risk impact; diverse candidate slates through structured sourcing. Artifact: Quarterly calibration session records; inter-rater reliability data; business-framed headcount narratives; diversity pipeline metrics. Distinguishing Test: Can show calibration data proving interviewer consistency improved over time — not just claimed but measured.",
    "level5Advanced": "Scope: Org-wide hiring standards. Key Behavior: Bar raiser discipline maintained under pressure; interviewer calibration is org model; headcount cases win at VP level; competitive hiring strategy wins talent beyond compensation through differentiated value proposition; hiring funnel instrumented with data-driven pipeline management. Artifact: Org-adopted calibration framework; independent reviewer veto process; differentiated value proposition docs; funnel optimization data. Distinguishing Test: Independent interviewers have veto power to protect quality, and structured hiring committees use calibrated rubrics — the process produces consistently strong signal with high inter-rater reliability.",
    "rationale": "Anchor 1/2: Covers hiring mechanics (interview loops, bar raising, headcount justification, interviewer calibration, structured interviews, competitive hiring strategy, funnel optimization). Maps to C11-O1, C11-O2, C11-O7 through C11-O11."
  },
  {
    "anchorId": "C11-2",
    "capabilityId": "C11",
    "sourceTopic": "Onboarding Excellence & Talent Brand",
    "level1Developing": "Scope: Own team only. Key Behavior: Onboarding is informal — new hires figure it out; no engineering brand presence; time-to-productivity not tracked; no referral pipeline. Artifact: None — no onboarding docs, no brand content. Distinguishing Test: New hires take 3+ months to be productive and cannot articulate what their first 30 days should have looked like.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Basic onboarding documentation exists; some buddy pairing; beginning to track new hire experience; occasional blog post or meetup participation. Artifact: Basic onboarding checklist; buddy assignments. Distinguishing Test: Onboarding exists but is inconsistent between hires — no measurable milestones.",
    "level3Competent": "Scope: Team onboarding + emerging brand. Key Behavior: Structured 30/60/90 onboarding with measurable milestones; time-to-productivity tracked and improving; buddy/mentor program formalized; active blog or conference presence emerging; referral pipeline maintained. Artifact: 30/60/90 plan template; time-to-productivity data; blog posts or talks; referral tracking. Distinguishing Test: Can show time-to-productivity improving quarter-over-quarter with specific bottlenecks identified and addressed.",
    "level4Distinguished": "Scope: Cross-team brand influence. Key Behavior: Onboarding continuously refined using new hire feedback surveys at 30/60/90 days; time-to-productivity benchmarked against org averages; engineering brand built through conference talks, blog posts, or open source; referral rate among highest on team. Artifact: New hire feedback surveys with action items; benchmark comparison data; published engineering content; referral metrics. Distinguishing Test: Engineering brand generates inbound senior talent interest — candidates cite team content in interviews.",
    "level5Advanced": "Scope: Org-wide onboarding model. Key Behavior: Time-to-productivity measured and optimized to org-best levels; engineering brand is a recruiting advantage — candidates cite it in interviews; conference talks and open source attract senior talent; onboarding process is a model other teams adopt. Artifact: Org-best time-to-productivity data; structured mentorship program producing <6-week ramp; engineering brand metrics (inbound applications, content reach). Distinguishing Test: Onboarding program adopted by other teams as the standard; engineering brand independently cited by candidates as reason for applying.",
    "rationale": "Anchor 2/2: Covers talent brand and onboarding (30/60/90 programs, time-to-productivity, engineering brand, referrals). Maps to C11-O3 through C11-O6. Intentionally separate from hiring mechanics — acquisition vs. integration are distinct sub-skills."
  },
  {
    "anchorId": "C12-1",
    "capabilityId": "C12",
    "sourceTopic": "Team Charter & Engineering Principles",
    "level1Developing": "Scope: Own team only. Key Behavior: Team culture exists by accident; recognition is sporadic; inclusion is 'we treat everyone the same.' Artifact: None — no team charter, no written norms. Distinguishing Test: Cannot articulate the team's operating norms or explain why the team behaves the way it does.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Starting to be intentional about team norms; some recognition happening but not systematized; awareness of inclusion beyond 'same treatment'; beginning to document engineering practices. Artifact: Draft team norms doc; occasional recognition. Distinguishing Test: Can name desired team norms but they aren't written or consistently reinforced.",
    "level3Competent": "Scope: Team with documented culture. Key Behavior: Team charter written and referenced; engineering principles documented and used in design reviews; regular recognition rituals; active inclusion practices; explicit feedback channels; cultural continuity maintained through growth; toxic behaviors addressed within 48 hours. Artifact: Team charter; engineering principles doc; recognition log; inclusion metrics. Distinguishing Test: New hires can find the team charter in their first week and reference it when navigating team norms.",
    "level4Distinguished": "Scope: Cross-team cultural influence. Key Behavior: Team charter reviewed and evolved quarterly based on feedback; engineering principles referenced in code review and architecture decisions; recognition operates peer-to-peer without manager initiation; inclusion practices produce measurable engagement survey results with action plans. Artifact: Quarterly charter review records; peer recognition log; engagement survey results with action items; gap analysis reports. Distinguishing Test: Recognition happens without manager prompting — the team self-reinforces its culture.",
    "level5Advanced": "Scope: Org-wide culture model. Key Behavior: Culture is intentional, measurable, and self-reinforcing; engineering principles adopted beyond own team; cultural continuity maintained through rapid growth without degradation; inclusive environment measurable and independently confirmed; toxic behaviors caught proactively — no 'missing stair' dynamics; candidates cite team culture as top-3 reason for accepting offer. Artifact: Written culture doc referenced org-wide; explicit team Health Checks making culture measurable and discussable; post-hire survey data confirming culture as hiring advantage. Distinguishing Test: Culture persists through leadership transitions and rapid growth — it's embedded in structures, not dependent on any individual leader.",
    "rationale": "Anchor 1/2: Covers culture foundations (team charter, engineering principles, recognition, inclusion, cultural continuity, inclusive feedback channels, toxic behavior intervention). Maps to C12-O1, C12-O2, C12-O4 through C12-O8."
  },
  {
    "anchorId": "C12-2",
    "capabilityId": "C12",
    "sourceTopic": "Knowledge Sharing & Documentation Culture",
    "level1Developing": "Scope: Own team only. Key Behavior: Knowledge sharing informal — tribal knowledge dominates; documentation sparse and outdated; no structured forums for cross-team learning. Artifact: None — no documentation standards, no tech talks. Distinguishing Test: Ask 3 team members 'where is X documented?' and get 3 different answers (or 'ask person Y').",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Some documentation emerging but inconsistent; occasional knowledge-sharing sessions; beginning to notice bus-factor risk from tribal knowledge. Artifact: Scattered docs; occasional sessions. Distinguishing Test: Documentation exists but team members don't know where to find it or whether it's current.",
    "level3Competent": "Scope: Team with documentation culture. Key Behavior: Documentation standards maintained and enforced; regular tech talks or knowledge-sharing rituals; design doc culture established; onboarding documentation reduces ramp time measurably. Artifact: Documentation standards; regular tech talk schedule; design doc archive; measurable onboarding ramp improvement. Distinguishing Test: New hires can answer most common questions from docs without asking a person — tribal knowledge converted to institutional knowledge.",
    "level4Distinguished": "Scope: Cross-team knowledge influence. Key Behavior: Documentation updated alongside code changes; design doc quality validated through structured peer review; knowledge-sharing sessions run with rotating ownership; documentation standards shared with adjacent teams. Artifact: PR-linked doc updates; peer review records for design docs; rotating presentation schedule; adjacent team adoption. Distinguishing Test: Documentation updates happen in the same PR as code changes — not as a separate task or afterthought.",
    "level5Advanced": "Scope: Org-wide knowledge culture. Key Behavior: Knowledge sharing institutionalized across org (tech talks, wikis, design doc culture); documentation coverage comprehensive and maintained; knowledge-sharing practices adopted by other teams; reduces organizational bus-factor risk at scale. Artifact: Writing culture where decisions are documented by default; design doc requirements institutionalized as organizational practice; cross-team knowledge-sharing metrics. Distinguishing Test: Knowledge sharing is an organizational practice, not an individual habit — it persists regardless of which individuals are on the team.",
    "rationale": "Anchor 2/2: Covers knowledge management (documentation, tech talks, design docs, cross-team learning). Maps to C12-O3. Intentionally separate from culture foundations — knowledge sharing is a distinct operational practice."
  },
  {
    "anchorId": "C13-1",
    "capabilityId": "C13",
    "sourceTopic": "Security Practices & Vulnerability Management",
    "level1Developing": "Scope: Own team only. Key Behavior: Ensures team follows security practices when reminded; patches vulnerabilities reactively; change management informal; compliance handled last-minute. Artifact: None — no threat models, no automated scanning, no vulnerability SLA tracking. Distinguishing Test: Cannot state the team's vulnerability backlog count or time-to-patch for the last critical CVE.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Security practices becoming more consistent without reminders; vulnerability patching improving but SLA sometimes missed; change management process emerging. Artifact: Basic scanning in CI; initial vulnerability tracking. Distinguishing Test: Vulnerability scanning exists but SLA compliance is inconsistent — some criticals take weeks to patch.",
    "level3Competent": "Scope: Team with security workflow. Key Behavior: Security embedded in development workflow (threat modeling, automated scanning, champion rotation); vulnerability SLAs met consistently (P0 <48hrs, P1 <7 days); threat models required for auth/payments/PII. Artifact: Security champion roster; CI/CD scanning config; vulnerability SLA dashboard; threat model archive. Distinguishing Test: Can show vulnerability SLA compliance data for the last quarter and a threat model for the most recent sensitive feature.",
    "level4Distinguished": "Scope: Cross-team security influence. Key Behavior: Engineers initiate threat models without prompting; vulnerability SLAs met with mean resolution time trending downward; security champion program produces engineers who improve practices independently; change management process influencing adjacent teams. Artifact: Mean resolution time trend data; champion-initiated improvements; adjacent team adoption evidence. Distinguishing Test: Engineers proactively start threat models for sensitive features — security is a cultural norm, not a manager-enforced requirement.",
    "level5Advanced": "Scope: Org-wide security culture. Key Behavior: Security culture driven across org; change management standards adopted org-wide; security champion program scaled beyond own team; automated scanning and threat modeling are organizational standards; teams proactively identify risks before they become vulnerabilities. Artifact: Org-wide security standards; champion program operating across multiple teams; proactive risk identification log. Distinguishing Test: Security is a proactive engineering discipline, not a compliance checkbox — zero-trust principles and mandatory security reviews are organizational norms.",
    "rationale": "Anchor 1/2: Covers security operations (threat modeling, vulnerability management, change management, security champions, automated scanning, threat modeling for sensitive features). Maps to C13-O1 through C13-O3, C13-O5 through C13-O7."
  },
  {
    "anchorId": "C13-2",
    "capabilityId": "C13",
    "sourceTopic": "Compliance & Governance Automation",
    "level1Developing": "Scope: Own team only. Key Behavior: Compliance handled last-minute before audits; evidence collection manual; access controls inconsistent; no automated compliance checks. Artifact: None — no automated evidence, no access governance process. Distinguishing Test: Audit preparation takes weeks and involves scrambling for evidence that should already exist.",
    "level2Emerging": "Scope: Own team (improving). Key Behavior: Beginning to embed compliance into workflows; some automated scanning; starting to track access control hygiene; audit preparation moving earlier. Artifact: Basic automated scanning; initial access tracking. Distinguishing Test: Some evidence is automated but audit prep still takes days — manual gaps remain.",
    "level3Competent": "Scope: Team with compliance automation. Key Behavior: Continuous compliance posture with automated evidence collection; access governance automated (quarterly reviews, auto-expiring elevated permissions); audit prep reduced from weeks to days. Artifact: Automated evidence collection pipeline; access governance system with auto-expiry; audit prep checklist. Distinguishing Test: Audit prep takes days, not weeks — evidence is collected continuously rather than assembled retroactively.",
    "level4Distinguished": "Scope: Cross-team compliance influence. Key Behavior: Compliance evidence collected automatically with real-time dashboard — audit preparation requires hours, not days; access governance runs exception-based reviews only; zero repeat findings across consecutive audit cycles; governance automation patterns shared with adjacent teams. Artifact: Real-time compliance dashboard; exception-based access review records; zero repeat findings evidence. Distinguishing Test: Zero repeat findings across consecutive audits — every finding gets a systemic fix, not just a point correction.",
    "level5Advanced": "Scope: Org-wide compliance model. Key Behavior: Zero P1/P2 findings in 3+ consecutive audit cycles; compliance is a competitive advantage, not a burden; governance automation is an org model; continuous compliance frameworks adopted beyond own team. Artifact: Continuous compliance dashboard as daily metric; governance automation templates adopted org-wide; multi-cycle audit history with zero critical findings. Distinguishing Test: Audit readiness is a dashboard metric, not a quarterly project — compliance evidence is a byproduct of engineering workflow, not a separate artifact.",
    "rationale": "Anchor 2/2: Covers compliance and governance (audit readiness, evidence collection, access governance, compliance automation). Maps to C13-O3, C13-O4. Intentionally separate from security operations — compliance is a distinct discipline from vulnerability management."
  },
  {
    "anchorId": "C14-1",
    "capabilityId": "C14",
    "sourceTopic": "Performance Reviews & Calibration",
    "level1Developing": "Scope: Own team only. Key Behavior: Writes vague reviews without specific evidence; calibration prep is last-minute; avoids PIPs entirely; career conversations are infrequent. Artifact: None — reviews lack specific examples or rubric alignment. Distinguishing Test: Cannot cite specific evidence for any rating given.",
    "level2Emerging": "Scope: Own team with HR support. Key Behavior: Writes reviews with improving specificity; prepares for calibration earlier; begins addressing performance issues with HR guidance; career conversations becoming regular. Artifact: Reviews include some specific examples but lack consistent rubric alignment. Distinguishing Test: Can cite examples but cannot defend ratings against calibration committee questioning.",
    "level3Competent": "Scope: Own team with cross-team calibration awareness. Key Behavior: Writes evidence-based reviews with rubric language; maintains weekly running notes; prepares calibration cases with cross-team data; executes PIPs with HR partnership; differentiates high performer development; conducts managed exits with dignity. Artifact: Weekly performance notes per report; calibration prep docs with cross-team comparisons; quarterly career conversation records. Distinguishing Test: Reviews are evidence-based with rubric alignment — calibration prep is routine, not heroic.",
    "level4Distinguished": "Scope: Cross-team calibration influence. Key Behavior: Calibration cases withstand committee scrutiny with ratings rarely revised; manages full performance spectrum — stretch assignments for high performers, structured PIPs for underperformers, dignified exits with knowledge transfer; career conversations produce written development plans. Artifact: Calibration prep with pre-built counter-arguments for contested ratings; written development plans referenced in subsequent reviews. Distinguishing Test: Ratings rarely require revision in calibration — committee trusts the evidence quality.",
    "level5Advanced": "Scope: Org-wide calibration leadership. Key Behavior: Reviews are referenced as examples by peer managers; presents calibration cases with pre-built counter-arguments for every contested rating; runs pre-calibration alignment sessions to surface bias; creates differentiated retention plans with quarterly evidence portfolios; executes managed exits with 24-hour team communication; coaches peer EMs on calibration technique. Artifact: Review templates adopted by peer managers; pre-calibration alignment session records; retention plans with quarterly evidence portfolio reviews; managed exit checklists with knowledge transfer documentation. Distinguishing Test: Peer managers seek out calibration coaching — review preparation takes days not weeks because continuous documentation is habitual.",
    "rationale": "Anchor 1/2: Covers performance operations (reviews, calibration, feedback, PIPs, continuous documentation, high performer management, managed exits). Maps to C14-O1 through C14-O4, C14-O6 through C14-O9."
  },
  {
    "anchorId": "C14-2",
    "capabilityId": "C14",
    "sourceTopic": "Promotion Readiness & Career Architecture",
    "level1Developing": "Scope: Own team only. Key Behavior: Writes reactive promotion packets without systematic evidence building; no gap-filling stretch assignments planned; promotion cases lack rubric alignment. Artifact: None — promotion packets assembled last-minute without deliberate evidence collection. Distinguishing Test: Cannot explain what specific evidence gaps exist for any promotion candidate.",
    "level2Emerging": "Scope: Own team with emerging committee awareness. Key Behavior: Plans promotions 1-2 cycles ahead; starts identifying evidence gaps and assigning stretch work; developing awareness of committee expectations. Artifact: Basic promotion timeline for candidates; some stretch assignments mapped to evidence gaps. Distinguishing Test: Can name specific evidence gaps for promotion candidates but gap-filling is ad hoc rather than systematic.",
    "level3Competent": "Scope: Cross-team evidence gathering. Key Behavior: Builds promotion cases over 2-3 cycles with deliberate gap-filling stretch assignments; gathers cross-team evidence systematically; structures cases with specific impact matching rubric criteria. Artifact: Multi-cycle promotion tracking documents; stretch assignment plans mapped to rubric gaps; structured promotion packets with cross-team evidence. Distinguishing Test: Promotion packets include evidence from multiple cycles and cross-team sources — not a single-cycle snapshot.",
    "level4Distinguished": "Scope: Multi-team influence on career architecture. Key Behavior: Promotion success rate demonstrates calibration skill — committees approve without extensive debate; stretch assignments deliberately designed for next-level evidence; career development reputation attracts internal transfers; coaches peer EMs on promotion readiness. Artifact: Evidence portfolios with multi-cycle tracking; stretch assignment designs mapped to specific rubric criteria; peer coaching frameworks for promotion readiness. Distinguishing Test: Internal transfers cite career development as a reason for joining — promotion success rate is consistently high without committee pushback.",
    "level5Advanced": "Scope: Org-wide career architecture ownership. Key Behavior: Builds promotion cases 3+ cycles ahead with monthly evidence tracking; maps candidates against every rubric criterion with deliberate gap-filling; gets pre-reads from committee members one cycle before submission; maintains career architecture documents for IC and management tracks; coaches peer EMs on evidence quality and packet construction. Artifact: Shared evidence tracking documents updated monthly per candidate; career architecture documents reviewed annually with Staff+ engineers; promotion packet templates and evidence quality rubrics used by peer managers. Distinguishing Test: Promotion cases are pre-validated through committee pre-reads — packets require minimal debate because evidence gaps were filled systematically over multiple cycles.",
    "rationale": "Anchor 2/2: Covers career growth architecture (promotion planning, stretch assignments, evidence building, career paths). Maps to C14-O5. Intentionally separate from performance operations — evaluation vs. development are distinct sub-skills."
  }
]