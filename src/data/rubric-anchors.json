[
  {
    "anchorId": "C1-1",
    "capabilityId": "C1",
    "sourceTopic": "Org Design & Team Topologies",
    "level1Developing": "Inherits team structure and works within it. Recognizes ownership confusion when it causes problems. Understands Conway's Law conceptually but doesn't apply it proactively.",
    "level2Emerging": "Recognizes when team structure causes friction. Beginning to track ownership gaps. Starts to think about Conway's Law in practice but doesn't initiate structural changes.",
    "level3Competent": "Assesses team cognitive load and advocates for structural changes. Maintains clear ownership registry. Proposes and executes team splits/merges when needed. Applies Conway's Law to align architecture and org structure.",
    "level4Distinguished": "Proactively proposes team topology changes based on cognitive load analysis and value-stream mapping, influencing org-level design before bottlenecks surface. Executes team changes with minimal disruption. Conway's Law applied intentionally to shape architecture through team structure.",
    "level5Advanced": "Designs team topology for the org. Re-orgs executed with <1 regrettable departure per re-org cycle. Platform team justification and creation. Inverse Conway maneuver applied successfully. Team topology rationale articulated to VP+ with architectural alignment evidence. Company benchmark: Amazon's org design principles \u2014 single-threaded ownership, two-pizza teams, and API boundaries between teams \u2014 exemplify this level of structural thinking applied at scale.",
    "rationale": "Anchor 1/2: Covers structural design (team splits/merges, ownership, Conway's Law). Maps to C1-O1 through C1-O5."
  },
  {
    "anchorId": "C1-2",
    "capabilityId": "C1",
    "sourceTopic": "Cross-Team Strategy & Long-Horizon Planning",
    "level1Developing": "Focuses on own team's goals without considering cross-team impact. Planning horizon limited to current quarter. Relies on manager for org-level context and strategic direction.",
    "level2Emerging": "Beginning to track dependencies across adjacent teams. Starting to consider 2-quarter planning horizons. Occasionally contributes to org-level discussions when invited.",
    "level3Competent": "Proactively manages cross-team dependencies and alignment. 3-4 quarter planning horizon with clear milestones. Translates business strategy into team-level goals with explicit success criteria. Contributes to org-level strategy discussions. Identifies systemic issues that cross team boundaries.",
    "level4Distinguished": "Drives cross-team initiatives spanning multiple quarters with explicit dependency maps and milestone tracking. Anticipates org-level shifts (re-orgs, strategy pivots, market changes) and repositions team before impact hits. Influences strategic direction beyond own scope through written proposals that leadership acts on. Represents org perspective in senior leadership forums.",
    "level5Advanced": "Sets multi-year strategic direction for the org. Drives cross-org alignment on complex, ambiguous initiatives. Translates company strategy into team goals with measurable outcomes and cross-team dependency maps. Strategic thinking sought by VP+ leadership. Legacy decisions create lasting positive impact across the organization. Company benchmark: Google's cross-org alignment processes and Amazon's multi-year strategic planning (Working Backwards PR/FAQ) demonstrate how elite orgs sustain long-horizon thinking across teams.",
    "rationale": "Anchor 2/2: Covers strategic leadership altitude (multi-quarter planning, cross-team alignment, org-level influence, strategy-to-team translation). Maps to C1-O6 through C1-O16. Intentionally separate from structural design \u2014 strategy vs. structure are distinct sub-skills."
  },
  {
    "anchorId": "C2-1",
    "capabilityId": "C2",
    "sourceTopic": "Strategic Alignment & Roadmapping",
    "level1Developing": "Plans reactively (PM tells you what to build). OKRs exist but are task lists. Engineering impact described in technical terms. Trade-offs implicit, not communicated.",
    "level2Emerging": "Beginning to participate in planning beyond task execution. Starting to write OKRs with some outcome focus. Trade-offs acknowledged but not consistently documented.",
    "level3Competent": "Structured quarterly planning with capacity model. Outcome-oriented OKRs (max 3 per team). Engineering impact translated to business metrics. Trade-offs explicit in planning docs with 'what we're NOT doing' section. Roadmap communicated at multiple altitudes.",
    "level4Distinguished": "Planning process documented and shared with adjacent teams \u2014 at least one team adopts elements of the approach. OKRs produce measurable outcomes reviewed mid-quarter with course corrections. Engineering impact framed in terms leadership uses in their own communications (revenue, retention, risk reduction). PM partnership operates as strategic co-ownership at the roadmap level.",
    "level5Advanced": "Org-level planning drives cross-team alignment. OKRs directly tied to company strategy with measurable outcomes. Engineering ROI articulated in board-level business terms. Planning process is the standard others adopt. Strategic partner to PM/business leadership. Company benchmark: Amazon's PR/FAQ process forces opportunity-cost thinking before any initiative starts, and Google's OKR framework ties every team's goals measurably to company strategy.",
    "rationale": "Anchor 1/2: Covers strategic planning mechanics (planning process, OKRs, roadmapping, capacity models). Maps to C2-O1, C2-O5, C2-O6."
  },
  {
    "anchorId": "C2-2",
    "capabilityId": "C2",
    "sourceTopic": "Trade-Off Discipline & Decision Rigor",
    "level1Developing": "Difficulty saying no to requests. Trade-offs implicit or avoided. Decisions escalated or deferred rather than framed. First principles thinking is aspirational, not practiced.",
    "level2Emerging": "Beginning to push back on requests with basic trade-off framing. Starting to differentiate reversible from irreversible decisions. Maintains informal sense of what's not being done but doesn't communicate it explicitly.",
    "level3Competent": "Declines requests by making trade-offs visible ('yes, if we deprioritize X'). Maintains explicit 'not doing' list communicated to stakeholders. Matches decision rigor to reversibility \u2014 ships fast on reversible choices, invests analysis on irreversible ones. Challenges assumptions with first principles when the situation demands it.",
    "level4Distinguished": "Trade-off framing applied proactively to incoming requests \u2014 stakeholders receive options analysis before escalating. 'Not doing' list maintained as a living planning artifact that stakeholders reference independently. Decision rigor calibrated to stakes: reversible decisions ship within days, irreversible decisions include written alternatives analysis. First principles thinking reframes problems in ways that unlock novel solutions.",
    "level5Advanced": "Trade-off discipline is an org model \u2014 other teams adopt the practice. Strategic 'not doing' decisions create organizational clarity and focus. Decision framework handles ambiguity at scale. First principles thinking produces novel solutions that influence org strategy. Company benchmark: Amazon's 'disagree and commit' principle and Type 1/Type 2 decision framework provide structural clarity for balancing speed with rigor at scale.",
    "rationale": "Anchor 2/2: Covers prioritization discipline (saying no, trade-off framing, decision rigor, first principles thinking). Maps to C2-O2, C2-O3, C2-O4. Intentionally separate from planning mechanics \u2014 strategic discipline vs. process execution are distinct sub-skills."
  },
  {
    "anchorId": "C3-1",
    "capabilityId": "C3",
    "sourceTopic": "Technical Strategy & System Ownership",
    "level1Developing": "Follows existing technical direction set by TL/Staff. Participates in design reviews. Aware of tech debt but doesn't prioritize it systematically. Relies on senior engineers for architecture decisions.",
    "level2Emerging": "Participates in design reviews with growing confidence. Starting to form technical opinions beyond own team's scope. Beginning to advocate for system health improvements based on data.",
    "level3Competent": "Co-authors tech strategy with TL/Staff. Runs design reviews with clear criteria. Understands system health metrics and acts on them. Makes build-vs-buy decisions with TCO analysis.",
    "level4Distinguished": "Technical strategy extends beyond own team \u2014 architecture decisions account for cross-team system interactions and shared infrastructure. Design review criteria refined based on post-mortems and production learnings. Influences platform-level technical direction through written proposals with data-backed justification. Peer EMs seek technical input on their architecture decisions.",
    "level5Advanced": "Sets multi-quarter technical vision that senior engineers follow. Architecture review process institutionalized across teams. Drives platform thinking across org. Recognized as technical strategy leader by peers. Company benchmark: Google's design doc culture and architecture review process scale technical governance across thousands of engineers without creating bottlenecks.",
    "rationale": "Anchor 1/2: Covers technical vision, design reviews, and system ownership. Maps to C3-O1, C3-O2, C3-O5 through C3-O8."
  },
  {
    "anchorId": "C3-2",
    "capabilityId": "C3",
    "sourceTopic": "Tech Debt & Platform Investment",
    "level1Developing": "Acknowledges tech debt exists but doesn't track or prioritize it systematically. Build-vs-buy decisions are ad hoc. No framework for evaluating platform opportunities.",
    "level2Emerging": "Tracks tech debt informally. Makes occasional build-vs-buy arguments with basic cost comparison. Starting to allocate sprint time for debt reduction but without consistent tracking.",
    "level3Competent": "Manages tech debt allocation (15-20% of sprint capacity per quarter, excluding emergency hotfixes). Quantifies tech debt in business terms with cost-of-delay and remediation estimates. Makes build-vs-buy decisions with TCO analysis. Identifies platform opportunities for team-level reuse. Tracks tech debt impact on velocity.",
    "level4Distinguished": "Identifies systemic tech debt that spans team boundaries \u2014 proposes remediation with measured velocity impact and cost-of-delay quantification. Tech debt allocation defended to leadership with data showing delivery speed correlation. Platform investment ROI evaluated across multiple consuming teams. Influences org-wide engineering standards through documented patterns and shared tooling.",
    "level5Advanced": "Strategic tech debt paydown with measurable velocity impact across the org. Tech debt quantified in business terms and presented to leadership with ROI justification. Drives platform thinking that creates org-wide leverage. Platform investments justified with multi-team ROI. Recognized as technical strategy leader for investment prioritization. Company benchmark: Amazon treats tech debt and platform investment as first-class budget line items with measurable ROI, reviewed in weekly business reviews alongside feature delivery.",
    "rationale": "Anchor 2/2: Covers tech debt management, build-vs-buy, and platform investment decisions. Maps to C3-O3, C3-O4, C3-O9 through C3-O11. Intentionally separate from technical vision \u2014 investment prioritization is a distinct sub-skill."
  },
  {
    "anchorId": "C4-1",
    "capabilityId": "C4",
    "sourceTopic": "Operating Cadence & Process",
    "level1Developing": "Follows existing processes. Sprint velocity inconsistent. Interrupts handled reactively. Remote practices undefined.",
    "level2Emerging": "Beginning to establish regular operating cadence. Some improvement in sprint predictability. Interrupt management emerging but inconsistent. Remote practices being documented.",
    "level3Competent": "Clear operating cadence with defined outputs. Interrupt rotation implemented. Remote-first practices documented. Retrospective action items tracked to completion. Velocity bottlenecks diagnosed with value stream analysis. Engineers have 4+ hours of daily focus time protected by structural calendar policies.",
    "level4Distinguished": "Operating cadence requires minimal manager intervention \u2014 team self-corrects when rituals drift. Toil identified and eliminated through structured quarterly reviews with measurable time reclaimed. Remote/hybrid practices produce equitable participation measured through async contribution rates and meeting engagement. Process improvements documented and shared with adjacent teams.",
    "level5Advanced": "Operating rhythm is the org standard. Teams self-manage within cadence. Toil systematically eliminated. Systemic velocity analysis and focus time protection are organizational norms. Remote/hybrid practices produce equal engagement across locations. Company benchmark: Spotify's squad Health Check model provides leading indicators of operational health, and Amazon's operational excellence reviews institutionalize mechanisms over good intentions.",
    "rationale": "Anchor 1/2: Covers team operating system (cadence, process, interrupts, remote practices, velocity diagnosis, focus time protection). Maps to C4-O1, C4-O7 through C4-O13."
  },
  {
    "anchorId": "C4-2",
    "capabilityId": "C4",
    "sourceTopic": "Delivery Predictability & Execution",
    "level1Developing": "Sprint velocity inconsistent. Scope creep is common. Commitments are aspirational rather than evidence-based. Delivery risks surface late in the cycle.",
    "level2Emerging": "Beginning to track delivery metrics. Some improvement in scope management. Risks identified mid-sprint but mitigation is reactive. Starting to use data for capacity planning.",
    "level3Competent": "Meeting 85%+ of committed scope for 3+ consecutive quarters. Capacity model accounts for interrupts, on-call, and PTO. Risks flagged early with mitigation plans. Scope negotiated proactively with stakeholders.",
    "level4Distinguished": "Delivery predictability maintained above 85% through team transitions, scope changes, and organizational pressure. Capacity model continuously refined using actuals-vs-plan variance analysis. Teams self-manage scope trade-offs within agreed boundaries without escalation. Delivery practices documented in a format that adjacent teams can adopt.",
    "level5Advanced": "Delivery practices are the org standard. Teams self-manage within cadence with minimal oversight. Predictability maintained through organizational change. Cross-team delivery coordination is seamless. Company benchmark: Google's DORA research demonstrates that elite delivery teams maintain high predictability alongside high velocity \u2014 the two are complementary, not competing.",
    "rationale": "Anchor 2/2: Covers delivery outcomes (predictability, capacity planning, scope management). Maps to C4-O2 through C4-O6. Intentionally separate from cadence \u2014 process vs. outcomes are distinct sub-skills."
  },
  {
    "anchorId": "C5-1",
    "capabilityId": "C5",
    "sourceTopic": "Cross-Functional Partnership",
    "level1Developing": "Works with PM on features but relationship is transactional. Design involvement is late. Dependencies are managed reactively. TPM relationship undefined.",
    "level2Emerging": "Building working relationship with PM beyond feature handoffs. Design involvement improving but still sometimes late. Starting to manage dependencies more proactively.",
    "level3Competent": "Healthy triad with PM/Design (regular syncs, shared ownership, constructive disagreements). Shared success metrics across Eng/PM/Design with regular triad syncs. Technical input shapes product direction. Dependencies negotiated proactively. TPM partnership has clear division of labor. DS and platform partnerships structured.",
    "level4Distinguished": "Technical insights shape product direction at the roadmap level \u2014 PM includes engineering perspective in quarterly planning as a co-equal input. Cross-functional disagreements resolved through structured trade-off analysis rather than hierarchy. TPM and DS partnerships produce joint deliverables with shared success criteria. Partner feedback collected and acted upon systematically.",
    "level5Advanced": "Triad is a model for the org. Shared OKRs and joint retrospectives are standard practice. Product strategy influenced by technical insights. Cross-org partnerships brokered at scale. Legal/privacy partnerships proactive. Partner feedback is consistently strong across all functions. Company benchmark: Meta's cross-functional triad model with shared OKRs and joint retrospectives treats Eng/PM/Design as co-equal owners of outcomes, not separate reporting chains.",
    "rationale": "Anchor 1/2: Covers peer-level cross-functional relationships (PM/Design triad, TPM, DS, platform partnerships, triad alignment cadence). Maps to C5-O1 through C5-O5, C5-O16."
  },
  {
    "anchorId": "C5-2",
    "capabilityId": "C5",
    "sourceTopic": "Upward Management & Sponsor Building",
    "level1Developing": "Relationship with manager is status-update focused. No sponsor relationships. Scope expansion happens to them, not by them. Political dynamics invisible or confusing.",
    "level2Emerging": "Beginning to invest in manager relationship beyond status updates. Recognizes the importance of sponsors and political capital. Starting to navigate org dynamics with awareness.",
    "level3Competent": "Manager relationship is strategic \u2014 proactive alignment, early risk signaling, mutual trust. Building sponsor relationships through consistent delivery and visibility. Navigates political dynamics constructively. Beginning to expand scope through demonstrated capability.",
    "level4Distinguished": "Manager relationship produces sponsorship for stretch assignments and leadership visibility opportunities. Multiple sponsors across leadership built through cross-team delivery and strategic contributions. Political capital invested to unblock cross-team initiatives \u2014 influence extends beyond direct reports. Scope expansion proposals grounded in demonstrated capability and strategic alignment.",
    "level5Advanced": "Sponsors at VP+ level actively advocate. Trusted advisor to senior leaders across the org. Political capital compounds through reputation. Scope expanded based on demonstrated strategic judgment. Others seek them for advice on navigating organizational dynamics. Company benchmark: Amazon's Leadership Principles expect leaders to actively build sponsor relationships and 'Earn Trust' through consistent delivery and transparent communication with senior leadership.",
    "rationale": "Anchor 2/2: Covers upward influence (managing up, sponsor relationships, political capital, scope navigation). Maps to C5-O6 through C5-O15. Intentionally separate from peer partnerships \u2014 upward influence is a distinct sub-skill."
  },
  {
    "anchorId": "C6-1",
    "capabilityId": "C6",
    "sourceTopic": "Team Health & Execution",
    "level1Developing": "Holds 1:1s but they're mostly status updates. Avoids difficult conversations. Capacity planning is rough estimate. Handles retention reactively (after notice given). Struggles with adversity communication.",
    "level2Emerging": "1:1s improving \u2014 mixing status with some career discussion. Beginning to address performance issues (though slowly). Capacity model emerging. Retention mostly reactive but starting to notice flight risk signals.",
    "level3Competent": "1:1s are trust-building with career development mix. Addresses underperformance within 2 weeks of identification. Capacity model accounts for interrupts, on-call, PTO. Retention managed proactively (stay interviews, flight risk identification). Leads team through adversity with honest communication.",
    "level4Distinguished": "Measures psychological safety with structured assessments (Edmondson scale or equivalent) and builds action plans for any dimension below threshold. Addresses underperformance through skill/will diagnosis with differentiated intervention paths. Stay interviews surface and address retention risks before they become resignations. Team members cite their manager as a reason for staying in engagement surveys.",
    "level5Advanced": "Team scores 4.0+ on psychological safety survey (Edmondson scale or equivalent) consistently. Zero avoidance of difficult conversations. Team executes predictably at sustainable pace. <1 regrettable departure per 12 engineers per rolling 12 months. Team navigates crises with minimal productivity loss. Company benchmark: Google's Project Oxygen and manager effectiveness research identified coaching as the #1 behavior distinguishing great managers, measured through anonymous upward feedback surveys.",
    "rationale": "Anchor 1/2: Covers EM-level coaching (1:1s, underperformance, retention, stretch assignments, Staff+ management, career development). Maps to C6-O1 through C6-O5, C6-O8 (stay interviews), C6-O9 (Staff+ management), C6-O10 (career conversations), C6-O13 (career pathways)."
  },
  {
    "anchorId": "C6-2",
    "capabilityId": "C6",
    "sourceTopic": "Managing Managers (Director Track)",
    "level1Developing": "n/a at EM level. Early Director: still doing EM-level work. Skip-levels infrequent or awkward. EMs operate independently without cross-calibration. Delegation is uncomfortable.",
    "level2Emerging": "n/a at EM level. Early Director: beginning to establish skip-level rhythm. Starting to coach EMs beyond project management. Cross-calibration conversations starting. Delegation improving.",
    "level3Competent": "Regular skip-level 1:1s providing org insight. EMs coached on management craft (not just project management). Cross-EM calibration sessions before perf cycles. Delegation boundaries clear. EM bench strength being developed.",
    "level4Distinguished": "Skip-level insights drive proactive interventions \u2014 org issues surfaced and addressed before they escalate. EMs coached on the full management craft (coaching, calibration, stakeholder management) with specific behavioral feedback. Cross-EM calibration produces consistent standards \u2014 rating distributions align across teams. EM bench development includes deliberate stretch assignments preparing next-generation managers.",
    "level5Advanced": "EMs are independently strong leaders. Skip-level insights drive proactive org interventions. Calibration standards consistent across all EMs. EM pipeline producing ready managers. Director operates at Director altitude consistently. Org health monitored and maintained systematically. Company benchmark: Amazon evaluates Directors on how independently their EMs operate \u2014 Director-level heroics are a failure mode, not a strength.",
    "rationale": "Anchor 2/2: Covers Director-level EM development (coaching EMs, skip-levels, bench building, succession planning). Maps to C6-O6, C6-O7, C6-O11 (coaching struggling EMs), C6-O12 (succession plans). Intentionally separate \u2014 different altitude of practice."
  },
  {
    "anchorId": "C7-1",
    "capabilityId": "C7",
    "sourceTopic": "Stakeholder Management & Influence",
    "level1Developing": "Communicates status when asked. Frames asks in engineering terms. Delivers bad news late or sugar-coated. Political capital not actively built. No executive presence.",
    "level2Emerging": "Starting to provide proactive updates without being asked. Beginning to frame proposals with some business context. Bad news delivery improving but still sometimes delayed.",
    "level3Competent": "Proactive weekly written updates. Proposals framed in business terms with ROI. Bad news delivered early with mitigation plan. Political capital built through consistent delivery and helping others. Demonstrates comfort in exec reviews.",
    "level4Distinguished": "Tailors communication altitude to audience \u2014 IC-level detail for engineers, business outcome framing for VPs. Proposals include pre-built options with trade-off analysis, not just a single recommendation. Bad news delivered with root cause analysis and mitigation options before being asked. Builds political capital by unblocking cross-team dependencies, not just delivering within own scope.",
    "level5Advanced": "Leadership trusts completely \u2014 maximum autonomy earned. Cross-org alignment driven without escalation. Executive communication is crisp and confident. Sponsors at VP+ level actively advocate for you. Seen as a leader beyond engineering function. Company benchmark: Amazon's 6-pager narrative format eliminates slide-deck hand-waving and forces structured thinking, while Netflix's 'context, not control' model delegates decisions with clear strategic framing.",
    "rationale": "Anchor 1/2: Covers external-facing communication (status updates, bad news delivery, exec presentations, managing up, difficult conversations, communication scaling). Maps to C7-O2 through C7-O5, C7-O8 (difficult conversations), C7-O9 (communication scaling), C7-O11 (explicit trade-off communication)."
  },
  {
    "anchorId": "C7-2",
    "capabilityId": "C7",
    "sourceTopic": "Decision Making & Prioritization",
    "level1Developing": "Decisions made in meetings without documentation. Difficulty saying no. Most decisions escalated or deferred. First principles thinking is aspirational, not practiced.",
    "level2Emerging": "Some decisions documented but inconsistently. Starting to differentiate reversible from irreversible decisions. Beginning to push back on requests with basic trade-off framing.",
    "level3Competent": "DACI/RFC culture established. Decisions classified by reversibility and handled appropriately. Says no with trade-off framing. First principles analysis applied to significant decisions. Systems thinking catches second-order effects.",
    "level4Distinguished": "Decision framework shared with peers and influencing adoption on adjacent teams. Makes high-stakes decisions under ambiguity with documented reasoning that holds up in retrospect. First principles analysis produces novel approaches \u2014 reframing problems, not just analyzing existing options. Systems thinking anticipates cross-team second-order effects before they materialize.",
    "level5Advanced": "Decision framework is the org standard. Prioritization under ambiguity is a strength \u2014 forward progress in uncertain situations. First principles thinking produces novel solutions. Systems thinking prevents cross-org negative externalities. Decision quality recognized by peers and leadership. Company benchmark: Amazon's Type 1/Type 2 decision framework and 'disagree and commit' principle provide structural clarity for decision-making at scale.",
    "rationale": "Anchor 2/2: Covers decision architecture (DACI/RFC, trade-off framing, reversibility, authority distribution). Maps to C7-O1, C7-O6, C7-O7, C7-O10 (decision authority distribution). Intentionally separate \u2014 communication vs decision-making are distinct sub-skills."
  },
  {
    "anchorId": "C8-1",
    "capabilityId": "C8",
    "sourceTopic": "Incident Response & Command",
    "level1Developing": "Participates in incident response. Post-mortems happen but action items slip. On-call exists but rotation health isn't tracked. Launches happen without formal readiness review.",
    "level2Emerging": "More active in incident response. Post-mortems happening consistently but action item follow-through improving. On-call rotation exists and basic health metrics being tracked.",
    "level3Competent": "Incident command process defined and practiced. Post-mortem action item completion >90%. On-call health tracked (page volume, false positives, off-hours pages). Launch readiness checklist enforced. Game days run quarterly.",
    "level4Distinguished": "Incident response process refined based on retrospective learnings \u2014 each post-mortem produces at least one process improvement. Post-mortem culture genuinely blameless with action items that address systemic causes, not just symptoms. On-call health metrics trending toward targets (<2 actionable pages per shift). Cross-team incident learnings shared through structured forums. Game days test novel failure modes beyond known scenarios.",
    "level5Advanced": "Org-level incident process owned and continuously improved. Cross-org failure mode analysis. On-call health exemplary (teams want to be on your rotation). Incident patterns analyzed across teams. Culture of proactive resilience building. Company benchmark: Google SRE's error budget model makes reliability investment decisions data-driven \u2014 teams spend their error budget on velocity and earn it back through stability.",
    "rationale": "Anchor 1/2: Covers reactive operational excellence (incident command, post-mortems, on-call health). Maps to C8-O1 through C8-O3."
  },
  {
    "anchorId": "C8-2",
    "capabilityId": "C8",
    "sourceTopic": "Systemic Risk Reduction",
    "level1Developing": "Risk management is reactive \u2014 responds to incidents after they happen. No proactive failure mode analysis. Launch readiness is informal. Dependency risks unknown.",
    "level2Emerging": "Beginning to think about risk proactively. Starting to conduct failure mode analysis for major launches. Some game day participation. Launch readiness checklist emerging.",
    "level3Competent": "Proactive failure mode analysis for all significant changes. Launch readiness reviews enforced with documented criteria. Dependency risks mapped and mitigated. Vendor risk registry maintained for critical dependencies.",
    "level4Distinguished": "Risk reduction applied systematically across the full team portfolio \u2014 not just major launches. Cross-team failure modes identified through dependency analysis and addressed through joint mitigation plans. Game days designed to reveal novel failure modes and produce architectural improvements. Vendor dependency risks mitigated with abstraction layers or fallback strategies. Influence on adjacent teams' risk practices emerging.",
    "level5Advanced": "Org-level risk reduction framework owned and continuously improved. Cross-org failure mode analysis prevents systemic incidents. Culture of proactive resilience building. Vendor dependency abstraction reduces single-point-of-failure risk across the org. Company benchmark: Netflix's chaos engineering practice (Chaos Monkey) proactively tests resilience, and Amazon's pre-mortem process identifies failure modes before they occur in production.",
    "rationale": "Anchor 2/2: Covers proactive risk management (failure mode analysis, launch readiness, dependency risk, vendor management, chaos engineering, error budgets). Maps to C8-O4 through C8-O6, C8-O7 (chaos engineering and game days), C8-O8 (error budget management). Intentionally separate from incident response \u2014 proactive vs. reactive are distinct sub-skills."
  },
  {
    "anchorId": "C9-1",
    "capabilityId": "C9",
    "sourceTopic": "Metric Selection & Dashboard Design",
    "level1Developing": "Aware of DORA metrics but doesn't track consistently. Follows existing release processes. Notices developer experience issues but doesn't systematically address them. Sprint management is reactive.",
    "level2Emerging": "Starting to track DORA metrics. Some awareness of team's developer experience gaps. Sprint health improving but inconsistent. Beginning to identify and address toil.",
    "level3Competent": "Reviews DORA metric dashboard weekly; discusses trends in team standup at least biweekly. Owns release quality and process for team. Runs developer experience improvements quarterly (top 2 pain points). Meeting 85%+ sprint commitment accuracy for 3+ consecutive quarters. Toil tracked and reduced systematically.",
    "level4Distinguished": "DORA metrics approaching elite benchmarks with diagnosed bottlenecks and improvement plans for each gap. Developer experience investments prioritized with before/after measurement \u2014 ROI documented per DX initiative. Metric pairings prevent single-dimension optimization (e.g., deploy frequency paired with change failure rate). Delivery practice documentation shared with adjacent teams.",
    "level5Advanced": "DORA metrics at elite level across teams. Developer experience investments justified with ROI data. Delivery practices standardized across org. Teams benchmark against each other constructively. Engineering productivity narrative presented to leadership. Company benchmark: Google and Microsoft's SPACE framework balances satisfaction, performance, activity, communication, and efficiency \u2014 preventing any single metric from dominating the productivity narrative.",
    "rationale": "Anchor 1/2: Covers metric infrastructure (DORA tracking, developer experience, delivery metrics, dashboards, metric pairings, maturity adaptation). Maps to C9-O1 through C9-O3, C9-O7, C9-O9 (metric pairings), C9-O10 (metric maturity adaptation)."
  },
  {
    "anchorId": "C9-2",
    "capabilityId": "C9",
    "sourceTopic": "Data-Driven Decision Making",
    "level1Developing": "Decisions based on intuition or anecdotes. Metrics reviewed only when asked. No regular review cadence. Engineering impact described in technical terms, not business outcomes.",
    "level2Emerging": "Starting to reference data in decision-making. Reviews team metrics periodically but doesn't act on trends consistently. Beginning to articulate engineering impact in outcome terms.",
    "level3Competent": "Engineering investments justified with data. OKRs measured with leading and lagging indicators. Impact articulated in quantitative business terms. Experiment results inform product and engineering decisions.",
    "level4Distinguished": "Identifies and closes metric gaps before they cause blind spots \u2014 adds or retires metrics based on decision utility. Experiment governance established: hypothesis quality standards, sample sizing, and structured result interpretation. Engineering investment proposals consistently win leadership support through quantitative impact framing. Measurement standards documented and shared with adjacent teams.",
    "level5Advanced": "Engineering investment narrative presented to leadership with rigorous data and ROI analysis. Measurement standards adopted across org. A/B testing governance with mandatory power analysis established. ROI framing for engineering investment is a recognized organizational strength. Company benchmark: Amazon's 6-pager culture requires data-backed narratives for every engineering investment, and Stripe's developer productivity research ties engineering metrics to business outcomes.",
    "rationale": "Anchor 2/2: Covers decision-making with data (OKR measurement, business impact articulation, experimentation, ROI framing). Maps to C9-O4 through C9-O6, C9-O8. Intentionally separate from metric infrastructure \u2014 collecting data vs. using data are distinct sub-skills."
  },
  {
    "anchorId": "C10-1",
    "capabilityId": "C10",
    "sourceTopic": "Budget, Headcount & Resource Planning",
    "level1Developing": "Requests headcount without financial framing. Cloud costs not tracked at team level. Build-vs-buy decisions are gut feel. Budget is someone else's problem.",
    "level2Emerging": "Beginning to think about headcount in terms of ROI. Starting to track cloud costs at team level. Some awareness of build-vs-buy trade-offs. Budget awareness growing.",
    "level3Competent": "Headcount justified with ROI analysis. Cloud costs attributed and optimized per service. Build-vs-buy uses TCO framework. Contractor strategy is principled. Budget defense prepared for cuts.",
    "level4Distinguished": "Headcount proposals connect to business outcomes with multi-scenario ROI modeling (best/base/worst case). Cloud cost attribution drives team-level accountability \u2014 engineers understand and optimize their service costs. Build-vs-buy decisions documented with TCO analysis reusable for future decisions. Finance partners proactively consult on engineering investment questions.",
    "level5Advanced": "Org-level headcount narrative wins at VP/finance level. FinOps culture established across teams. Engineering investment framed as business asset with measurable returns. Proactive cost optimization (savings reinvested). Trusted steward of company resources. Company benchmark: Netflix's 'highly aligned, loosely coupled' model lets teams self-allocate within strategic guardrails, and Amazon's FinOps culture treats cost as a first-class engineering constraint.",
    "rationale": "Anchor 1/2: Covers financial stewardship (headcount justification, cloud costs, build-vs-buy, contractor strategy). Maps to C10-O1, C10-O3, C10-O4, C10-O5."
  },
  {
    "anchorId": "C10-2",
    "capabilityId": "C10",
    "sourceTopic": "Strategic Reallocation & ROI Framing",
    "level1Developing": "Resources stay where they were assigned regardless of changing priorities. No framework for comparing ROI across investments. Reacts to cuts rather than proactively optimizing allocation.",
    "level2Emerging": "Beginning to evaluate resource allocation against impact. Some awareness that reallocation is a tool, not just a response to cuts. Starting to frame engineering investments in outcome terms.",
    "level3Competent": "Presents resource requests as tiered options with quantified trade-offs at each level. Tracks cost-per-outcome to demonstrate engineering ROI. Proactively reallocates capacity to highest-impact work without waiting for top-down direction. During cuts, communicates explicitly what stops \u2014 not just what continues.",
    "level4Distinguished": "Tiered resource proposals adopted as the default format for engineering requests to leadership. Cost-per-outcome data actively informs quarterly reallocation decisions \u2014 not just tracked but acted upon. Reallocation happens within established boundaries without requiring executive approval for each shift. Adjacent teams begin adopting the tiered proposal framework.",
    "level5Advanced": "Resource allocation narrative presented at VP/finance level with rigorous ROI analysis. Proactive cost optimization generates savings that are reinvested in high-impact areas. Engineering investment framing is a recognized organizational capability. Teams across org adopt similar ROI tracking practices. Company benchmark: Amazon's FinOps culture treats cost as a first-class engineering constraint, with weekly business reviews comparing cost-per-outcome across teams and reinvesting savings into highest-ROI initiatives.",
    "rationale": "Anchor 2/2: Covers dynamic allocation (tiered proposals, ROI tracking, proactive reallocation, cut communication). Maps to C10-O2, C10-O6, C10-O7, C10-O8. Intentionally separate from financial stewardship \u2014 static budgeting vs. dynamic allocation are distinct sub-skills."
  },
  {
    "anchorId": "C11-1",
    "capabilityId": "C11",
    "sourceTopic": "Hiring Process & Bar Raising",
    "level1Developing": "Participates in hiring loops. Onboarding is informal (buddy system if lucky). Headcount requests are 'we need more people.' No engineering brand presence.",
    "level2Emerging": "Hiring loops becoming more structured. Some onboarding documentation exists. Starting to justify headcount with basic data. Referral network emerging.",
    "level3Competent": "Calibrated hiring loops with trained interviewers and rubrics. Structured interview processes evaluate demonstrated competencies over pedigree. Headcount justified with capacity model and business impact. Referral pipeline active. Bar raiser discipline maintained under hiring pressure. Hiring funnel conversion tracked at each stage.",
    "level4Distinguished": "Hiring bar maintained under volume pressure \u2014 interview calibration sessions run quarterly with inter-rater reliability tracked. Structured interview process produces consistent signal across interviewers. Headcount narrative connects team capacity to business strategy with specific revenue or risk impact. Diverse candidate slates achieved through structured sourcing, not just pipeline volume.",
    "level5Advanced": "Bar raiser discipline maintained under pressure. Interviewer calibration is org model. Headcount cases win at VP level. Hiring process produces consistently strong signal with high inter-rater reliability. Competitive hiring strategy wins talent beyond compensation through differentiated value proposition. Hiring funnel instrumented and optimized with data-driven pipeline management. Company benchmark: Amazon's Bar Raiser program gives independent interviewers veto power to protect hiring quality, and Google's structured hiring committees use calibrated rubrics to eliminate bias.",
    "rationale": "Anchor 1/2: Covers hiring mechanics (interview loops, bar raising, headcount justification, interviewer calibration, structured interviews, competitive hiring strategy, funnel optimization). Maps to C11-O1, C11-O2, C11-O7 through C11-O11."
  },
  {
    "anchorId": "C11-2",
    "capabilityId": "C11",
    "sourceTopic": "Onboarding Excellence & Talent Brand",
    "level1Developing": "Onboarding is informal \u2014 new hires figure it out. No engineering brand presence. Time-to-productivity not tracked. No referral pipeline.",
    "level2Emerging": "Basic onboarding documentation exists. Some buddy pairing. Beginning to track new hire experience. Occasional blog post or meetup participation.",
    "level3Competent": "Structured 30/60/90 onboarding with measurable milestones. Time-to-productivity tracked and improving. Buddy/mentor program formalized. Active blog or conference presence emerging. Referral pipeline maintained.",
    "level4Distinguished": "Onboarding continuously refined using new hire feedback surveys at 30/60/90 days. Time-to-productivity benchmarked against org averages with diagnosed bottlenecks. Engineering brand built through conference talks, blog posts, or open source contributions that generate inbound senior talent interest. Referral rate among highest on team \u2014 engineers actively recruit from their networks.",
    "level5Advanced": "Time-to-productivity measured and optimized to org-best levels. Engineering brand is a recruiting advantage \u2014 candidates cite it in interviews. Conference talks and open source presence attract senior talent. Onboarding process is a model other teams adopt. Company benchmark: Meta's Bootcamp onboarding gets new engineers productive within 6 weeks through structured mentorship, and Google's engineering brand (publications, open source) drives inbound senior talent.",
    "rationale": "Anchor 2/2: Covers talent brand and onboarding (30/60/90 programs, time-to-productivity, engineering brand, referrals). Maps to C11-O3 through C11-O6. Intentionally separate from hiring mechanics \u2014 acquisition vs. integration are distinct sub-skills."
  },
  {
    "anchorId": "C12-1",
    "capabilityId": "C12",
    "sourceTopic": "Team Charter & Engineering Principles",
    "level1Developing": "Team culture exists by accident (whatever norms emerged). Recognition is sporadic. Inclusion is 'we treat everyone the same.'",
    "level2Emerging": "Starting to be intentional about team norms. Some recognition happening but not systematized. Awareness of inclusion beyond 'same treatment.' Beginning to document engineering practices.",
    "level3Competent": "Team charter written and referenced. Engineering principles documented and used in design reviews. Regular recognition rituals. Active inclusion practices (meeting facilitation, async options, equitable opportunity distribution). Explicit feedback channels ensuring every voice is heard. Cultural continuity maintained through growth and leadership transitions. Toxic behaviors addressed within 48 hours.",
    "level4Distinguished": "Team charter reviewed and evolved quarterly based on team feedback and growth. Engineering principles referenced in code review and architecture decision feedback, not just design reviews. Recognition operates peer-to-peer without requiring manager initiation. Inclusion practices produce measurable engagement survey results with action plans for identified gaps.",
    "level5Advanced": "Culture is intentional, measurable, and self-reinforcing. Engineering principles adopted beyond own team. Cultural continuity maintained through rapid growth and leadership transitions without degradation. Inclusive environment measurable in surveys and independently confirmed. Toxic behaviors caught and addressed proactively \u2014 no 'missing stair' dynamics. Candidates cite team culture as top-3 reason for accepting offer in post-hire surveys. Company benchmark: Netflix's Freedom and Responsibility culture doc is the canonical example of intentional culture at scale, and Spotify's explicit team Health Checks make culture measurable and discussable.",
    "rationale": "Anchor 1/2: Covers culture foundations (team charter, engineering principles, recognition, inclusion, cultural continuity, inclusive feedback channels, toxic behavior intervention). Maps to C12-O1, C12-O2, C12-O4 through C12-O8."
  },
  {
    "anchorId": "C12-2",
    "capabilityId": "C12",
    "sourceTopic": "Knowledge Sharing & Documentation Culture",
    "level1Developing": "Knowledge sharing is informal \u2014 tribal knowledge dominates. Documentation sparse and outdated. No structured forums for cross-team learning.",
    "level2Emerging": "Some documentation emerging but inconsistent. Occasional knowledge-sharing sessions. Beginning to notice that tribal knowledge creates bottlenecks and bus-factor risk.",
    "level3Competent": "Documentation standards maintained and enforced. Regular tech talks or knowledge-sharing rituals. Design doc culture established. Onboarding documentation reduces ramp time measurably.",
    "level4Distinguished": "Team members contribute documentation as part of their workflow \u2014 updates happen alongside code changes, not as separate tasks. Design doc quality validated through structured peer review with clear approval criteria. Knowledge-sharing sessions run with rotating ownership \u2014 engineers volunteer to present. Documentation standards shared with at least one adjacent team.",
    "level5Advanced": "Knowledge sharing institutionalized across org (tech talks, wikis, design doc culture). Documentation coverage comprehensive and maintained. Knowledge-sharing practices are a model other teams adopt. Reduces organizational bus-factor risk at scale. Company benchmark: Stripe's writing culture and Amazon's design doc requirements institutionalize knowledge sharing as an organizational practice, not an individual habit.",
    "rationale": "Anchor 2/2: Covers knowledge management (documentation, tech talks, design docs, cross-team learning). Maps to C12-O3. Intentionally separate from culture foundations \u2014 knowledge sharing is a distinct operational practice."
  },
  {
    "anchorId": "C13-1",
    "capabilityId": "C13",
    "sourceTopic": "Security Practices & Vulnerability Management",
    "level1Developing": "Ensures team follows security practices when reminded. Patches vulnerabilities reactively. Change management is informal. Compliance is handled last-minute before audits.",
    "level2Emerging": "Security practices becoming more consistent without reminders. Vulnerability patching improving but SLA sometimes missed. Change management process emerging. Starting to prepare for audits proactively.",
    "level3Competent": "Security embedded in development workflow (threat modeling, automated scanning, champion rotation). Security champions rotate quarterly and attend security guild. Automated security scanning integrated into CI/CD with severity-based deployment gates. Vulnerability SLAs met consistently (P0 <48hrs, P1 <7 days). Threat models required for features touching auth, payments, or PII. Tiered change management process in place.",
    "level4Distinguished": "Security practices embedded in development culture \u2014 engineers initiate threat models without prompting for sensitive features. Vulnerability SLAs met consistently with mean resolution time trending downward. Security champion program produces engineers who improve team security practices independently. Change management process documented and influencing adoption by adjacent teams.",
    "level5Advanced": "Security culture driven across org. Change management standards adopted org-wide. Security champion program scaled beyond own team. Automated scanning and threat modeling are organizational standards. Teams proactively identify and mitigate security risks before they become vulnerabilities. Company benchmark: Google's BeyondCorp zero-trust model and Amazon's mandatory security review process demonstrate security as a proactive engineering discipline, not a compliance checkbox.",
    "rationale": "Anchor 1/2: Covers security operations (threat modeling, vulnerability management, change management, security champions, automated scanning, threat modeling for sensitive features). Maps to C13-O1 through C13-O3, C13-O5 through C13-O7."
  },
  {
    "anchorId": "C13-2",
    "capabilityId": "C13",
    "sourceTopic": "Compliance & Governance Automation",
    "level1Developing": "Compliance is handled last-minute before audits. Evidence collection is manual. Access controls inconsistent. No automated compliance checks.",
    "level2Emerging": "Beginning to embed compliance into workflows. Some automated scanning in place. Starting to track access control hygiene. Audit preparation moving earlier in the cycle.",
    "level3Competent": "Continuous compliance posture with automated evidence collection. Access governance automated (quarterly reviews, auto-expiring elevated permissions). Audit prep reduced from weeks to days.",
    "level4Distinguished": "Compliance evidence collected automatically with real-time dashboard \u2014 audit preparation requires hours, not days. Access governance runs exception-based reviews only \u2014 routine changes fully automated. Zero repeat findings across consecutive audit cycles. Governance automation patterns documented and shared with adjacent teams.",
    "level5Advanced": "Zero P1/P2 findings in 3+ consecutive audit cycles. Compliance is a competitive advantage, not a burden. Governance automation is an org model. Continuous compliance frameworks adopted beyond own team. Company benchmark: Netflix automates compliance evidence collection continuously, making audit readiness a dashboard metric rather than a quarterly project.",
    "rationale": "Anchor 2/2: Covers compliance and governance (audit readiness, evidence collection, access governance, compliance automation). Maps to C13-O3, C13-O4. Intentionally separate from security operations \u2014 compliance is a distinct discipline from vulnerability management."
  },
  {
    "anchorId": "C14-1",
    "capabilityId": "C14",
    "sourceTopic": "Performance Reviews & Calibration",
    "level1Developing": "Writes reviews but they're vague. Calibration prep is last-minute. Avoids PIPs. Career conversations are infrequent.",
    "level2Emerging": "Reviews improving in specificity. Starting to prepare for calibration earlier. Beginning to address performance issues with HR guidance. Career conversations starting to happen regularly.",
    "level3Competent": "Evidence-based reviews with specific impact and rubric language. Continuous performance documentation \u2014 running notes updated weekly, not reconstructed at review time. Calibration cases prepared with cross-team comparison data. PIPs executed when needed with HR partnership. High performers managed with differentiated development plans and retention-conscious conversations. Managed exits executed with dignity and clear process. Quarterly career conversations for all reports.",
    "level4Distinguished": "Calibration cases include cross-team comparison data that withstands committee scrutiny \u2014 ratings rarely require revision. Performance management covers the full spectrum without avoidance: high performers receive differentiated stretch assignments, underperformers receive structured improvement plans with clear milestones, managed exits execute with dignity and complete knowledge transfer. Career conversations produce written development plans referenced in subsequent reviews.",
    "level5Advanced": "Reviews set the standard for the org. Continuous performance documentation produces evidence-rich reviews with zero surprises. Calibration presence is authoritative and credible. High performer retention above 90% through differentiated investment. Performance management (including exits) handled cleanly \u2014 managed exits executed with dignity, full knowledge transfer, and zero legal escalations. Career development is the reason people join your team. Calibrates other EMs' performance standards. Company benchmark: Google's calibration committee model separates promotion decisions from direct managers, ensuring cross-team consistency and reducing individual bias in performance assessment.",
    "rationale": "Anchor 1/2: Covers performance operations (reviews, calibration, feedback, PIPs, continuous documentation, high performer management, managed exits). Maps to C14-O1 through C14-O4, C14-O6 through C14-O9."
  },
  {
    "anchorId": "C14-2",
    "capabilityId": "C14",
    "sourceTopic": "Promotion Readiness & Career Architecture",
    "level1Developing": "Promotion packets are reactive ('they've been doing this for a while'). No systematic approach to building promotion cases over time. Gap-filling stretch assignments not planned.",
    "level2Emerging": "Beginning to plan promotions 1-2 cycles ahead. Starting to identify and fill gaps through stretch assignments. Some awareness of what calibration committees look for.",
    "level3Competent": "Promo packets built over 2-3 cycles with deliberate gap-filling stretch assignments. Cross-team evidence gathering is systematic. Promotion cases well-structured with specific impact examples matching rubric criteria.",
    "level4Distinguished": "Promotion success rate demonstrates calibration skill \u2014 candidates prepared over multiple cycles with evidence portfolios that committees approve without extensive debate. Gap-filling stretch assignments deliberately designed to build specific next-level evidence. Career development reputation attracts internal transfers to the team. Coaches at least one peer EM on promotion readiness with structured frameworks.",
    "level5Advanced": "Promotion track record is among the best in the org. Promo packet construction is a model others follow. Career architecture creates clear, achievable growth paths. Development reputation is a recruiting advantage. Company benchmark: Amazon's promotion process requires documented evidence of sustained next-level impact over multiple review cycles, and Google's promo committees evaluate packets independently of manager advocacy.",
    "rationale": "Anchor 2/2: Covers career growth architecture (promotion planning, stretch assignments, evidence building, career paths). Maps to C14-O5. Intentionally separate from performance operations \u2014 evaluation vs. development are distinct sub-skills."
  }
]