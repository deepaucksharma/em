[
  {
    "id": "P-C6-1",
    "slug": "your-top-performer-gives-2-weeks-notice",
    "observableIds": [
      "C6-O3",
      "C6-O4"
    ],
    "capabilityIds": [
      "C6"
    ],
    "title": "Your Top Performer Gives 2 Weeks Notice",
    "context": "Your best senior engineer has an offer from a competitor. They seem decided but are giving you a courtesy conversation.",
    "topicsActivated": [
      "Team Health (Retention)",
      "Performance (Managing High Performers)",
      "Stakeholder Mgmt (Delivering Bad News)"
    ],
    "decisionFramework": "1. Listen: understand root cause (comp? growth? culture? manager?). 2. Assess: is this fixable? Be honest with yourself. 3. If fixable: make specific, genuine offer (not panicked counter). 4. If not fixable: wish them well, plan transition. 5. Inform your manager immediately. 6. Plan knowledge transfer. 7. Communicate to team. 8. Post-mortem: what could you have done differently?",
    "commonMistakes": "Panicked counter-offer that doesn't address root cause (buys 6 months at best). Guilt-tripping. Not telling your manager immediately. Not planning knowledge transfer. Ignoring the signal for the rest of the team.",
    "whatGoodLooksLike": "Root cause conversation within 24hrs. Honest assessment of fixability. If counter-offering: specific, addresses root cause, approved by leadership. If not: graceful exit with knowledge transfer plan. Team comms within 48hrs. Retrospective on retention practices. Industry reference: Netflix's Keeper Test philosophy frames retention honestly — if you wouldn't fight to keep them, help them leave well. Amazon separates comp levers from role/scope levers when constructing retention offers. Calibration signal: \"Counter-offers that don't address root cause just delay departure by 6 months\" (SIG-051).",
    "mappingNotes": "Top performer retention scenario",
    "suggestedMetricIds": [
      "7.1",
      "7.2",
      "5.1"
    ]
  },
  {
    "id": "P-C8-1",
    "slug": "major-production-incident-during-your-vacation",
    "observableIds": [
      "C8-O1",
      "C8-O2",
      "C8-O4"
    ],
    "capabilityIds": [
      "C8"
    ],
    "title": "Major Production Incident During Your Vacation",
    "context": "You're on vacation. A Sev1 incident fires and your team is handling it. Your manager pings you asking for status.",
    "topicsActivated": [
      "Operational Risk (Incident Response)",
      "Team Health (Trust)",
      "Stakeholder Mgmt (Managing Up)"
    ],
    "decisionFramework": "1. Check: is the team handling it? If yes, trust them. 2. If team is struggling: provide guidance, don't take over. 3. Respond to manager with what you know. 4. Post-vacation: review incident response. 5. If team handled it well: celebrate. If not: identify gaps for training/process/runbooks.",
    "commonMistakes": "Taking over incident from vacation (undermines team, proves you haven't delegated). Going completely dark when your manager needs a response. Blaming team for not handling it perfectly. Using it as evidence that you can't take vacation.",
    "whatGoodLooksLike": "Team handles it using documented processes. You provide brief guidance if asked, but don't run the incident. Post-vacation retro focuses on process improvement, not blame. Your team proved they can function without you — that's a WIN. Industry reference: Google SRE's incident commander rotation ensures no single person is critical path. Amazon's COE (Correction of Error) process guarantees structural learning regardless of who runs the incident. Calibration signal: \"Standard practice at Big Tech — EMs who skip this create visible risk\" (SIG-033).",
    "mappingNotes": "Production incident during vacation",
    "suggestedMetricIds": [
      "1.3",
      "3.5",
      "3.3",
      "3.6"
    ]
  },
  {
    "id": "P-C5-1",
    "slug": "pm-wants-to-ship-a-feature-you-know-will-create-tech-debt",
    "observableIds": [
      "C5-O1",
      "C3-O4"
    ],
    "capabilityIds": [
      "C5",
      "C3"
    ],
    "title": "PM Wants to Ship a Feature You Know Will Create Tech Debt",
    "context": "PM has committed to a deadline with stakeholders. Meeting the deadline requires cutting corners that will cost the team months of debt later.",
    "topicsActivated": [
      "Cross-Functional Partnership (PM Disagreement)",
      "Decision Making (Trade-offs)",
      "Strategic Alignment (Trade-off Communication)"
    ],
    "decisionFramework": "1. Quantify the debt: specifically what gets cut, what's the future cost (hours, risk, incidents). 2. Present options: (A) Ship on time with known debt + remediation plan, (B) Extend deadline by X to do it right, (C) Reduce scope to hit deadline without debt. 3. Make trade-off PM's decision with full information. 4. If PM chooses debt: document the plan to pay it back. 5. Never: passive-aggressively build it 'right' and miss deadline.",
    "commonMistakes": "Just saying 'no' without alternatives. Passive compliance then complaining about tech debt later. Building it your way and missing the deadline. Not quantifying the actual cost of debt. Escalating without first trying to align directly with PM.",
    "whatGoodLooksLike": "You present 3 clear options with honest trade-offs. PM makes an informed decision. If debt is accepted: remediation plan is written, capacity allocated, and PM agrees to the payback timeline. No resentment — it was a joint decision with eyes open. Industry reference: Amazon's 6-pager format forces explicit trade-off documentation before any investment decision, preventing the 'we didn't know the cost' problem. Stripe's writing culture similarly requires architectural trade-offs to be written, not just discussed. Calibration signal: \"'Strong partner feedback from PM and Design' — explicit calibration dimension at most Big Tech companies\" (SIG-106).",
    "mappingNotes": "PM tech debt conflict",
    "suggestedMetricIds": [
      "8.6",
      "8.4",
      "1.2"
    ]
  },
  {
    "id": "P-C12-1",
    "slug": "two-of-your-engineers-are-in-conflict",
    "observableIds": [
      "C12-O1",
      "C12-O2"
    ],
    "capabilityIds": [
      "C12"
    ],
    "title": "Two of Your Engineers Are in Conflict",
    "context": "Two senior engineers disagree on a technical approach. It's gotten personal — code reviews are hostile, standup is tense, team morale is affected.",
    "topicsActivated": [
      "Team Health (Constructive Conflict, Psych Safety)",
      "Performance (Feedback)",
      "Engineering Culture (Inclusive Environment)"
    ],
    "decisionFramework": "1. Talk to each privately: understand their perspective and concerns. 2. Separate the technical from the personal. 3. Resolve the technical disagreement: use RFC process, get external reviewer, make a decision. 4. Address the personal: specific feedback to each on behavior (not opinions). 5. Mediated conversation if needed. 6. Clear expectations going forward. 7. Follow up in 2 weeks.",
    "commonMistakes": "Ignoring it ('they'll work it out'). Taking sides publicly. Making it about who's right technically while ignoring interpersonal damage. Punishing both equally. Moving one to another team without addressing the root cause.",
    "whatGoodLooksLike": "Technical decision made through structured process (not by who argues louder). Both engineers receive specific, private feedback on behavior. Both acknowledge the impact on the team. Clear working agreement going forward. Follow-up confirms improvement. Team morale recovers. Industry reference: Microsoft's Model-Coach-Care framework gives managers a structured approach for coaching through interpersonal conflict without taking sides. Google's Project Aristotle research shows that how teams handle conflict is a key differentiator of effective teams. Calibration signal: \"EMs who avoid conflict are a red flag in calibration\" (SIG-044).",
    "mappingNotes": "Engineer conflict resolution",
    "suggestedMetricIds": [
      "7.9",
      "5.1",
      "7.8"
    ]
  },
  {
    "id": "P-C1-1",
    "slug": "you-inherit-a-low-performing-team",
    "observableIds": [
      "C1-O8",
      "C1-O7",
      "C6-O2"
    ],
    "capabilityIds": [
      "C1",
      "C6"
    ],
    "title": "You Inherit a Low-Performing Team",
    "context": "You join as EM. In your first month you discover: 2 engineers should probably be on a PIP, the team's metrics are worst in the org, morale is low, and the previous EM left because of burnout.",
    "topicsActivated": [
      "First 90 Days (Assessment, Credibility)",
      "Performance (Underperformance, PIPs)",
      "Team Health (Psych Safety)",
      "Stakeholder Mgmt (Managing Up)"
    ],
    "decisionFramework": "1. Month 1: Listen and assess. Build trust. Understand WHY performance is low (people? process? scope? leadership?). 2. Month 2: Quick wins (fix one obvious pain point). Start 1:1 cadence. Begin addressing most critical performance gap. 3. Month 3: Comprehensive plan to leadership — honest assessment + proposed path. Begin formal performance processes where needed. 4. Communicate to team: 'here's what I've observed, here's what we're going to change, here's how I'll support you.'",
    "commonMistakes": "Making changes in week 1 before understanding context. Putting people on PIP immediately (you weren't there, you don't have evidence yet). Blaming previous EM publicly. Promising everything will be better without a plan. Sugar-coating the situation to leadership.",
    "whatGoodLooksLike": "Honest assessment to leadership at 30 days (with plan, not just problems). Trust built through listening and quick wins. Performance conversations started with clear expectations and genuine support. Team feels heard and sees a path forward. Measurable improvement at 90 days. Industry reference: Amazon's Leadership Principles give new EMs a shared language for diagnosing team problems (e.g., 'Dive Deep,' 'Insist on the Highest Standards'). Meta's structured onboarding model accelerates new hire effectiveness through immersive ramp-up programs with clear milestones. Calibration signal: \"Crisis leadership defines Director reputation\" (SIG-099).",
    "mappingNotes": "Inherit low-performing team",
    "suggestedMetricIds": [
      "5.1",
      "7.1",
      "7.9",
      "1.2",
      "1.4"
    ]
  },
  {
    "id": "P-C1-2",
    "slug": "layoffs-are-coming-and-you-know-before-your-team",
    "observableIds": [
      "C1-O4",
      "C7-O3"
    ],
    "capabilityIds": [
      "C1",
      "C7"
    ],
    "title": "Layoffs Are Coming and You Know Before Your Team",
    "context": "Your Director told you confidentially: 20% RIF next week. You know who's affected on your team. You can't tell anyone yet.",
    "topicsActivated": [
      "Team Health (Leading Through Layoffs)",
      "Stakeholder Mgmt (Managing Up)",
      "Engineering Culture (Trust)"
    ],
    "decisionFramework": "1. Prepare: talking points for affected and remaining team members. 2. Coordinate: logistics (access revocation, equipment, benefits information). 3. Day of: tell affected people first, in private, with humanity. 4. Same day: address remaining team honestly. 5. Following weeks: over-communicate stability, re-plan roadmap, address survivor guilt. 6. Don't pretend nothing happened.",
    "commonMistakes": "Leaking the news early. Being cold or scripted with affected people. Not being available for remaining team. Immediately jumping to 'let's focus on the work' without acknowledging grief. Not re-planning the roadmap (pretending you can do the same work with fewer people).",
    "whatGoodLooksLike": "Affected people treated with dignity and respect — they remember how they were treated. Remaining team hears honesty: what happened, what's changing, what's not. Roadmap re-planned publicly (team sees you're realistic). 1:1s with every remaining team member within 1 week. Zero additional voluntary attrition in following quarter. Industry reference: Netflix's radical transparency culture means layoff communication is direct and humane. Amazon's 'disagree and commit' principle applies — once the decision is made, execute with compassion and clarity. Calibration signal: \"Directors initiate re-orgs; EMs execute them\" (SIG-040).",
    "mappingNotes": "Layoffs incoming — pre-disclosure",
    "suggestedMetricIds": [
      "7.1",
      "5.1",
      "7.9"
    ]
  },
  {
    "id": "P-C5-2",
    "slug": "your-new-vp-wants-to-change-everything",
    "observableIds": [
      "C5-O10",
      "C1-O4"
    ],
    "capabilityIds": [
      "C5",
      "C1"
    ],
    "title": "Your New VP Wants to Change Everything",
    "context": "New VP joins. They want to re-org, change tech stack, adopt new processes, and reset all roadmaps. Your team is anxious.",
    "topicsActivated": [
      "Team Health (New Leadership, Managing Through Adversity)",
      "Stakeholder Mgmt (Managing Up)",
      "Strategic Alignment (Planning)"
    ],
    "decisionFramework": "1. Listen to VP's vision — understand the intent behind changes. 2. Identify: which changes are reasonable? Which are destructive? 3. Push back constructively on changes that would hurt (with data). 4. Shield team from unnecessary churn while supporting legitimate changes. 5. Communicate to team: what's changing, what's not, what you're advocating for. 6. Build trust with new VP through early results.",
    "commonMistakes": "Immediately complying with everything (your team suffers). Immediately resisting everything (you get replaced). Badmouthing the VP to your team. Letting team anxiety fester without addressing it. Not adapting at all to new leadership's style.",
    "whatGoodLooksLike": "You earn VP's trust by delivering results on their priorities. You protect your team from unnecessary churn. Changes that happen are well-communicated and well-executed. Team sees you as their advocate who also adapts to new reality. Six months later: VP trusts you, team trusts you, things are stable. Industry reference: Amazon's Working Backwards process gives new leaders a structured way to propose changes (PR/FAQ format), which channels 'change everything' energy into evidence-based proposals. Calibration signal: \"Director skill: 'Expanded org scope to include [X] based on [demonstrated capability], increasing org impact by [Y]'\" (SIG-123).",
    "mappingNotes": "New VP changes everything",
    "suggestedMetricIds": [
      "1.2",
      "3.5",
      "5.1",
      "8.4"
    ]
  },
  {
    "id": "P-C6-2",
    "slug": "you-discover-a-senior-engineer-is-quietly-job-searching",
    "observableIds": [
      "C6-O3",
      "C6-O8"
    ],
    "capabilityIds": [
      "C6"
    ],
    "title": "You Discover a Senior Engineer Is Quietly Job Searching",
    "context": "Through the grapevine (or obvious calendar gaps), you realize one of your senior engineers is interviewing elsewhere. They haven't told you.",
    "topicsActivated": [
      "Team Health (Flight Risk, Stay Interviews)",
      "Performance (Managing High Performers, Career Conversations)"
    ],
    "decisionFramework": "1. Don't confront them about job searching (it's their right). 2. Create opportunity for honest career conversation: 'How are you feeling about your growth here? What would make the next year exciting for you?' 3. Listen for the underlying need. 4. If you can address it: propose specific changes (scope, comp, growth path). 5. If you can't: be honest about constraints. 6. Either way: plan for potential departure (knowledge transfer, succession).",
    "commonMistakes": "Confronting them directly about interviewing (violation of trust). Panic counter-offer out of nowhere. Ignoring the signals hoping they'll stay. Punishing them with reduced access or scope. Telling their teammates.",
    "whatGoodLooksLike": "Proactive career conversation that feels natural, not surveillance-triggered. You address underlying needs (if possible) before an offer forces your hand. If they stay: because root cause was addressed. If they leave: you had time to plan. Either way: trust preserved. Industry reference: Netflix proactively conducts 'stay interviews' with key talent rather than waiting for exit signals. Google's manager effectiveness research shows that career development conversations are the #1 retention driver. Calibration signal: \"Counter-offers that don't address root cause just delay departure by 6 months\" (SIG-051).",
    "mappingNotes": "Senior engineer quietly job searching",
    "suggestedMetricIds": [
      "7.1",
      "5.1",
      "7.8"
    ]
  },
  {
    "id": "P-C3-1",
    "slug": "your-team-needs-to-deliver-a-critical-migration-under-tight-deadline",
    "observableIds": [
      "C3-O6",
      "C4-O2"
    ],
    "capabilityIds": [
      "C3",
      "C4"
    ],
    "title": "Your Team Needs to Deliver a Critical Migration Under Tight Deadline",
    "context": "Leadership committed to migrating off a legacy system in Q3. Your team owns it. The deadline is real (vendor contract expiration). Scope is larger than estimated.",
    "topicsActivated": [
      "Technical Strategy (Migration Strategy)",
      "Engineering Excellence (Delivery Practices)",
      "Stakeholder Mgmt (Trade-off Communication, Delivering Bad News)",
      "Team Health (Capacity Planning)"
    ],
    "decisionFramework": "1. Re-estimate honestly: what's actually required? What's the gap vs. plan? 2. Present options: (A) Full migration by deadline with X% overtime risk, (B) Phased migration — critical paths by deadline, remaining by Q4, (C) Full migration with additional resources. 3. Communicate gap EARLY — don't wait until deadline is missed. 4. If proceeding under pressure: protect team health (no sustained overtime), cut scope aggressively, track weekly.",
    "commonMistakes": "Hiding the gap ('we'll figure it out'). Asking team to work overtime for months. Promising delivery you know isn't realistic. Not flagging the risk until it's a crisis. Blaming the estimate instead of managing the situation.",
    "whatGoodLooksLike": "Risk flagged at earliest moment with clear options. Leadership makes informed trade-off decision. Team executes with realistic scope and sustainable pace. Weekly progress visible to stakeholders. If phased: phase 1 delivered on deadline, clear plan for phase 2. Team emerges without burnout. Industry reference: Amazon's 'escalation is not failure' culture means timeline risks are surfaced weekly in business reviews. Google's design doc culture ensures migration scope is documented and reviewed before execution begins.",
    "mappingNotes": "Critical migration under deadline",
    "suggestedMetricIds": [
      "1.2",
      "1.4",
      "2.5",
      "3.5"
    ]
  },
  {
    "id": "P-C14-1",
    "slug": "calibration-committee-wants-to-downgrade-your-engineers-rating",
    "observableIds": [
      "C14-O2",
      "C14-O3"
    ],
    "capabilityIds": [
      "C14"
    ],
    "title": "Calibration Committee Wants to Downgrade Your Engineer's Rating",
    "context": "You submitted 'Exceeds Expectations' for a strong senior engineer. In calibration, peer managers are pushing for 'Meets' based on cross-team comparison. You disagree.",
    "topicsActivated": [
      "Performance (Calibration Preparation, Ratings Distribution)",
      "Stakeholder Mgmt (Influence Without Authority)"
    ],
    "decisionFramework": "1. Present evidence calmly: specific impact examples, peer feedback, cross-team comparison data you prepared. 2. Listen to counter-arguments: is there a valid point you're missing? 3. If your case is strong: hold firm with evidence. 4. If their case has merit: gracefully accept and adjust. 5. Either way: communicate outcome to your engineer honestly. 6. Never: throw the committee under the bus.",
    "commonMistakes": "Caving without defending your case. Getting emotional or combative. Not preparing comparison data in advance. Telling your engineer 'I fought for you but they overruled me' (blames the system, undermines trust in the process). Inflating evidence to win the argument.",
    "whatGoodLooksLike": "Strong, evidence-based case that even skeptics find credible. If you win: earned through preparation, not politics. If you lose: you understand why and can explain it honestly to your engineer. Your credibility in the room is intact or strengthened regardless of outcome. Industry reference: Google's promo committee model makes calibration evidence-based against published rubrics — managers present cases, an independent committee decides, reducing bias from any single manager. Calibration signal: \"Manager prepares calibration cases with 3+ specific examples per rating dimension, anticipates which ratings will be ...\" (SIG-292).",
    "mappingNotes": "Calibration downgrade fight",
    "suggestedMetricIds": [
      "5.1",
      "7.8"
    ]
  },
  {
    "id": "P-C1-3",
    "slug": "a-cross-team-initiative-is-stalled-because-no-one-owns-it",
    "observableIds": [
      "C1-O2",
      "C5-O7"
    ],
    "capabilityIds": [
      "C1",
      "C5"
    ],
    "title": "A Cross-Team Initiative Is Stalled Because No One Owns It",
    "context": "An important cross-org project has been discussed for months but nobody is driving it. Multiple teams would benefit but each expects the other to lead.",
    "topicsActivated": [
      "Cross-Functional (Dependency Negotiation)",
      "Stakeholder Mgmt (Driving Alignment, Political Capital)",
      "Decision Making (First Principles)"
    ],
    "decisionFramework": "1. Assess: should your team own this? (Impact on your goals, capacity, strategic value) 2. If yes: volunteer to drive. Write a proposal with scope, timeline, resource needs. 3. Get buy-in from other teams (pre-wire 1:1). 4. Establish DACI. 5. Drive to completion with regular cross-team syncs. 6. If no: identify who should own it and facilitate that conversation.",
    "commonMistakes": "Waiting for someone else to step up (it's been months — they won't). Volunteering without capacity (good intentions, failed execution). Trying to drive without explicit buy-in from participating teams. Not getting DACI clear upfront.",
    "whatGoodLooksLike": "You either own it and drive it to completion, or you facilitate someone else owning it. Cross-team initiative ships. Your role is visible to leadership. Political capital built through execution, not just volunteering. 'Drove [initiative] across [X] teams, delivering [Y] outcome.' Industry reference: Amazon's single-threaded leadership model assigns one clear owner to every cross-team initiative. The DACI framework (Driver, Approver, Contributors, Informed), widely adopted at companies like Google and Intuit, prevents the 'everyone contributes, nobody drives' stall. Calibration signal: \"Director-level insight: 'Applied inverse Conway — re-orged teams to align with target microservices architecture, ena...\" (SIG-039).",
    "mappingNotes": "Stalled cross-team initiative",
    "suggestedMetricIds": [
      "2.6",
      "1.2",
      "8.1"
    ]
  },
  {
    "id": "P-C14-2",
    "slug": "your-manager-asks-you-to-rate-everyone-meets-expectations",
    "observableIds": [
      "C14-O3",
      "C14-O4"
    ],
    "capabilityIds": [
      "C14"
    ],
    "title": "Your Manager Asks You to Rate Everyone 'Meets Expectations'",
    "context": "Your Director says the org needs to compress ratings this cycle. They want fewer 'Exceeds' ratings. You have 3 engineers who genuinely exceeded.",
    "topicsActivated": [
      "Performance (Ratings Distribution, Delivering Feedback)",
      "Stakeholder Mgmt (Managing Up)",
      "Decision Making (First Principles)"
    ],
    "decisionFramework": "1. Understand the constraint: is this a hard mandate or guidance? What's driving it? 2. Present your strongest case: 'I can compress 2 of the 3, but this one engineer delivered [X] — here's the evidence.' 3. If Director insists across the board: push back with data one more time, clearly. 4. If overruled: you own the outcome — deliver feedback honestly to your engineers. 5. Never: blame your Director to your team.",
    "commonMistakes": "Silently complying and giving unfair ratings. Openly fighting your Director in calibration (burning political capital). Telling your engineers 'I wanted to give you Exceeds but my boss said no.' Distributing the pain randomly instead of based on evidence.",
    "whatGoodLooksLike": "You advocate clearly with evidence. You compress where reasonable and hold firm where it matters most. If compromised: you deliver honest feedback to affected engineers ('here's what you accomplished, here's the context for the rating, here's your growth path'). Trust preserved in both directions. Industry reference: Google's calibration committees include managers from outside the immediate org to prevent local rating compression. Netflix skips forced distributions entirely, trusting managers to differentiate honestly. Calibration signal: \"Directors manage distribution across their EMs — the meta-calibration\" (SIG-077).",
    "mappingNotes": "Forced ratings compression",
    "suggestedMetricIds": [
      "7.1",
      "7.9",
      "5.1"
    ]
  },
  {
    "id": "P-C11-1",
    "slug": "you-need-to-build-a-new-team-from-scratch",
    "observableIds": [
      "C11-O1",
      "C11-O2",
      "C11-O4"
    ],
    "capabilityIds": [
      "C11"
    ],
    "title": "You Need to Build a New Team From Scratch",
    "context": "You've been given headcount to build a new team for a greenfield project. You have 6 months to hire 6 engineers and deliver first milestone.",
    "topicsActivated": [
      "Talent Acquisition (Hiring Loops, Bar, Onboarding)",
      "Org Design (Team Structure)",
      "Technical Strategy (Vision)",
      "Team Health (First 90 Days)"
    ],
    "decisionFramework": "1. First: define team mission, scope, and technical vision (BEFORE hiring). 2. Hire tech lead first (partner on architecture and subsequent hiring). 3. Hiring loop: calibrated, structured, bar raiser. Don't lower bar for speed. 4. Onboard in waves (don't hire all 6 simultaneously). 5. Establish team charter, principles, and rituals from day 1. 6. First milestone should be deliberately achievable — team needs an early win.",
    "commonMistakes": "Hiring all 6 simultaneously (onboarding chaos). Not defining technical direction before hiring (building a team without a mission). Hiring fast at the expense of quality. Not establishing culture intentionally (letting it happen by accident). Setting unrealistic first milestone.",
    "whatGoodLooksLike": "Tech lead hired in month 1, contributing to architecture and hiring. Engineers hired in waves of 2, each wave onboarded before next. Team charter established early. First milestone delivered on time — team has confidence and momentum. By month 6: functioning, cohesive team with clear identity. Industry reference: Amazon's 'two-pizza team' principle with single-threaded leadership provides a proven template for new team formation. Meta's structured onboarding program gets new engineers productive within 6 weeks through mentorship pairing and progressive ramp-up milestones. Calibration signal: \"Director-level skill: 'Secured [X] headcount by demonstrating [Y] revenue impact / [Z] risk if unfilled'\" (SIG-068).",
    "mappingNotes": "Build team from scratch",
    "suggestedMetricIds": [
      "7.3",
      "7.4",
      "7.5"
    ]
  },
  {
    "id": "P-C6-3",
    "slug": "one-of-your-ems-is-struggling-director-scenario",
    "observableIds": [
      "C6-O6",
      "C6-O7"
    ],
    "capabilityIds": [
      "C6"
    ],
    "title": "One of Your EMs Is Struggling (Director Scenario)",
    "context": "One of your EMs is underperforming: their team's metrics are declining, skip-level feedback is concerning, and the EM seems overwhelmed.",
    "topicsActivated": [
      "Managing Managers (Coaching EMs, Skip-Levels)",
      "Performance (Underperformance)",
      "Team Health (First 90 Days for the EM's team)"
    ],
    "decisionFramework": "1. Diagnose the root cause first — don't assume: is this a skill gap (can't do it), will gap (won't do it), or situation gap (set up to fail — wrong scope, inadequate resources, inherited dysfunction)? Each requires a fundamentally different intervention. 2. If skill gap: build a structured coaching plan with specific milestones — weekly 1:1s focused on the exact skills needed (e.g., running effective team meetings, giving direct feedback, technical credibility). See P-C6-4 for detailed coaching frameworks. 3. If will gap: have a direct, honest conversation about expectations and mutual fit. Some people don't want to be managers — creating a path back to IC can be a win for everyone. 4. If situation gap: fix the situation before judging the person. Reduce scope, add a TL, remove a toxic team member, clarify expectations with your leadership. 5. Set a clear 60-90 day improvement window with explicit success criteria. Document what improvement looks like so both of you can measure it objectively.",
    "commonMistakes": "Ignoring it hoping they'll figure it out — the EM's team suffers while you wait. Taking over their responsibilities yourself — they never develop and you become the bottleneck. Not diagnosing the root cause (skill/will/situation) and applying the wrong intervention. Waiting too long to act while the team's best engineers start interviewing elsewhere. Replacing them without any genuine coaching attempt, which signals to your other EMs that they have no safety net.",
    "whatGoodLooksLike": "Honest root-cause diagnosis shared with the EM within 1 week. Specific coaching plan with measurable milestones (not vague 'be better at X'). If skill gap: EM shows measurable improvement within 60 days because the coaching addressed the actual gap. If situation gap: you fixed the environment and the EM thrived — proving your diagnostic was right. If will gap: managed transition with dignity, the EM's team was protected throughout, and replacement was ready. Microsoft's Model-Coach-Care framework provides the diagnostic lens: skill gap (coach), will gap (direct conversation), situation gap (fix the environment). Calibration signal: \"'Developed [X] engineers into management roles' — org building evidence\" (SIG-096).",
    "mappingNotes": "Struggling EM (Director scenario)",
    "suggestedMetricIds": [
      "5.1",
      "7.1",
      "7.9",
      "1.2"
    ]
  },
  {
    "id": "P-C8-2",
    "slug": "production-outage-caused-by-your-teams-deploy",
    "observableIds": [
      "C8-O1",
      "C8-O4"
    ],
    "capabilityIds": [
      "C8"
    ],
    "title": "Production Outage Caused by Your Team's Deploy",
    "context": "Your team deployed a change that caused a major production incident. Customers are affected. Leadership is asking what happened.",
    "topicsActivated": [
      "Operational Risk (Incident Response, Post-Mortems)",
      "Stakeholder Mgmt (Delivering Bad News, Status Communication)",
      "Engineering Excellence (Release Management, Testing)"
    ],
    "decisionFramework": "1. First: resolve the incident (rollback, mitigation). 2. Communicate: status updates to stakeholders every 30min during incident. 3. Take ownership immediately with leadership: 'our deploy caused this, here's what we're doing.' 4. Post-incident: blameless post-mortem within 48hrs. 5. Action items with owners and deadlines. 6. Prevention: what process/test/check would have caught this? 7. Communicate lessons learned broadly.",
    "commonMistakes": "Pointing fingers at the engineer who deployed. Hiding or minimizing the impact. Not communicating status during the incident. Post-mortem that's actually a blame session. Promising 'it won't happen again' without systemic changes.",
    "whatGoodLooksLike": "Incident resolved quickly through documented process. Ownership taken immediately and clearly. Blameless post-mortem produces systemic improvements (not 'be more careful'). Action items completed within 2 weeks. Similar class of incident prevented going forward. Team trust preserved. Industry reference: Google's blameless post-mortem template requires systemic root causes and rejects 'human error' as an acceptable finding. Amazon's COE process ensures action items have owners, deadlines, and executive visibility. Calibration signal: \"Standard practice at Big Tech — EMs who skip this create visible risk\" (SIG-033).",
    "mappingNotes": "Outage caused by your deploy",
    "suggestedMetricIds": [
      "1.4",
      "1.3",
      "3.3",
      "3.5"
    ]
  },
  {
    "id": "P-C5-3",
    "slug": "youre-asked-to-take-on-scope-that-doesnt-fit-your-team",
    "observableIds": [
      "C5-O3",
      "C2-O2"
    ],
    "capabilityIds": [
      "C5",
      "C2"
    ],
    "title": "You're Asked to Take on Scope That Doesn't Fit Your Team",
    "context": "Your VP wants your team to own a new area that doesn't align with your team's expertise or mission. It's a political hot potato nobody else wants.",
    "topicsActivated": [
      "Org Design (Cognitive Load, Single-Threaded Ownership)",
      "Stakeholder Mgmt (Managing Up, Framing Asks)",
      "Decision Making (Saying No, Trade-offs)"
    ],
    "decisionFramework": "1. Understand the real ask: why your team? What's the actual need? 2. Assess honestly: can your team absorb this without degrading current work? What's the cost? 3. If no: propose alternatives (which team should own this? new team needed?). 4. If yes with conditions: 'we can take this if we deprioritize X or get Y headcount.' 5. Never: silently absorb scope that will crush your team.",
    "commonMistakes": "Accepting without understanding the cost. Declining without offering alternatives. Complaining to your team about leadership decisions. Absorbing it and burning out your team to prove you can handle it.",
    "whatGoodLooksLike": "Clear-eyed assessment presented to VP: 'here's what this would cost us' with specific trade-offs. If you accept: explicit agreement on what you're deprioritizing or what resources you're getting. If you decline: alternative proposal that solves the VP's actual problem. Either way: you're seen as thoughtful and solutions-oriented. Industry reference: Amazon's 'two-pizza team' cognitive load principle provides a framework for saying no to scope expansion — teams should own what they can cognitively manage, not what politics assigns. Calibration signal: \"Director-level: 'Technical insight led to [product direction change], driving [Y] outcome that PM hadn't considered'\" (SIG-108).",
    "mappingNotes": "Asked to take misfit scope",
    "suggestedMetricIds": [
      "8.4",
      "2.5",
      "2.6"
    ]
  },
  {
    "id": "P-C5-4",
    "slug": "a-key-platform-dependency-is-unreliable-and-the-platform-team-wont-prioritize-fixes",
    "observableIds": [
      "C5-O3",
      "C5-O4"
    ],
    "capabilityIds": [
      "C5"
    ],
    "title": "A Key Platform Dependency Is Unreliable and the Platform Team Won't Prioritize Fixes",
    "context": "Your team's reliability is suffering because of an upstream platform dependency. The platform team acknowledges the issue but has higher priorities.",
    "topicsActivated": [
      "Cross-Functional (Dependency Negotiation, Platform Partnership)",
      "Technical Strategy (Dependency Management)",
      "Operational Risk (Dependency Risk)"
    ],
    "decisionFramework": "1. Quantify impact: how many incidents? Customer impact? Eng hours spent on workarounds? 2. Build relationship with platform EM (understand their constraints). 3. Propose mutual solutions: can you contribute a fix? Can you implement a fallback? 4. If blocked: escalate with data to Director level. 5. Parallel: build resilience against this dependency (circuit breakers, fallbacks, caching).",
    "commonMistakes": "Complaining without data. Escalating without first trying direct partnership. Building a competing solution in secret. Blaming the platform team publicly. Doing nothing and accepting the pain.",
    "whatGoodLooksLike": "Data-driven conversation with platform team. Mutual problem-solving — maybe you contribute the fix, they review and merge. Fallback strategy implemented regardless. If escalation needed: both EMs present jointly to leadership with shared facts and proposed solution. Relationship preserved. Industry reference: Google's SRE model defines explicit SLOs and error budgets for platform dependencies, giving consuming teams concrete data for escalation. Amazon's service ownership model means every dependency has a named owner. Calibration signal: \"Director-level: 'Technical insight led to [product direction change], driving [Y] outcome that PM hadn't considered'\" (SIG-108).",
    "mappingNotes": "Unreliable platform dependency",
    "suggestedMetricIds": [
      "3.5",
      "3.3",
      "1.2"
    ]
  },
  {
    "id": "P-C12-2",
    "slug": "your-team-has-zero-diversity-and-youre-told-to-fix-it",
    "observableIds": [
      "C12-O5",
      "C11-O3"
    ],
    "capabilityIds": [
      "C12",
      "C11"
    ],
    "title": "Your Team Has Zero Diversity and You're Told to Fix It",
    "context": "HR/leadership points out your team is homogeneous. You're asked to 'improve diversity' in your next hires.",
    "topicsActivated": [
      "Talent Acquisition (Sourcing, Hiring Loops)",
      "Engineering Culture (Inclusive Environment)",
      "Decision Making (First Principles)"
    ],
    "decisionFramework": "1. Audit the full pipeline: where are candidates dropping off? (Sourcing? Screen? Onsite? Offer?) 2. Fix the pipeline, not the outcome: diverse sourcing channels, structured interviews with rubrics, diverse interview panels, bias training. 3. Fix retention too: is your team environment actually inclusive? Would diverse hires thrive? 4. Never: lower the bar or hire for optics. 5. This is a sustained effort, not a one-quarter initiative.",
    "commonMistakes": "Token hiring (one diverse hire into unwelcoming environment — they leave). Lowering the bar (insulting and counterproductive). Treating it as a checkbox exercise. Only fixing sourcing without fixing culture. Delegating entirely to recruiting team.",
    "whatGoodLooksLike": "Pipeline audited and fixed at each stage. Interview process restructured to reduce bias (structured rubrics, diverse panels). Culture assessed honestly — are diverse voices actually heard? Sustained effort over multiple quarters. Diversity improves AND inclusion improves AND bar maintained. Industry reference: Meta's structured hiring rubrics evaluate demonstrated competencies (not 'culture fit'), and interview panels are deliberately diverse. Google's hiring committees review for bias patterns across interview cycles. Calibration signal: \"D&I is a team health and innovation driver\" (SIG-090).",
    "mappingNotes": "Zero diversity fix mandate",
    "suggestedMetricIds": [
      "7.7",
      "5.1",
      "7.4"
    ]
  },
  {
    "id": "P-C7-1",
    "slug": "you-strongly-disagree-with-a-decision-made-above-you",
    "observableIds": [
      "C7-O1",
      "C7-O7"
    ],
    "capabilityIds": [
      "C7"
    ],
    "title": "You Strongly Disagree with a Decision Made Above You",
    "context": "Your Director/VP made a strategic decision you think is wrong. It directly affects your team's direction.",
    "topicsActivated": [
      "Stakeholder Mgmt (Managing Up)",
      "Decision Making (Reversible vs Irreversible, First Principles)",
      "Team Health (Trust)"
    ],
    "decisionFramework": "1. Understand the full context (you might not have it). Ask questions, not accusations. 2. If you still disagree after understanding context: make your case once, clearly, with data. 3. If overruled: disagree and commit. Execute fully. 4. Never: undermine the decision through passive resistance or complaining to your team. 5. If the decision fails: don't say 'I told you so.' Help fix it.",
    "commonMistakes": "Going along silently (your perspective was valuable and you withheld it). Fighting the same battle repeatedly after the decision is made. Telling your team 'I disagree but we have to do it' (undermines the decision and your leadership above). Sabotaging through half-hearted execution.",
    "whatGoodLooksLike": "You made your case clearly and respectfully. After the decision: you commit fully and execute as if it were your own idea. Your team sees you supporting the direction. If the decision proves wrong: you help course-correct without blame. Your Director remembers you raised the concern constructively. Industry reference: Amazon's 'disagree and commit' principle provides a cultural framework: voice dissent clearly with data, then execute fully once the decision is made. Netflix's culture memo explicitly states that 'silence is agreement.' Calibration signal: \"Judgment signal: 'Identified [X] issues in first month, addressed critical ones immediately, built 90-day plan for sy...\" (SIG-064).",
    "mappingNotes": "Disagree with decision above you",
    "suggestedMetricIds": [
      "8.1",
      "8.2",
      "8.4"
    ]
  },
  {
    "id": "P-C5-5",
    "slug": "youre-preparing-for-your-own-promotion-to-director",
    "observableIds": [
      "C5-O5",
      "C5-O6",
      "C14-O2"
    ],
    "capabilityIds": [
      "C5",
      "C14"
    ],
    "title": "You're Preparing for Your Own Promotion to Director",
    "context": "You've been a strong EM for 2+ years and want to make the jump to Director. What do you actually need to demonstrate?",
    "topicsActivated": [
      "Performance (Promo Packets, Career Growth)",
      "Stakeholder Mgmt (Sponsor Relationships, Executive Communication)",
      "Managing Managers (all principles)",
      "Org Design (all principles)"
    ],
    "decisionFramework": "1. Map current scope to Director-level rubric — identify gaps honestly. 2. Build evidence of Director-level work: cross-team impact, org-level strategy, developing other managers. 3. Get a sponsor (not just a mentor) at VP+ level. 4. Demonstrate you can operate at Director altitude while still delivering as EM. 5. Your promo case should be obvious before you submit it.",
    "commonMistakes": "Asking for the title without doing the job. Focusing on managing your team well (necessary but not sufficient — Directors operate across teams). Not building sponsor relationships. Waiting for your manager to nominate you instead of driving your own career. Neglecting your current team while chasing Director scope.",
    "whatGoodLooksLike": "You've been operating at Director level for 6+ months before the promo discussion. Cross-team impact is documented. You've developed at least one future EM. You have a sponsor who advocates for you. Your current team continues to thrive. The promo feels like recognition of reality, not aspiration. Industry reference: Google's promo committee model evaluates cross-team impact and leadership scope, not just team management. Amazon expects Director candidates to demonstrate 'Hire and Develop the Best' through their EMs' independence. Calibration signal: \"Directors broker org-level platform partnerships: 'Partnered with [platform team] on [capability], adopted by [X] teams'\" (SIG-110).",
    "mappingNotes": "Preparing own Director promotion",
    "suggestedMetricIds": [
      "1.2",
      "3.5",
      "7.1",
      "8.4",
      "8.2"
    ]
  },
  {
    "id": "P-C9-1",
    "slug": "dora-metrics-show-decline-but-team-feels-fine",
    "observableIds": [
      "C9-O1",
      "C9-O3"
    ],
    "capabilityIds": [
      "C9"
    ],
    "title": "DORA Metrics Show Decline But Team Feels Fine",
    "context": "Your DORA metrics have been trending down for 2 quarters — deploy frequency dropped 40%, lead time increased 3x. But team morale is fine and nobody is complaining. Your VP saw the dashboard and is asking questions.",
    "topicsActivated": [
      "Metrics Measurement (DORA analysis)",
      "Activity Metrics (diagnostic vs punitive)",
      "Stakeholder Mgmt (framing to VP)"
    ],
    "decisionFramework": "1. Investigate root cause: is it real degradation or measurement artifact (team grew, monolith split, deploy definition changed)? 2. Correlate with outcomes: are incidents up? Is velocity actually down? Customer impact? 3. If real: identify bottleneck (CI pipeline? review queue? test suite? environment contention?). 4. If artifact: fix the measurement, communicate context to VP. 5. Either way: present findings with context, not just numbers. 6. Create improvement plan with team ownership, not top-down targets.",
    "commonMistakes": "Setting DORA targets as team KPIs (Goodhart's Law). Gaming deploy frequency by splitting deploys. Presenting raw numbers to VP without context. Blaming the team instead of investigating systemic causes. Ignoring the signal because \"team feels fine.\"",
    "whatGoodLooksLike": "Root cause identified within 1 week. VP gets context-rich update (not just \"we'll fix it\"). If real: targeted improvement with measurable progress within 1 quarter. If artifact: measurement corrected, team educated on what metrics actually mean. Metrics used as shared investigation tool, never punishment. Industry reference: Google's DORA team recommends investigating metrics as diagnostic signals, not setting them as performance targets. Microsoft's SPACE framework pairs behavioral metrics (DORA) with perceptual metrics (developer satisfaction) to avoid tunnel vision. Calibration signal: \"Calibration evidence: 'Reduced lead time from 5 days to 8 hours by [specific action: eliminated manual QA gate / para...\" (SIG-013).",
    "mappingNotes": "DORA decline investigation scenario — metrics as diagnostic",
    "suggestedMetricIds": [
      "1.2",
      "1.4",
      "1.1",
      "5.1",
      "2.5"
    ]
  },
  {
    "id": "P-C10-1",
    "slug": "budget-cut-lose-20-headcount-keep-critical-deliverables",
    "observableIds": [
      "C10-O1",
      "C10-O2",
      "C10-O5"
    ],
    "capabilityIds": [
      "C10"
    ],
    "title": "Budget Cut: Lose 20% Headcount, Keep Critical Deliverables",
    "context": "Company announces 20% headcount reduction. You have 25 engineers across 4 teams. You need to identify 5 positions to cut while maintaining delivery on your top-3 commitments. You have 2 weeks before the announcement.",
    "topicsActivated": [
      "Resource Allocation (headcount planning)",
      "Strategic Prioritization (ruthless triage)",
      "People Management (layoff execution)",
      "Stakeholder Mgmt (managing up during crisis)"
    ],
    "decisionFramework": "1. Map all current work to business impact tiers: critical (revenue/compliance), important (strategic), nice-to-have (quality-of-life). 2. Identify what STOPS — be explicit, write it down. 3. Assess team composition: performance, criticality of knowledge, single points of failure. 4. Optimize for org survival: protect critical path, maintain minimum viable team per area. 5. Prepare talking points for affected individuals (with your HR partner). 6. Tell affected people FIRST, with humanity. 7. Re-plan roadmap with remaining team within 1 week. 8. Communicate revised plan to stakeholders within 2 weeks.",
    "commonMistakes": "Spreading cuts evenly across teams (peanut butter approach). Cutting based solely on performance ratings. Not communicating what stops. Protecting pet projects over business-critical work. Delaying the conversation with affected people. Not re-planning the roadmap after cuts.",
    "whatGoodLooksLike": "Clear-eyed impact analysis delivered to leadership within days. Affected individuals told with dignity and support. Remaining team has revised, achievable roadmap within 1 week. Stakeholders know what's changing. Zero regrettable attrition from survivors. Team productive within 3-4 weeks. Industry reference: Netflix prioritizes density of talent over headcount — fewer, stronger engineers over more warm bodies. Amazon's resource allocation process forces explicit Tier 1/2/3 prioritization during budget constraints. Calibration signal: \"Directors own org-level headcount story; EMs own team-level justification\" (SIG-124).",
    "mappingNotes": "Headcount reduction scenario — resource allocation under constraint",
    "suggestedMetricIds": [
      "8.4",
      "8.2",
      "7.1",
      "1.2"
    ]
  },
  {
    "id": "P-C13-1",
    "slug": "security-audit-finding-critical-vulnerability-in-production",
    "observableIds": [
      "C13-O1",
      "C13-O2",
      "C13-O4"
    ],
    "capabilityIds": [
      "C13"
    ],
    "title": "Security Audit Finding: Critical Vulnerability in Production",
    "context": "Internal security audit found a critical vulnerability (e.g., unencrypted PII in logs, open S3 bucket, SQL injection vector) in your team's production service. Security team has given you a 72-hour remediation window. Your team is mid-sprint on a high-priority feature launch.",
    "topicsActivated": [
      "Security Compliance (vulnerability SLAs)",
      "Operational Risk (incident-adjacent)",
      "Stakeholder Mgmt (communicating priority shift)",
      "Strategic Prioritization (competing demands)"
    ],
    "decisionFramework": "1. Assess severity and blast radius immediately — what data is exposed, who is affected, what's the regulatory implication? 2. Communicate to your manager and stakeholders SAME DAY: feature timeline is shifting. 3. Assign your best engineer to the fix — this is not junior work. 4. Implement fix + write regression test + add to automated scanning to prevent recurrence. 5. Document root cause: how did this get to production? What process failed? 6. Close the loop with security team within SLA. 7. Post-fix: update threat model, add to security review checklist, share learning broadly.",
    "commonMistakes": "Treating it as security team's problem. Trying to fix it quietly without telling stakeholders about the feature delay. Assigning it to whoever is \"free\" instead of your most capable engineer. Fixing the symptom without addressing how it got to production. Not communicating the feature timeline impact upward.",
    "whatGoodLooksLike": "Vulnerability patched within 72hrs. Root cause documented. Process gap identified and closed (new automated check, updated review checklist). Stakeholders informed proactively about feature delay. Security team relationship strengthened, not adversarial. Team learns from it — similar vulnerability never recurs. Industry reference: Google's zero-day response model uses pre-defined severity tiers with SLA-based response timelines. Amazon's mandatory security review process ensures vulnerabilities are caught before launch, not after audit. Calibration signal: \"Auditors and security teams track this\" (SIG-025).",
    "mappingNotes": "Critical security finding response scenario",
    "suggestedMetricIds": [
      "9.1",
      "9.3",
      "9.2"
    ]
  },
  {
    "id": "P-C3-2",
    "slug": "your-teams-tech-stack-becomes-deprecated-company-wide",
    "observableIds": [
      "C3-O4",
      "C3-O6"
    ],
    "capabilityIds": [
      "C3"
    ],
    "title": "Your Team's Tech Stack Becomes Deprecated Company-Wide",
    "context": "Platform team announces your primary framework/language is sunset. Migration timeline is 18 months. You have 100K lines of code in the deprecated stack, active feature development in flight, and no one on your team has deep experience with the target stack.",
    "topicsActivated": [
      "Technical Strategy (Migration Planning, Tech Debt)",
      "Team Health (Skill Development, Morale)",
      "Stakeholder Mgmt (Timeline Negotiation)",
      "Engineering Excellence (Incremental Migration)"
    ],
    "decisionFramework": "1. Assess blast radius: how much of your codebase is affected? What's the dependency graph? Which services are most critical? 2. Evaluate team readiness: who has experience with the target stack? What training is needed? Budget learning time explicitly. 3. Negotiate timeline: 18 months may be unrealistic for 100K LOC — present data-driven counter-proposal if needed. 4. Choose migration strategy: strangler fig (incremental) vs. big bang rewrite. Almost always choose strangler fig. 5. Create migration backlog: prioritize by risk (most critical services first) and coupling (least coupled first for early wins). 6. Run feature development and migration in parallel — do NOT freeze features for 18 months. 7. Establish migration metrics: % migrated, velocity per sprint, blockers. Report weekly to platform team and leadership. 8. Negotiate for dedicated migration capacity (at least 30% of team bandwidth).",
    "commonMistakes": "Attempting a big-bang rewrite (historically fails at 100K+ LOC scale). Freezing all feature work to focus on migration (business won't tolerate it). Not investing in team upskilling early (migration stalls in month 3 when nobody knows the new stack). Treating migration as purely technical without communicating business impact upward. Waiting until month 12 to flag that the timeline is unrealistic. Not negotiating for dedicated capacity — trying to squeeze migration into 'spare time.'",
    "whatGoodLooksLike": "Strangler fig migration plan published within 2 weeks of announcement. Team upskilling started immediately (pairing sessions, training budget, internal tech talks). First service migrated within 6 weeks as proof of concept. Feature development continues at 70% capacity during migration. Weekly migration metrics visible to all stakeholders. Timeline renegotiated early if needed — with data, not excuses. Team emerges with new skills and stronger architecture. Zero production incidents caused by the migration itself. Industry reference: Google's internal migrations use the strangler fig pattern (coined by Martin Fowler) combined with design doc culture to provide a proven framework for large-scale stack transitions. Amazon tracks migration completion as a first-class program metric in weekly business reviews. Calibration signal: \"Tech debt tracking explicitly includes AI-generated code debt — monitoring for DRY violations, duplicated logic acros...\" (SIG-303).",
    "mappingNotes": "Tech stack deprecation and large-scale migration planning",
    "suggestedMetricIds": [
      "8.4",
      "8.6",
      "1.2",
      "3.5"
    ]
  },
  {
    "id": "P-C1-4",
    "slug": "merging-two-teams-after-acquisition-re-org",
    "observableIds": [
      "C1-O3",
      "C1-O14"
    ],
    "capabilityIds": [
      "C1"
    ],
    "title": "Merging Two Teams After Acquisition/Re-org",
    "context": "Company acquired a startup. You're responsible for merging their 8-person team with your 12-person team. Different tech stacks (they use Python/Django, you use Java/Spring), different cultures (startup speed vs. enterprise process), different processes (they have no sprint planning, you run 2-week sprints with full ceremonies). Some role overlap exists. Both teams are anxious about who stays, who leads, and what changes.",
    "topicsActivated": [
      "Org Design (Team Mergers, Culture Integration)",
      "Team Health (Psychological Safety, Trust Building)",
      "Technical Strategy (Stack Consolidation)",
      "Stakeholder Mgmt (Managing Up, Managing Expectations)"
    ],
    "decisionFramework": "1. Week 1: Meet every person on the acquired team individually — understand their role, concerns, what they value about their current culture. Listen more than you talk. 2. Week 2: Identify overlaps and complementary skills. Map who owns what. Do NOT make org changes yet. 3. Weeks 3-4: Establish shared rituals — joint standup, shared Slack channel, cross-team pairing sessions. Let cultures cross-pollinate before forcing convergence. 4. Month 2: Make org structure decisions — be transparent about rationale. If roles overlap, decide based on skill and contribution, not 'which team they came from.' 5. Month 2-3: Consolidate processes gradually — take the best of both. Don't force the acquired team to adopt all your processes wholesale. 6. Months 3-6: Tech stack convergence plan — decide on target stack with input from both teams. This is a multi-quarter effort. 7. Throughout: over-communicate. Ambiguity breeds anxiety and attrition.",
    "commonMistakes": "Treating the acquired team as subordinate ('you're joining us, adopt our ways'). Making org changes in week 1 before understanding the people. Forcing immediate tech stack convergence (creates resentment and risk). Ignoring cultural differences and expecting instant alignment. Losing key acquired talent because they feel undervalued. Not addressing role overlap directly — letting it fester into political infighting. Assuming your processes are better because you're the acquirer.",
    "whatGoodLooksLike": "Every acquired team member feels heard within the first 2 weeks. Org structure decisions made transparently with clear rationale. Best practices adopted from BOTH teams (acquired team sees their ideas valued). Zero regrettable attrition from either team in the first 6 months. Shared team identity emerges organically. Tech stack convergence plan is data-driven and has buy-in from engineers on both sides. By month 6: it's one team, not 'us and them.' Industry reference: Spotify's Squad model demonstrates that teams can maintain identity and autonomy even through organizational changes. Meta's structured onboarding ensures acquired engineers integrate through progressive immersion with mentorship pairing rather than forced assimilation. Calibration signal: \"'Executed re-org of [X] engineers across [Y] teams with zero regrettable attrition and full productivity restored wit...\" (SIG-041).",
    "mappingNotes": "Post-acquisition team merger and cultural integration",
    "suggestedMetricIds": [
      "1.2",
      "5.1",
      "7.1",
      "2.5"
    ]
  },
  {
    "id": "P-C8-3",
    "slug": "managing-through-a-major-security-incident-data-breach",
    "observableIds": [
      "C8-O1",
      "C8-O2",
      "C13-O1",
      "C13-O2"
    ],
    "capabilityIds": [
      "C8",
      "C13"
    ],
    "title": "Managing Through a Major Security Incident (Data Breach)",
    "context": "Security team discovers unauthorized data access to your team's service. Customer PII (names, emails, hashed passwords) may be compromised. The scope is unclear — could be hundreds or millions of records. Legal, PR, and the exec team are involved. Your team owns the affected service. The incident is already escalated to the CEO. Clock is ticking on regulatory notification requirements (72 hours under GDPR).",
    "topicsActivated": [
      "Operational Risk (Incident Command, War Room)",
      "Security Compliance (Breach Response, Regulatory Notification)",
      "Stakeholder Mgmt (Exec Communication, Cross-Functional Coordination)",
      "Team Health (Crisis Management, Protecting Engineers)"
    ],
    "decisionFramework": "1. Immediate (Hour 0-4): Establish incident command structure. Your role is EM-level incident commander for the technical response. Contain the breach — revoke compromised credentials, patch the vulnerability, preserve forensic evidence (do NOT destroy logs). 2. Hour 4-12: Scope the impact — how many records, what data types, what time period. Work with security team on forensic analysis. Provide regular updates to exec war room (every 2 hours). 3. Hour 12-48: Support legal team with technical facts for regulatory notification. Prepare customer-facing technical explanation (work with PR). Continue forensic analysis — understand attack vector completely. 4. Day 2-7: Implement permanent fix (not just patch). Conduct thorough security review of adjacent systems. Begin post-incident review. 5. Week 2-4: Full post-mortem with systemic improvements. Security architecture review. Team well-being check — breaches are extremely stressful. 6. Throughout: protect your engineers from exec pressure. They need to focus on the technical response, not fielding questions from 15 VPs.",
    "commonMistakes": "Destroying evidence in the rush to fix (deleting logs, wiping servers). Downplaying severity to leadership ('it's probably not that bad'). Letting every executive directly ping your engineers for status. Not involving legal immediately (regulatory clock is ticking). Promising a root cause before forensics are complete. Blaming the engineer who wrote the vulnerable code. Not checking on your team's well-being during and after the crisis. Treating the post-mortem as optional because 'we already fixed it.'",
    "whatGoodLooksLike": "Breach contained within hours, not days. Forensic evidence preserved from minute one. Exec team gets regular, honest updates (not optimistic guesses). Legal has what they need for regulatory notification within 72 hours. Customer communication is transparent and accurate. Root cause identified and systemic fix implemented (not just a patch). Security review of adjacent systems completed. Post-mortem produces architectural improvements that prevent this class of vulnerability. Your team is recognized for their response, not blamed for the breach. No one burns out from the crisis. Industry reference: Google's BeyondCorp zero-trust model and incident response framework provide a mature template for breach containment. Amazon's security incident response requires forensic evidence preservation from minute one. Calibration signal: \"Auditors and security teams track this\" (SIG-025).",
    "mappingNotes": "Major security incident and data breach response",
    "suggestedMetricIds": [
      "9.1",
      "9.3",
      "1.3",
      "3.5"
    ]
  },
  {
    "id": "P-C6-4",
    "slug": "your-skip-level-reports-are-unhappy-with-their-em",
    "observableIds": [
      "C6-O6",
      "C6-O11"
    ],
    "capabilityIds": [
      "C6"
    ],
    "title": "Your Skip-Level Reports Are Unhappy With Their EM",
    "context": "In skip-level 1:1s, multiple engineers (3 out of 7) express frustration with their EM. Recurring themes: lack of career development conversations, micromanagement on technical decisions, poor communication about team priorities, and feeling unheard. The EM has been in role for 18 months and was promoted from within. Their self-assessment is that things are going well.",
    "topicsActivated": [
      "Managing Managers (Skip-Level Feedback, Coaching EMs)",
      "Performance (Underperformance, Feedback Delivery)",
      "Team Health (Psychological Safety, Trust)"
    ],
    "decisionFramework": "1. Validate the signal: are 3 engineers saying the same thing independently, or is this a clique? Are the themes consistent? Is there performance data that corroborates (retention risk, engagement survey scores, velocity trends)? 2. Do NOT relay skip-level feedback verbatim to the EM — that destroys skip-level trust. Instead, synthesize themes: 'I'm hearing signals that career development and communication could be stronger on your team.' 3. Have a direct coaching conversation with the EM: share observed themes (not attributing to specific people), ask for their perspective, listen for self-awareness. 4. Co-create a development plan: specific actions on career conversations (monthly with each report), delegation framework (decide which technical decisions they truly need to own vs. delegate), communication cadence (weekly team priorities update). 5. Set a 60-day checkpoint. Increase your own 1:1 cadence with this EM to weekly. 6. Follow up with skip-level reports in 30 and 60 days — is it improving? 7. If no improvement after 60 days with genuine coaching: this may be a performance conversation, not a coaching conversation.",
    "commonMistakes": "Sharing skip-level feedback directly with attribution ('Alex said you're micromanaging') — destroys skip-level trust permanently. Ignoring the signal because the EM's self-assessment is positive. Taking over the EM's responsibilities instead of coaching them. Moving engineers to other teams without addressing the root cause (the EM's behavior). Giving vague feedback ('you need to communicate better') without specific, actionable guidance. Waiting too long — 3 unhappy engineers become 5, then attrition starts.",
    "whatGoodLooksLike": "EM receives synthesized, actionable feedback without knowing who said what. Development plan has specific, measurable actions (not 'be better at communication'). EM shows genuine self-awareness and desire to improve. Skip-level reports notice improvement within 30-60 days. If EM improves: you developed a manager and built trust in both directions. If EM doesn't improve: you have documented pattern and can make a fair decision. Either way: skip-level reports trust that their feedback was heard and acted upon. Industry reference: Google's manager effectiveness survey provides anonymous, structured feedback that Directors can use to identify coaching gaps without burning skip-level trust. Microsoft's 'Connects' continuous feedback model catches issues before they become patterns. Calibration signal: \"Director's highest-leverage people work\" (SIG-154).",
    "mappingNotes": "Skip-level dissatisfaction with EM — coaching and intervention",
    "suggestedMetricIds": [
      "5.1",
      "7.8",
      "7.1"
    ]
  },
  {
    "id": "P-C10-2",
    "slug": "budget-is-frozen-but-commitments-remain",
    "observableIds": [
      "C10-O2",
      "C10-O5",
      "C2-O2"
    ],
    "capabilityIds": [
      "C10",
      "C2"
    ],
    "title": "Budget Is Frozen But Commitments Remain",
    "context": "Mid-year budget freeze announced. No new hires (you had 4 open reqs), no new vendor contracts, limited travel and conference budget. But your Q3-Q4 commitments haven't been adjusted. Leadership expects delivery on the original roadmap. Your team is already at capacity, and two of the open reqs were for a critical new initiative that starts next quarter.",
    "topicsActivated": [
      "Resource Allocation (Constraint-Based Planning)",
      "Strategic Prioritization (Ruthless Triage)",
      "Stakeholder Mgmt (Managing Up, Resetting Expectations)",
      "Delivery Management (Doing More With Less)"
    ],
    "decisionFramework": "1. Immediate: map every commitment to required capacity (people-weeks). Total it up. Compare to available capacity without the 4 missing hires. Quantify the gap precisely (e.g., 'we're 35% short of committed capacity'). 2. Tier your commitments: Tier 1 (must deliver — revenue, contractual, compliance), Tier 2 (should deliver — strategic, high-value), Tier 3 (could deliver — quality-of-life, nice-to-have). 3. Present the gap to leadership with options: (A) Cut Tier 3 entirely and reduce Tier 2 scope — deliver Tier 1 on time. (B) Delay Tier 2 to Q1 next year — deliver Tier 1 + reduced Tier 3. (C) Deliver everything with explicit quality/reliability trade-offs (not recommended). 4. Force the prioritization decision upward — do not silently absorb an impossible scope. 5. Communicate to team: 'here's what changed, here's what we're focusing on, here's what we're NOT doing.' 6. Explore creative solutions: internal transfers, contractor budget (if not frozen), automation of manual work, scope reduction on individual projects.",
    "commonMistakes": "Silently accepting the original commitments and burning out your team trying to deliver. Not quantifying the capacity gap with data (just saying 'we can't do it all'). Spreading the team thin across everything instead of cutting scope decisively. Waiting until Q3 delivery misses to raise the alarm. Not exploring creative alternatives (internal mobility, automation, scope negotiation). Complaining to your team about leadership instead of actively managing upward.",
    "whatGoodLooksLike": "Capacity gap quantified and presented to leadership within 1 week of freeze announcement. Prioritization framework forces explicit trade-off decisions at the leadership level. Team has a clear, achievable plan — they know what they're doing and what they're NOT doing. Creative alternatives explored and implemented where possible. Tier 1 commitments delivered on time. Leadership sees you as someone who manages constraints proactively, not someone who either over-promises or just complains. Team morale stays stable because expectations are realistic. Industry reference: Netflix's 'highly aligned, loosely coupled' model lets teams self-prioritize within strategic constraints during budget freezes. Amazon's weekly business reviews force explicit scope-to-capacity matching when resources contract. Calibration signal: \"Reprioritization communication: 'When 3 engineers were pulled for incident response, immediately published updated ti...\" (SIG-274).",
    "mappingNotes": "Budget freeze with unchanged commitments — constraint-based planning",
    "suggestedMetricIds": [
      "8.4",
      "8.3",
      "2.5",
      "1.2"
    ]
  },
  {
    "id": "P-C10-3",
    "slug": "leading-a-cost-optimization-initiative-finops",
    "observableIds": [
      "C10-O3",
      "C3-O9"
    ],
    "capabilityIds": [
      "C10",
      "C3"
    ],
    "title": "Leading a Cost Optimization Initiative (FinOps)",
    "context": "CFO mandates 30% cloud cost reduction across engineering. You're asked to lead the initiative for your org (4 teams, ~40 engineers). Current monthly spend is $2M across AWS services. No one has a clear picture of what's driving costs. Teams have never been held accountable for infrastructure spend. Finance wants a plan in 2 weeks and results within the quarter.",
    "topicsActivated": [
      "Financial Management (Cloud Cost Optimization, FinOps)",
      "Technical Strategy (Infrastructure Efficiency)",
      "Cross-Functional Partnership (Finance/Engineering Alignment)",
      "Stakeholder Mgmt (Driving Org-Wide Initiative)"
    ],
    "decisionFramework": "1. Week 1 — Visibility: Get cost breakdown by team, service, and environment. Tag everything. Identify top 10 cost drivers (usually 80/20 rule applies — a few services drive most spend). Distinguish production from non-production costs. 2. Week 2 — Quick wins: Identify low-hanging fruit (unused resources, oversized instances, non-prod environments running 24/7, unattached EBS volumes, idle load balancers). These often yield 10-15% savings with minimal effort. 3. Week 2 — Plan: Present tiered savings plan: Tier 1 (quick wins, 10-15%, 2 weeks), Tier 2 (right-sizing and reserved instances, 10-15%, 6-8 weeks), Tier 3 (architectural changes — caching, serverless migration, data tiering, 5-10%, 1-2 quarters). 4. Execution: Assign cost owners per team. Create cost dashboards visible to all engineers. Set up weekly cost review cadence. Implement automated alerts for spend anomalies. 5. Sustainability: Bake cost awareness into architecture reviews and PR reviews. Make cloud cost a standing item in team retrospectives. Celebrate wins publicly. 6. Report to CFO monthly with progress against target, broken down by initiative.",
    "commonMistakes": "Promising 30% savings purely from quick wins (architectural changes are needed for the last 10-15%). Not tagging resources properly (you can't optimize what you can't measure). Making engineers feel punished for using cloud resources (kills innovation). Cutting non-prod environments that teams actually need for testing. Not setting up sustainable practices (costs creep back within 6 months). Treating it as a one-time project instead of an ongoing discipline. Over-optimizing and causing production reliability issues (saving $50K/month but causing outages that cost more).",
    "whatGoodLooksLike": "Full cost visibility achieved within 2 weeks. Quick wins (10-15%) delivered within first month. 30% target achieved within the quarter through combination of quick wins, right-sizing, and architectural improvements. Cost dashboards used by engineers proactively (not just finance). FinOps practices embedded in team culture — cost is considered in architecture decisions going forward. No production reliability degradation. CFO sees engineering as a partner in financial discipline, not a cost center to be squeezed. Industry reference: Amazon's FinOps practice requires every team to track cost-per-transaction and present monthly cost attribution. Google Cloud's internal cost allocation model tags every resource to a team and product, making waste visible. Calibration signal: \"Quantitative rigor: 'TCO analysis demonstrated buying [X] saves $Ym over 3 years vs\" (SIG-128).",
    "mappingNotes": "Cloud cost optimization and FinOps leadership",
    "suggestedMetricIds": [
      "10.1",
      "10.2",
      "8.4"
    ]
  },
  {
    "id": "P-C11-2",
    "slug": "managing-through-a-hiring-freeze",
    "observableIds": [
      "C11-O1",
      "C11-O5",
      "C6-O4"
    ],
    "capabilityIds": [
      "C11",
      "C6"
    ],
    "title": "Managing Through a Hiring Freeze",
    "context": "6-month hiring freeze announced company-wide. You have 3 open reqs mid-pipeline — one candidate just passed the onsite, another is in final round, and a third is at phone screen stage. Your team of 9 is already stretched across 3 projects. Two senior engineers are showing signs of burnout. One mid-level engineer recently mentioned they've been getting recruiter outreach. Attrition risk is real and you can't backfill.",
    "topicsActivated": [
      "Talent Acquisition (Pipeline Management During Freeze)",
      "Team Health (Retention, Burnout Prevention)",
      "Performance (Capacity Planning)",
      "Stakeholder Mgmt (Priority Negotiation)"
    ],
    "decisionFramework": "1. Candidates in pipeline: Fight for exceptions for the candidate who passed onsite — they're the hardest to re-recruit later. Present the cost of losing them (recruiting fees, ramp time, offer expiration). For earlier-stage candidates: communicate honestly and keep them warm if possible. 2. Scope reduction: Immediately re-prioritize. With 9 people instead of planned 12, you cannot run 3 full projects. Present leadership with options: (A) 2 projects at full speed + 1 paused, (B) 3 projects at reduced scope, (C) rank order and leadership decides. 3. Retention: Individual conversations with each team member within 1 week. Understand their concerns. Invest in what you CAN offer — stretch assignments, learning opportunities, conference talks, visibility projects, flexible work arrangements. 4. Burnout mitigation: Reduce meeting load, protect focus time, be explicit about what's NOT getting done. No hero culture. 5. Internal mobility: Can you borrow people from teams with lower priority work? Propose temporary reallocation. 6. Plan for freeze end: Keep job descriptions updated, maintain recruiter relationships, have a priority order for reqs when hiring resumes.",
    "commonMistakes": "Accepting the freeze without fighting for in-pipeline candidates (especially post-onsite — you'll never get them back). Not adjusting scope to match reduced team (expecting 9 people to do the work of 12). Ignoring retention risk because 'where would they go in this market?' (good engineers always have options). Canceling all development investment (training, conferences, hackathons) — this is exactly when you need retention tools most. Burning out your best people by giving them the extra work. Not planning for freeze end — scrambling when hiring reopens wastes months.",
    "whatGoodLooksLike": "Exception secured for post-onsite candidate (or at least a genuine attempt with VP-level advocacy). Scope explicitly reduced and communicated to stakeholders — no silent over-commitment. Every team member has had a 1:1 within the first week addressing their concerns and growth path. Burnout signals monitored actively — workload redistributed, low-priority work paused. Zero regrettable attrition during freeze. When hiring reopens: reqs are ready, pipeline is warm, team hits the ground running. Industry reference: Google maintains hiring committee relationships and pipeline data even during freezes, enabling rapid restart when hiring reopens. Amazon's Bar Raiser network preserves interviewer calibration across hiring cycles. Calibration signal: \"Director-level skill: 'Secured [X] headcount by demonstrating [Y] revenue impact / [Z] risk if unfilled'\" (SIG-068).",
    "mappingNotes": "Hiring freeze management and retention during constraints",
    "suggestedMetricIds": [
      "7.3",
      "7.5",
      "5.1",
      "7.9"
    ]
  },
  {
    "id": "P-C3-3",
    "slug": "your-product-is-being-sunset-deprecated",
    "observableIds": [
      "C3-O6",
      "C1-O15",
      "C7-O4"
    ],
    "capabilityIds": [
      "C3",
      "C1",
      "C7"
    ],
    "title": "Your Product Is Being Sunset/Deprecated",
    "context": "Leadership decides to sunset your team's product — a service with 50K active users. Revenue doesn't justify continued investment. Users need to be migrated to an alternative (internal or third-party). Timeline is 9 months. Your team of 10 engineers built this product over 3 years. They're anxious about their futures — will they be laid off, reassigned, or forgotten?",
    "topicsActivated": [
      "Technical Strategy (Deprecation Planning, User Migration)",
      "Org Design (Team Dissolution/Reassignment)",
      "Team Health (Morale, Career Anxiety)",
      "Stakeholder Mgmt (Communicating Difficult Decisions)",
      "Change Management (Product Lifecycle End)"
    ],
    "decisionFramework": "1. People first (Day 1-3): Before talking about migration plans, address your team's concerns. Be transparent about what you know and don't know. Commit to advocating for every team member's next role. If you have guarantees about no layoffs, say so. If you don't, be honest about the uncertainty. 2. Migration planning (Week 1-2): Assess user segmentation — which users migrate to what alternative? Create migration tiers: self-serve users (automated migration tools), mid-tier (guided migration with docs), enterprise/high-touch (white-glove migration support). 3. Communication plan (Week 2): Draft user communication in partnership with PM and marketing. Give users maximum notice. Provide clear timelines, migration guides, and support channels. 4. Technical execution (Month 1-8): Build migration tooling, maintain product stability during wind-down (no new features, but fix critical bugs and security issues), decommission infrastructure in phases. 5. Team placement (Month 1-6): Work with your manager and HR to identify landing spots for every team member. Match skills and interests to open roles. Start conversations early — don't wait until month 8. 6. Knowledge transfer (Month 6-9): Document institutional knowledge, transfer ownership of any shared components, archive code and documentation. 7. Sunset (Month 9): Final user cutover, infrastructure decommission, retrospective on the product lifecycle.",
    "commonMistakes": "Not addressing team anxiety immediately (people start job searching externally on Day 1 if they feel uncertain). Treating migration as an afterthought (50K users migrating poorly creates customer support nightmares and brand damage). Letting product quality degrade during wind-down (users still depend on it until they're migrated). Not fighting for your team members' placement (they remember who advocated for them). Waiting until month 7 to start placing people (best roles are taken). Pretending the product was a failure (it served users for 3 years — honor that). Not doing a retrospective (valuable lessons about product-market fit, technical decisions, and lifecycle management).",
    "whatGoodLooksLike": "Every team member knows their next role by month 6 — zero involuntary departures. User migration is smooth: 90%+ migrated with minimal support tickets. Product quality maintained until final cutover — no embarrassing outages during sunset. User communication is transparent and empathetic — NPS impact minimized. Team morale stays functional throughout (it won't be great, but it stays professional). Retrospective captures genuine lessons learned. Team members look back and say 'that was hard, but it was handled well.' Industry reference: Amazon's Working Backwards process (starting with the customer PR/FAQ) provides a framework for communicating product sunset with empathy. Google's deprecation policy mandates minimum notice periods and migration support for internal customers. Calibration signal: \"How you wind down reveals leadership character\" (SIG-144).",
    "mappingNotes": "Product sunset, user migration, and team reassignment",
    "suggestedMetricIds": [
      "8.2",
      "8.4",
      "1.2"
    ]
  },
  {
    "id": "P-C2-1",
    "slug": "quarterly-planning-product-engineering-disagree",
    "observableIds": [
      "C2-O1",
      "C2-O2"
    ],
    "capabilityIds": [
      "C2",
      "C5"
    ],
    "title": "Quarterly Planning When Product and Engineering Disagree on Priorities",
    "context": "It's planning season. Product wants to invest heavily in new features for a key customer segment. Engineering wants to pay down critical tech debt that's causing weekly incidents. Both sides have compelling data. You need to facilitate alignment.",
    "topicsActivated": [
      "Strategic Prioritization (Investment Balance)",
      "Cross-Functional Influence (PM Partnership)",
      "Decision Framing (Trade-off Communication)"
    ],
    "decisionFramework": "1. Quantify both sides: feature revenue impact vs. incident cost (engineer time, customer impact, pager fatigue). 2. Find the false dichotomy: can scope be split? Can debt be paid incrementally alongside features? 3. Use a capacity allocation framework (e.g., 70/20/10 — see P-C3-4 for detailed implementation): 70% features, 20% debt, 10% exploration — adjust ratios with data. 4. Create a shared artifact (investment portfolio view) both sides can see. 5. Agree on metrics to evaluate next quarter whether the split was right. 6. Escalate only if you've genuinely tried to align and can articulate the trade-off crisply.",
    "commonMistakes": "Picking a side instead of facilitating trade-off analysis. Using vague 'tech debt' framing without specific incidents/costs. Letting loudest voice win. Deferring entirely to product ('they own the roadmap'). Committing to both without acknowledging capacity constraints.",
    "whatGoodLooksLike": "Shared investment portfolio document with clear ratios. Both product and engineering feel heard. Trade-offs are explicit and documented. Success metrics defined for both feature and debt investments. Re-evaluation cadence agreed upon. Industry reference: Amazon's 6-pager narrative format forces explicit trade-off documentation between competing investments. Google's OKR framework requires measurable outcomes for both feature and infrastructure investments, preventing either from being 'just trust us.' Calibration signal: \"OKR quality: 'Redesigned OKRs from task lists to measurable outcomes — team co-creates goals, 85% KR achievement rate...\" (SIG-251).",
    "mappingNotes": "Strategic prioritization with cross-functional tension",
    "suggestedMetricIds": [
      "8.4",
      "8.1",
      "8.3",
      "1.2"
    ]
  },
  {
    "id": "P-C2-2",
    "slug": "asked-to-build-conflicting-with-okrs",
    "observableIds": [
      "C2-O2",
      "C2-O3"
    ],
    "capabilityIds": [
      "C2",
      "C10"
    ],
    "title": "Asked to Build Something That Conflicts with Your OKRs",
    "context": "A VP from another org asks your team to take on an urgent project that doesn't align with your quarterly OKRs. Your manager seems supportive of helping. Taking this on means dropping or delaying a committed OKR.",
    "topicsActivated": [
      "Strategic Prioritization (Saying No)",
      "Resource Allocation (Scope Management)",
      "Stakeholder Mgmt (Managing Up and Across)"
    ],
    "decisionFramework": "1. Clarify the ask: scope, timeline, why your team specifically. 2. Map impact: what gets dropped or delayed if you say yes. 3. Present trade-off to your manager explicitly: 'We can do X, but Y slips by Z weeks.' 4. If manager says do both: push back with data on capacity. 5. If you take it on: renegotiate OKRs formally — don't silently absorb. 6. Document the decision and who made it.",
    "commonMistakes": "Silently absorbing the work and burning out the team. Saying yes without surfacing what gets dropped. Saying no without offering alternatives (different team, reduced scope, later timeline). Not looping in your manager before committing. Treating OKRs as immutable when the business context has changed.",
    "whatGoodLooksLike": "Trade-off conversation with manager within 24 hours. Clear written communication to requesting VP with options. OKRs formally adjusted if work is accepted. Team understands why priorities shifted. No hero-mode — scope is right-sized to capacity. Industry reference: Amazon's 'disagree and commit' principle combined with Type 1/Type 2 decision classification helps managers navigate conflicting priorities — reversible commitments can flex, irreversible ones need escalation. Calibration signal: \"The mark of a strong EM/Director: 'What you chose NOT to do, and why' — calibration values this over endless yeses\" (SIG-134).",
    "mappingNotes": "Priority conflict with cross-org pressure",
    "suggestedMetricIds": [
      "8.1",
      "8.4",
      "8.3"
    ]
  },
  {
    "id": "P-C4-1",
    "slug": "team-delivery-predictability-collapsed",
    "observableIds": [
      "C4-O1",
      "C4-O2",
      "C4-O8"
    ],
    "capabilityIds": [
      "C4",
      "C9"
    ],
    "title": "Your Team's Delivery Predictability Has Collapsed",
    "context": "Over the past two quarters, your team has missed 70% of committed sprint goals. Stakeholders are losing trust. The team feels demoralized and says estimates are impossible given constant interruptions and changing requirements.",
    "topicsActivated": [
      "Operational Leadership (Delivery Cadence)",
      "Metrics & Measurement (Predictability)",
      "Team Health (Morale)"
    ],
    "decisionFramework": "1. Diagnose: categorize why work was missed — scope creep, interrupts, underestimation, dependencies, or changing priorities? 2. Measure interrupt load: what percentage of sprint capacity goes to unplanned work? 3. Right-size commitments: commit to less, deliver consistently. 4. Protect the sprint: create an interrupt buffer (e.g., 20% unplanned capacity) or designate an interrupt handler rotation. 5. Shorten planning horizon if needed (2-week sprints → 1-week). 6. Rebuild trust: show 3 consecutive on-target deliveries before expanding scope.",
    "commonMistakes": "Adding more process (longer planning, more estimation ceremonies) without addressing root cause. Blaming the team for bad estimates when the real problem is scope creep. Committing to aggressive goals to 'make up' for missed ones. Not addressing the interrupt source (often another team or on-call). Tracking velocity as a performance metric instead of a planning tool.",
    "whatGoodLooksLike": "Root cause analysis completed within 1 week. Interrupt load measured and buffer created. Commitments reduced to 70% of historical capacity. 3 consecutive sprints hitting >80% of commitments. Stakeholders see improving trend and regain trust. Team morale improves as they start hitting goals. Industry reference: Spotify's squad Health Check model provides a structured diagnostic for delivery predictability issues. Google's DORA research identifies four key metrics that, when tracked together, prevent optimizing one dimension at the expense of others. Calibration signal: \"EMs define team's communication operating system\" (SIG-060).",
    "mappingNotes": "Delivery predictability recovery",
    "suggestedMetricIds": [
      "2.5",
      "1.2",
      "2.6",
      "1.4",
      "5.1"
    ]
  },
  {
    "id": "P-C4-2",
    "slug": "inherit-team-no-operating-cadence",
    "observableIds": [
      "C4-O1",
      "C4-O3",
      "C4-O9"
    ],
    "capabilityIds": [
      "C4"
    ],
    "title": "You Inherit a Team with No Operating Cadence",
    "context": "You've just taken over a team that has no regular standup, no sprint planning, no retrospectives, and no documented on-call process. Work gets done through Slack threads and ad-hoc requests. The team is productive but chaotic, and knowledge is siloed.",
    "topicsActivated": [
      "Operational Leadership (Cadence Design)",
      "Developer Experience (Process)",
      "Culture (Norms)"
    ],
    "decisionFramework": "1. Observe first: spend 2 weeks understanding how work actually flows before changing anything. 2. Identify the biggest pain point (not your biggest concern — theirs). 3. Introduce ONE ceremony at a time. Start with a weekly sync — lowest friction, highest visibility. 4. Add planning/retro after the team sees value in the sync. 5. Document decisions and on-call in a shared runbook. 6. Don't over-process: match cadence to team size and maturity. A 4-person team doesn't need SAFe.",
    "commonMistakes": "Introducing 5 new meetings in week 1. Copying your previous team's exact process. Not asking the team what's actually painful. Making process compliance the goal instead of outcomes. Assuming chaos means dysfunction — sometimes small teams work fine with minimal process.",
    "whatGoodLooksLike": "2-week observation period before changes. Team co-designs the operating cadence. One ceremony introduced per 2-week cycle. Written runbook for on-call within 30 days. Team reports less chaos and fewer knowledge silos within 60 days. Process is lightweight and valued, not resented. Industry reference: Spotify introduces operating cadences incrementally — starting with a Health Check before adding ceremonies. Amazon's operational excellence model focuses on mechanisms (repeatable processes) rather than good intentions. Calibration signal: \"EMs define team's communication operating system\" (SIG-060).",
    "mappingNotes": "Operating cadence bootstrapping",
    "suggestedMetricIds": [
      "1.2",
      "1.4",
      "2.5",
      "5.1"
    ]
  },
  {
    "id": "P-C12-3",
    "slug": "your-team-culture-is-deteriorating-after-rapid-growth",
    "observableIds": [
      "C12-O1",
      "C12-O2",
      "C12-O6"
    ],
    "capabilityIds": [
      "C12"
    ],
    "title": "Your Team Culture Is Deteriorating After Rapid Growth",
    "context": "Your team doubled from 6 to 12 in 3 months. Skip-level feedback reveals new hires feel excluded, original team members feel their culture is gone, and collaboration norms are breaking down.",
    "topicsActivated": [
      "Team Health (Culture Continuity)",
      "Onboarding (Cultural Integration)",
      "Communication (Scaling Norms)"
    ],
    "decisionFramework": "1. Diagnose: run anonymous team health survey to quantify the problem. 2. Acknowledge: in team meeting, name the challenge — growth is hard, culture erosion is natural, and you're going to address it intentionally. 3. Rebuild: facilitate team charter session with ALL members (not just originals) to co-create norms. 4. Pair: create culture buddy system pairing new hires with veterans. 5. Formalize: document onboarding culture guide. 6. Monitor: monthly culture pulse checks for 2 quarters. 7. Adjust: iterate on norms based on feedback.",
    "commonMistakes": "Assuming culture will 'rub off' naturally. Only listening to original team members. Treating it as a one-time off-site exercise. Not documenting norms (they live in people's heads). Blaming new hires for 'not getting it'. Trying to preserve old culture exactly instead of evolving it.",
    "whatGoodLooksLike": "Co-created team charter within 4 weeks of recognizing the problem. New and tenured members both feel ownership of team norms. Culture buddy program running. Monthly pulse checks showing improvement within 2 months. New hires report feeling integrated within 60 days. Industry reference: Spotify uses squad Health Checks to make cultural dimensions explicit and discussable, enabling rapid intervention when scores decline. Calibration signal: \"EMs who avoid conflict are a red flag in calibration\" (SIG-044).",
    "mappingNotes": "Culture deterioration during growth scenario",
    "suggestedMetricIds": [
      "7.1",
      "7.2",
      "5.1"
    ]
  },
  {
    "id": "P-C12-4",
    "slug": "addressing-a-toxic-team-member-senior-engineer-others-avoid",
    "observableIds": [
      "C12-O1",
      "C12-O8"
    ],
    "capabilityIds": [
      "C12",
      "C6"
    ],
    "title": "Addressing a Toxic Senior Engineer the Team Works Around",
    "context": "Your most technically skilled senior engineer is dismissive in code reviews, dominates discussions, and makes junior engineers afraid to speak up. The team has learned to 'work around' this person, but you're losing good people.",
    "topicsActivated": [
      "Team Health (Psychological Safety)",
      "Performance (Behavioral Standards)",
      "Culture (Norm Enforcement)"
    ],
    "decisionFramework": "1. Document: collect specific behavioral examples with dates and impact (code review comments, meeting behaviors, peer feedback). 2. Private 1:1: deliver feedback using SBI framework — specific behavior, its impact on team and business, expected change. 3. Set timeline: clear behavioral expectations with 30-day checkpoint. 4. Support: provide coaching or resources (communication training, mentoring). 5. Monitor: check with team members (without naming) about improvement. 6. Follow through: if no improvement at 30 days, escalate to formal performance process. 7. Communicate: when behavior improves, acknowledge it; if person exits, address team about culture standards.",
    "commonMistakes": "Avoiding the conversation because of the person's technical value. Giving vague feedback like 'be nicer'. Addressing it publicly instead of privately first. Not documenting specific examples. Treating it as a personality issue rather than behavioral impact. Waiting for an HR complaint instead of acting proactively.",
    "whatGoodLooksLike": "Feedback delivered within 1 week of recognizing the pattern. Specific behavioral examples cited. Clear improvement plan with timeline. Psychological safety survey administered before and 30 days after intervention. Outcome: either behavioral improvement or managed exit. Team knows culture standards are enforced regardless of seniority. Industry reference: Netflix's culture memo explicitly states that brilliant jerks are a net negative — technical skill doesn't exempt anyone from behavioral standards. Calibration signal: \"EMs who avoid conflict are a red flag in calibration\" (SIG-044).",
    "mappingNotes": "Toxic senior engineer scenario",
    "suggestedMetricIds": [
      "7.1",
      "5.1",
      "7.2"
    ]
  },
  {
    "id": "P-C10-4",
    "slug": "asked-to-do-more-with-fewer-people-after-layoffs",
    "observableIds": [
      "C10-O2",
      "C10-O6",
      "C10-O8"
    ],
    "capabilityIds": [
      "C10"
    ],
    "title": "Asked to Do More With Fewer People After Layoffs",
    "context": "After a round of layoffs, your team lost 3 of 10 engineers. Leadership still expects the same deliverables. Team morale is low, survivors are anxious, and you need to figure out what's actually possible.",
    "topicsActivated": [
      "Resource Allocation (Capacity Planning)",
      "Stakeholder Mgmt (Expectation Setting)",
      "Team Health (Post-Layoff Recovery)"
    ],
    "decisionFramework": "1. Acknowledge first, plan second (Week 1): Before any roadmap discussion, hold honest conversations with your team. Name what happened, honor the people who left, and address survivor anxiety directly. 'We lost good people, and this affects us all. Here's what I know, here's what I don't.' 2. Assess capacity and emotional readiness (Week 1-2): Map remaining capacity honestly, but also assess team morale. A team in shock can't deliver at 100% of mathematical capacity — plan for 70-80% utilization in the first quarter. 3. Present tiered options to leadership (Week 2-3): Go to leadership with clear trade-offs — (A) what's deliverable with current team at sustainable pace, (B) what requires timeline extensions, (C) what needs to stop entirely. Frame this as protecting the investment in the remaining team, not complaining about headcount. 4. Rebuild team identity (Month 1-2): The team's identity has been disrupted. Redistribute ownership thoughtfully, create new rituals, and invest in the remaining team's growth to signal commitment. 5. Monitor for delayed attrition (Ongoing): The biggest risk post-layoffs is losing people you intended to keep. Increase 1:1 frequency, watch for disengagement signals, and proactively discuss career growth with top performers.",
    "commonMistakes": "Immediately jumping to roadmap replanning without acknowledging the emotional impact. Treating this as a purely resource-allocation problem when it's also a trust and morale crisis. Being vague about what stops — 'we'll try our best' is not a plan. Not renegotiating timelines with external stakeholders. Burning out remaining team trying to prove the layoffs weren't needed. Ignoring survivor guilt and anxiety in skip-level conversations.",
    "whatGoodLooksLike": "Honest capacity assessment within 2 weeks. Tiered options presented to leadership with quantified trade-offs. At least 30% of previous scope explicitly deprioritized or stopped. Team morale stabilized within 6 weeks (measured). No additional regrettable departures in the quarter following layoffs. Industry reference: Shopify's 2023 post-layoff playbook focused on radical scope reduction rather than expecting remaining team to absorb the load — leadership explicitly killed projects proportional to headcount reduction. Calibration signal: \"Reprioritization communication: 'When 3 engineers were pulled for incident response, immediately published updated ti...\" (SIG-274).",
    "mappingNotes": "Post-layoff resource management scenario",
    "suggestedMetricIds": [
      "7.1",
      "5.1",
      "1.2"
    ]
  },
  {
    "id": "P-C7-2",
    "slug": "communicating-a-major-technical-decision-that-not-everyone-agrees-with",
    "observableIds": [
      "C7-O1",
      "C7-O6",
      "C7-O8"
    ],
    "capabilityIds": [
      "C7"
    ],
    "title": "Communicating a Major Technical Decision Not Everyone Agrees With",
    "context": "After an RFC process, you've decided to migrate from microservices back to a modular monolith. Senior engineers are split — some are excited, others feel their microservices expertise is being devalued. You need to communicate the decision without creating factions.",
    "topicsActivated": [
      "Decision Framing (Controversial Decisions)",
      "Communication (Technical Audience)",
      "Culture (Constructive Disagreement)"
    ],
    "decisionFramework": "1. Document the decision fully: what was decided, why, what alternatives were considered and why they were rejected. 2. Acknowledge dissent: name the strongest counter-arguments and explain specifically why you decided differently. 3. Communicate in writing first: publish the decision doc before the meeting so people can process it. 4. Hold a Q&A: let people ask questions and express concerns without relitigating. 5. Commit explicitly: 'This is the decision. If you disagree, I respect that, and I need you to commit to execution.' 6. Follow up individually: talk to the loudest dissenters 1:1 to address concerns and check for commitment.",
    "commonMistakes": "Announcing without explaining the reasoning. Dismissing disagreement as 'not understanding'. Not documenting the alternative options considered. Relitigating the decision in every meeting. Making it personal — 'I decided' vs. 'we evaluated'. Not following up with dissenters who might sabotage silently.",
    "whatGoodLooksLike": "Decision doc published with alternatives analysis. Team can articulate the reasoning even if they disagreed. Execution begins within 2 weeks. No underground resistance. Dissenters feel heard even though they didn't 'win'. Team retrospective at 90 days evaluates whether the decision is working. Industry reference: Amazon's 'disagree and commit' principle provides a clear framework — voice disagreement during the decision process, commit fully once the decision is made. Calibration signal: \"Judgment signal: 'Identified [X] issues in first month, addressed critical ones immediately, built 90-day plan for sy...\" (SIG-064).",
    "mappingNotes": "Controversial technical decision communication scenario",
    "suggestedMetricIds": [
      "1.2",
      "8.4"
    ]
  },
  {
    "id": "P-C7-3",
    "slug": "your-team-has-communication-debt-decisions-not-documented",
    "observableIds": [
      "C7-O6",
      "C7-O9"
    ],
    "capabilityIds": [
      "C7",
      "C4"
    ],
    "title": "Your Team Has Communication Debt — Decisions Aren't Documented",
    "context": "Your team has grown from 5 to 15 people. Key technical decisions live in Slack threads, hallway conversations, and individual memories. New hires can't find context. The same decisions are relitigated monthly. You're drowning in 'quick syncs'.",
    "topicsActivated": [
      "Communication (Documentation Practices)",
      "Decision Making (Process Design)",
      "Operational Rhythm (Scaling)"
    ],
    "decisionFramework": "1. Audit: List the top 10 decisions made in the last quarter — can you find the documentation? If not, you have communication debt. 2. Triage: retroactively document the most impactful undocumented decisions (tech choices, process decisions, ownership assignments). 3. Introduce lightweight RFC process: not bureaucratic — a simple template for decisions that affect >2 people. 4. Create decision log: a single page listing recent decisions with links to context. 5. Establish norms: 'If it was decided in Slack, it didn't happen until it's in the decision log.' 6. Review: monthly audit of decision documentation completeness.",
    "commonMistakes": "Making the RFC process too heavy (kills adoption). Not retroactively documenting the most critical past decisions. Expecting the team to change overnight (habits take weeks). Documenting everything (leads to documentation fatigue). Not having a single searchable location for decisions. Confusing meeting notes with decision documentation.",
    "whatGoodLooksLike": "Within 6 weeks: lightweight RFC template adopted, decision log created and maintained, new hires can find context for major decisions without asking. Within 3 months: same-decision relitigations drop measurably. Communication overhead per person decreases as team scales. Industry reference: Spotify's decision log (DACI-based) ensures every significant decision has a single documented source of truth accessible to the entire squad and stakeholders. Calibration signal: \"Engineering maturity signal: 'Established RFC culture, [X] design docs/quarter, referenced in [Y] onboarding sessions'\" (SIG-132).",
    "mappingNotes": "Communication debt and documentation scaling scenario",
    "suggestedMetricIds": [
      "2.3",
      "2.5"
    ]
  },
  {
    "id": "P-C14-3",
    "slug": "high-performer-wants-promotion-but-isnt-ready",
    "observableIds": [
      "C14-O5",
      "C14-O8"
    ],
    "capabilityIds": [
      "C14",
      "C6"
    ],
    "title": "High Performer Wants Promotion But Isn't Ready",
    "context": "Your best engineer is pushing hard for promotion to Senior. They're technically excellent and deliver consistently, but they lack the influence, mentoring, and scope leadership expected at the next level. They're getting impatient and you're worried about retention.",
    "topicsActivated": [
      "Performance (Promotion Readiness)",
      "Coaching (Development Planning)",
      "Retention (Managing Expectations)"
    ],
    "decisionFramework": "1. Validate: Review the level expectations document and honestly assess where the engineer is vs. where they need to be. 2. Prepare the conversation: document specific gaps with examples (not 'you need more leadership' but 'at Senior level, you'd need to have led a cross-team initiative end-to-end'). 3. Deliver with care: acknowledge their strengths, name the specific gaps, and frame it as 'here's what we're building toward' not 'you're not good enough'. 4. Co-create a plan: identify 2-3 specific stretch opportunities over the next 6 months that address the gaps. 5. Commit to support: regular check-ins on progress, sponsor for the right opportunities, provide visibility. 6. Manage retention: separate promo from retention — explore comp adjustment, scope expansion, or recognition to address flight risk without premature promotion.",
    "commonMistakes": "Promoting to retain (creates level inflation and sets them up to fail). Vague feedback like 'you need more scope'. Not separating retention from promotion — they're different problems. Saying 'maybe next cycle' without a concrete plan. Comparing them to others who were promoted. Not sponsoring opportunities that would fill the gaps. Over-promising timeline for promotion.",
    "whatGoodLooksLike": "Specific gap analysis delivered within 2 weeks. 6-month development plan with concrete milestones co-created. At least one stretch assignment initiated within 30 days. Monthly check-ins on progress. Engineer feels invested in (not rejected). If comp is the real issue, address it separately. Industry reference: Google's promo committee model requires sustained next-level evidence across multiple review periods — managers coach to the bar rather than lowering it, and use 'development partnership' framing to retain ambitious engineers. Calibration signal: \"Manager maintains a running promotion document for each promotion-track engineer with gap analysis updated monthly — ...\" (SIG-295).",
    "mappingNotes": "Promotion readiness gap management scenario",
    "suggestedMetricIds": [
      "5.1",
      "7.1"
    ]
  },
  {
    "id": "P-C11-3",
    "slug": "your-interview-process-is-rejecting-good-candidates",
    "observableIds": [
      "C11-O1",
      "C11-O9"
    ],
    "capabilityIds": [
      "C11"
    ],
    "title": "Your Interview Process Is Rejecting Good Candidates",
    "context": "Your team has been hiring for 4 months with zero offers extended. Recruiting says candidate quality is high, but your interviewers keep saying 'not a strong enough signal.' You suspect the interview process itself is the problem.",
    "topicsActivated": [
      "Hiring (Interview Process Design)",
      "Metrics (Conversion Rates)",
      "Culture (Bias in Assessment)"
    ],
    "decisionFramework": "1. Diagnose (Week 1): Pull conversion rates by interview stage. Where's the biggest drop-off? Is it consistent across interviewers? 2. Audit (Week 1-2): Review recent interview scorecards. Are scores specific (behavioral evidence) or vague ('didn't seem senior enough')? Are rubrics being used? Is scoring happening before debrief? 3. Calibrate (Week 2-3): Run a calibration session with all interviewers using a mock candidate. Identify scoring discrepancies. 4. Fix (Week 3-4): Revise rubrics with specific behavioral anchors. Retrain interviewers. Implement independent scoring before debrief. 5. Monitor (Ongoing): Track conversion rates weekly, interviewer agreement rates, and candidate feedback scores.",
    "commonMistakes": "Blaming recruiting for candidate quality without examining your interview process. Using vague rubrics that different interviewers interpret differently. Not collecting candidate feedback on the interview experience. Lowering the bar instead of fixing the process. Assuming the process works because some good candidates get through.",
    "whatGoodLooksLike": "Root cause identified within 2 weeks. Rubrics revised with specific behavioral anchors. Interviewer calibration complete. Conversion rate improving within 6 weeks. Hire within 8 weeks of fix. Candidate experience scores improving. Industry reference: Meta's structured interviewing program requires predefined rubrics with behavioral anchors and independent scoring, achieving consistent evaluation across thousands of interviews per month. Calibration signal: \"Director-level skill: 'Secured [X] headcount by demonstrating [Y] revenue impact / [Z] risk if unfilled'\" (SIG-068).",
    "mappingNotes": "Broken interview process diagnosis scenario",
    "suggestedMetricIds": [
      "7.3"
    ]
  },
  {
    "id": "P-C4-3",
    "slug": "your-team-is-agile-in-name-only",
    "observableIds": [
      "C4-O1",
      "C4-O3"
    ],
    "capabilityIds": [
      "C4"
    ],
    "title": "Your Team Is Agile in Name Only",
    "context": "Your team runs 'sprints' but nothing is really iterative. Sprint planning is task assignment by the EM. Retros generate action items nobody completes. Standup is a status report to you. The team jokes about 'waterfall with standups.'",
    "topicsActivated": [
      "Process (Agile Implementation)",
      "Team Health (Ownership)",
      "Operational Rhythm (Ceremony Design)"
    ],
    "decisionFramework": "1. Honest audit (Week 1): For each ceremony, ask 'What decision did this change last month?' If nothing, it's cargo cult. 2. Strip to essentials (Week 2): Cancel all ceremonies except retro and planning. Let the team feel the difference. 3. Rebuild from need (Week 3-4): Ask the team 'What problems do we have that a recurring meeting could solve?' Reintroduce only ceremonies the team wants. 4. Fix ownership (Month 2): Shift standup from status-report-to-EM to team-coordination. Let the team run planning. Make retro action items the first item on next planning. 5. Measure (Ongoing): Track ceremony participation, action item completion, and team satisfaction with process.",
    "commonMistakes": "Adding more process to fix a process problem. Adopting SAFe/LeSS/Scrum-of-Scrums when basic Scrum isn't working. Making it about the framework instead of the outcomes. Not letting the team own their process. Running retros without ever completing the action items. Treating agile as a noun instead of an adjective.",
    "whatGoodLooksLike": "Within 6 weeks: team owns their ceremonies, retro action items >80% completion rate, standup is team coordination (not status report), sprint commitments are team-made (not EM-assigned). Team describes their process as 'what works for us' rather than 'what we're supposed to do.' Industry reference: Spotify evolved away from prescriptive Scrum because squads adapted processes to their actual needs — the best agile teams don't follow a framework, they continuously improve how they work. Calibration signal: \"EMs define team's communication operating system\" (SIG-060).",
    "mappingNotes": "Cargo cult agile diagnosis and fix scenario",
    "suggestedMetricIds": [
      "2.5",
      "2.4"
    ]
  },
  {
    "id": "P-C9-2",
    "slug": "leadership-wants-engineering-productivity-metrics-yesterday",
    "observableIds": [
      "C9-O1",
      "C9-O9",
      "C9-O10"
    ],
    "capabilityIds": [
      "C9"
    ],
    "title": "Leadership Wants Engineering Productivity Metrics Yesterday",
    "context": "Your VP asks for a 'developer productivity dashboard' by next month. They want to see engineering ROI. Your team currently has no metrics infrastructure. You know that naive productivity metrics can be destructive.",
    "topicsActivated": [
      "Metrics (Framework Selection)",
      "Stakeholder Mgmt (Managing Expectations)",
      "Team Health (Avoiding Surveillance)"
    ],
    "decisionFramework": "1. Understand the real need (Week 1): What decision is leadership trying to make? 'See engineering ROI' could mean many things. Ask: 'If you had this dashboard, what would you do differently?' 2. Propose a phased approach (Week 1): Phase 1 (Month 1): 3 core DORA metrics from existing tooling. Phase 2 (Month 2): Add developer satisfaction survey. Phase 3 (Month 3): Add business outcome connection (features → impact). 3. Set guardrails (Week 2): Metrics for team-level trends, not individual surveillance. Paired metrics (speed + quality). No metrics as performance targets. 4. Build Phase 1 (Month 1): Deploy basic DORA dashboard from CI/CD data. 5. Iterate (Ongoing): Add metrics only when previous ones drive decisions. Retire metrics that don't inform action.",
    "commonMistakes": "Giving leadership lines-of-code or tickets-closed metrics (gameable and misleading). Building a complex dashboard nobody uses. Using metrics for individual performance evaluation. Not asking what decisions the metrics should inform. Building everything at once instead of iterating. Not pairing speed metrics with quality metrics.",
    "whatGoodLooksLike": "Basic DORA dashboard live within 4 weeks. Leadership can see team-level trends. No individual developer surveillance. Metrics driving at least one decision per month. Developer satisfaction survey running by Month 2. Team trusts the metrics (not cynical about measurement). Industry reference: Microsoft's SPACE framework explicitly combines satisfaction, performance, activity, communication, and efficiency — preventing single-dimension measurement while giving leadership the multi-faceted view they need. Calibration signal: \"Calibration evidence: 'Reduced lead time from 5 days to 8 hours by [specific action: eliminated manual QA gate / para...\" (SIG-013).",
    "mappingNotes": "Engineering productivity metrics rollout scenario",
    "suggestedMetricIds": [
      "2.1",
      "2.2",
      "1.2"
    ]
  },
  {
    "id": "P-C5-6",
    "slug": "product-and-engineering-have-separate-roadmaps",
    "observableIds": [
      "C5-O1",
      "C5-O16"
    ],
    "capabilityIds": [
      "C5",
      "C2"
    ],
    "title": "Product and Engineering Have Separate Roadmaps",
    "context": "You discover that PM has a roadmap they share with stakeholders, and your engineering team has a separate technical roadmap. The two don't align. PM is frustrated that engineering is 'doing their own thing.' Engineering is frustrated that PM doesn't account for technical investment.",
    "topicsActivated": [
      "Cross-Functional Partnership (Alignment)",
      "Strategic Prioritization (Unified Planning)",
      "Communication (Shared Artifacts)"
    ],
    "decisionFramework": "1. Diagnose (Week 1): Compare both roadmaps. What's overlapping? What's unique to each? Where are the conflicts? 2. Align on reality (Week 2): Joint session with PM to reconcile. Present engineering investment needs (tech debt, platform, reliability) as business investments with expected ROI. 3. Unify (Week 3): Create a single roadmap with explicit capacity allocation — e.g., 60% product features, 20% tech debt/platform, 10% reliability, 10% experimentation. 4. Share (Week 4): Publish unified roadmap to all stakeholders. Both PM and Engineering present it together. 5. Sustain (Ongoing): Joint planning sessions quarterly. Weekly triad syncs. Any roadmap changes require mutual agreement.",
    "commonMistakes": "Engineering building their own roadmap in secret. PM not allocating any capacity for engineering-initiated work. Treating it as a power struggle instead of an alignment problem. Not quantifying the business impact of technical investment. Agreeing on a unified roadmap but reverting to separate ones within a quarter.",
    "whatGoodLooksLike": "Single unified roadmap within 4 weeks. Explicit capacity allocation visible to all stakeholders. Both PM and Engineering can articulate the full roadmap (not just their part). No more 'surprise' engineering work or 'surprise' PM commitments. Industry reference: Spotify's squad model requires squads to own both product and technical outcomes through shared OKRs, making separate roadmaps structurally impossible. Calibration signal: \"'Strong partner feedback from PM and Design' — explicit calibration dimension at most Big Tech companies\" (SIG-106).",
    "mappingNotes": "Separate roadmaps alignment scenario",
    "suggestedMetricIds": [
      "1.2",
      "8.4"
    ]
  },
  {
    "id": "P-C6-5",
    "slug": "your-new-engineering-manager-is-struggling-with-the-transition-from-ic",
    "observableIds": [
      "C6-O1",
      "C6-O13"
    ],
    "capabilityIds": [
      "C6"
    ],
    "title": "Your New Engineering Manager Is Struggling With the IC-to-EM Transition",
    "context": "You promoted your best senior engineer to EM 3 months ago. They're still writing code 50% of the time, 1:1s are technical problem-solving sessions, and skip-level feedback reveals reports feel unsupported. Your new EM is frustrated that 'management is keeping me from real work.'",
    "topicsActivated": [
      "Coaching (New Manager Development)",
      "Career Development (IC to EM Transition)",
      "Team Health (Report Experience)"
    ],
    "decisionFramework": "1. Validate (Week 1): Confirm the problem through skip-level feedback and 1:1 with the new EM. Understand their perspective without judgment. 2. Redefine success (Week 1-2): Have an explicit conversation about what 'good EM work' looks like. It's not coding. It's growing people, removing blockers, setting direction. 3. Create structure (Week 2-3): Set explicit time allocation targets (code: <10%, 1:1s: 25%, strategy/planning: 25%, cross-functional: 20%, admin: 20%). Pair them with an experienced EM mentor. 4. Build skills (Month 2): Coaching on feedback delivery, 1:1 structure, career conversations. Shadow their 1:1s (with consent) and provide feedback. 5. Check and adjust (Month 3): Review skip-level feedback. If improving, continue coaching. If not, have honest conversation about whether management is the right path. 6. Offer the exit ramp (If needed): Make return to IC genuinely safe and unstigmatized.",
    "commonMistakes": "Expecting the transition to happen naturally without coaching. Not reducing their IC responsibilities explicitly. Criticizing their management without teaching alternatives. Not offering a genuine return-to-IC path. Promoting again too soon (next person) without providing better transition support.",
    "whatGoodLooksLike": "Within 90 days: EM spending <20% time on code, 1:1s focused on career and growth, skip-level feedback improving. Within 6 months: EM has found their management identity, reports feel supported, EM is invested in management (not pining for IC work). Or: EM has returned to IC track gracefully, and someone better suited has taken the role. Both outcomes are wins. Industry reference: Google's Managing@Google program provides structured training for all new managers in their first 90 days, covering feedback, 1:1s, performance management, and career development. Calibration signal: \"Retention correlates with career development quality\" (SIG-080).",
    "mappingNotes": "IC-to-EM transition support scenario",
    "suggestedMetricIds": [
      "5.1",
      "7.1"
    ]
  },
  {
    "id": "P-C8-4",
    "slug": "building-incident-readiness-before-your-first-major-outage",
    "observableIds": [
      "C8-O1",
      "C8-O7"
    ],
    "capabilityIds": [
      "C8"
    ],
    "title": "Building Incident Readiness Before Your First Major Outage",
    "context": "Your team owns a growing service but has never had a serious incident. There are no runbooks, no defined on-call rotation, and no incident response process. You know it's a matter of when, not if.",
    "topicsActivated": [
      "Reliability (Incident Preparation)",
      "Operational Rhythm (On-Call Design)",
      "Risk Management (Pre-Incident Planning)"
    ],
    "decisionFramework": "1. Inventory (Week 1): List the 5 most likely failure modes for your service. For each, write a one-page playbook: what happens, how you detect it, how you mitigate it, who to notify. 2. Set up basics (Week 2): Define on-call rotation, escalation path, incident severity levels, and communication channels. 3. Practice (Month 2): Run a tabletop exercise — walk through a simulated incident using the playbooks. Identify gaps. 4. Graduate (Month 3): Run a controlled game day — actually inject a failure in staging, have the on-call respond using the playbook. 5. Improve (Ongoing): After each real incident, update playbooks. Review runbook accuracy quarterly. The process of building playbooks is itself a prevention tool — the conversations surface risks you would have missed.",
    "commonMistakes": "Trying to write runbooks for every possible failure (write for the 5 most likely first). Making on-call purely punitive instead of a learning rotation. Not practicing — having runbooks nobody has ever used. Writing runbooks at the wrong altitude (too abstract or too detailed). Not updating runbooks after incidents reveal gaps.",
    "whatGoodLooksLike": "Within 60 days: on-call rotation running, top 5 failure mode playbooks written, first tabletop exercise completed. Within 90 days: first game day run, runbook gaps identified and fixed. When the first real incident hits, the team responds with composure rather than panic because they've practiced. Yonatan Zunger's 'when, not if' framing: building playbooks is both practical preparation and psychological tool — planning for failure makes hard conversations easier and removes perverse incentives to hide risk.",
    "mappingNotes": "Pre-incident readiness building scenario",
    "suggestedMetricIds": [
      "3.3",
      "3.5"
    ]
  },
  {
    "id": "P-C8-5",
    "slug": "your-on-call-rotation-is-burning-people-out",
    "observableIds": [
      "C8-O2",
      "C8-O8"
    ],
    "capabilityIds": [
      "C8",
      "C4"
    ],
    "title": "Your On-Call Rotation Is Burning People Out",
    "context": "Your on-call engineers are getting paged 10+ times per week. The same alerts fire repeatedly. Morale is cratering. Two engineers have asked to be removed from the rotation. You're losing people to teams with better operational hygiene.",
    "topicsActivated": [
      "Reliability (Alert Quality)",
      "Team Health (On-Call Sustainability)",
      "Operational Rhythm (Toil Reduction)"
    ],
    "decisionFramework": "1. Quantify (Week 1): Pull alert data for the last 30 days — frequency, repeat offenders, pages that required action vs. noise, time to resolve. 2. Triage alerts (Week 1-2): Categorize every alert: actionable (keep), informational (convert to ticket), noise (delete), duplicate (consolidate). 3. Fix top offenders (Week 2-4): Take the top 5 most frequent alerts and either fix the underlying issue, tune the threshold, or convert to non-paging. 4. Redesign rotation (Month 2): Ensure on-call burden is shared equitably, build in compensatory time off, pair junior on-call with experienced backup. 5. Set SLOs (Month 2-3): Define what 'healthy on-call' looks like — target <2 actionable pages per shift, <30 min mean time to engage. 6. Sustain (Ongoing): Review alert quality monthly, track on-call happiness, treat recurring alerts as bugs to fix, not toil to endure.",
    "commonMistakes": "Adding more people to the rotation instead of fixing alert quality (spreads suffering, doesn't fix it). Accepting alert fatigue as 'just how on-call works'. Not tracking alert-to-action ratio. Removing people from rotation when they complain instead of fixing the root cause. Compensating with on-call bonus instead of reducing toil.",
    "whatGoodLooksLike": "Within 30 days: alert volume reduced 50%+ by eliminating noise and fixing top offenders. Within 60 days: on-call engineers report sustainable workload. On-call shift averages <2 actionable pages. Nobody requests removal from rotation. Team sees on-call as learning opportunity, not punishment. Liz Fong-Jones' principle: if you can't afford the engineering resources to do chaos engineering, you definitely can't afford to have unreliable systems — same applies to alert quality: if you can't invest in reducing toil, you can't afford the attrition it causes. Calibration signal: \"Error budget discipline: 'Implemented SLO-based error budgets — automatically triggered reliability sprint when check...\" (SIG-210).",
    "mappingNotes": "On-call burnout and alert fatigue scenario",
    "suggestedMetricIds": [
      "3.3",
      "7.1"
    ]
  },
  {
    "id": "P-C9-3",
    "slug": "your-metrics-are-being-gamed-and-everyone-knows-it",
    "observableIds": [
      "C9-O1",
      "C9-O9"
    ],
    "capabilityIds": [
      "C9"
    ],
    "title": "Your Metrics Are Being Gamed and Everyone Knows It",
    "context": "Leadership set a deployment frequency target. Your team is now deploying tiny changes to hit the number, but cycle time is actually worse and code review quality has dropped. Engineers are cynical about measurement.",
    "topicsActivated": [
      "Metrics (Gaming Prevention)",
      "Culture (Measurement Trust)",
      "Leadership (Incentive Design)"
    ],
    "decisionFramework": "1. Acknowledge (Week 1): Name the problem openly with leadership and the team. Gaming is a rational response to misaligned incentives — it's a system design failure, not a character failure. 2. Diagnose (Week 1-2): Identify which metrics became targets rather than diagnostics. For each, find the paired metric that would have caught the gaming (deployment frequency should be paired with change failure rate or PR size). 3. Redesign (Week 2-3): Replace single-metric targets with paired metrics. Present speed metrics alongside quality counterparts. Stop setting metrics as individual or team performance targets — use them as diagnostic tools only. 4. Rebuild trust (Month 2): Communicate the new approach: metrics exist to help the team improve, not to judge them. Involve engineers in choosing which metrics matter. 5. Monitor (Ongoing): Ask quarterly: 'Which metrics drove a real decision? Which are being ignored or gamed?' Retire any metric that isn't informing action.",
    "commonMistakes": "Punishing the gaming instead of fixing the incentive structure. Replacing gamed metrics with more metrics (creates an arms race). Using metrics as individual performance targets (guarantees gaming). Not involving the team in metric selection. Reacting to metric fluctuations without investigating context.",
    "whatGoodLooksLike": "Within 4 weeks: single-metric targets replaced with paired metrics, team involved in selecting what to measure. Within 8 weeks: engineers engage with metrics constructively instead of gaming them. Metrics drive at least one real process improvement per month. Gaming stops because incentives are aligned. The article 'Keep your delivery in balance with these metrics pairings' emphasizes: metrics viewed in isolation are misleading — pairing deployment frequency with PR size, change failure rate with unreviewed PRs, reveals the full picture and prevents single-dimension optimization. Calibration signal: \"Calibration evidence: 'Reduced lead time from 5 days to 8 hours by [specific action: eliminated manual QA gate / para...\" (SIG-013).",
    "mappingNotes": "Metrics gaming diagnosis and correction scenario",
    "suggestedMetricIds": [
      "2.1",
      "2.2"
    ]
  },
  {
    "id": "P-C13-3",
    "slug": "your-team-keeps-failing-security-audits-with-the-same-findings",
    "observableIds": [
      "C13-O3",
      "C13-O6"
    ],
    "capabilityIds": [
      "C13"
    ],
    "title": "Your Team Keeps Failing Security Audits With the Same Findings",
    "context": "For the third year in a row, your external audit surfaces the same categories of findings: stale access permissions, unpatched dependencies, and missing encryption at rest. Your team scrambles before each audit but nothing sticks. Leadership is frustrated.",
    "topicsActivated": [
      "Security (Continuous Compliance vs. Audit Cramming)",
      "Process (Automated Evidence Collection)",
      "Culture (Security Ownership)"
    ],
    "decisionFramework": "1. Diagnose the pattern (Week 1): Map every repeat finding to a root cause. Stale permissions → no automated access review. Unpatched deps → no dependency scanning in CI. Missing encryption → no infrastructure-as-code enforcement. The findings are symptoms; the root causes are missing automation. 2. Automate evidence (Week 2-4): For each repeat finding, implement continuous automated checks: scheduled access reviews with auto-expiration, dependency scanning in CI/CD with severity gates, IaC policies that enforce encryption. The goal: compliance evidence generated continuously, not reconstructed annually. 3. Make it visible (Month 2): Create a compliance dashboard showing real-time posture — not a spreadsheet updated once a year. When something drifts, the team sees it within hours, not months. 4. Shift ownership (Month 2-3): Rotate a security champion role. Make compliance part of the definition of done, not a separate review step. 5. Prepare for next audit with confidence (Ongoing): If evidence is generated continuously, audit prep becomes a reporting exercise, not a fire drill.",
    "commonMistakes": "Treating audit prep as a periodic sprint instead of fixing root causes. Blaming the auditors for 'not understanding our context.' Adding headcount to manually check things that should be automated. Confusing compliance (checking boxes) with security (actually being secure). Building a compliance team instead of embedding compliance into engineering.",
    "whatGoodLooksLike": "Within 90 days: automated scanning for all previous repeat findings, compliance dashboard live, access reviews automated. Next audit: zero repeat findings, audit prep takes days not weeks. The LeadDev article on DevSecOps culture asks the right diagnostic question: when a vulnerability is found, is the response pathological (fingers pointed), bureaucratic (heads roll), or generative (seen as opportunity to improve)? Your answer reveals whether you'll keep repeating audit findings. Calibration signal: \"Compliance requirement at Big Tech — EMs who ignore this create org-level risk and VP-level escalations\" (SIG-026).",
    "mappingNotes": "Recurring security audit failure scenario",
    "suggestedMetricIds": [
      "9.1",
      "9.3"
    ]
  },
  {
    "id": "P-C13-4",
    "slug": "balancing-security-risk-with-shipping-speed-after-a-close-call",
    "observableIds": [
      "C13-O1",
      "C13-O7"
    ],
    "capabilityIds": [
      "C13",
      "C8"
    ],
    "title": "Balancing Security Risk With Shipping Speed After a Close Call",
    "context": "A near-miss security incident revealed that a feature shipped without a threat model — a misconfigured API endpoint was exposing customer PII to unauthenticated requests, caught in production monitoring before data was exfiltrated. Leadership wants to know how this happened and what you are doing to prevent it.",
    "topicsActivated": [
      "Security (Shift-Left Practices)",
      "Risk Management (Near-Miss Response)",
      "Process (Security Gates Without Bottlenecks)"
    ],
    "decisionFramework": "1. Treat the near-miss like an incident (Week 1): Run a blameless post-mortem on the near-miss. How did a feature touching user input ship without a security review? Map the failure in the process — not who to blame, but what structural gap allowed it. 2. Define security-sensitive triggers (Week 2): Not every feature needs a threat model. Define clear triggers: features touching authentication, user input, payments, PII, or external APIs require one. Everything else doesn't. This prevents security from becoming a bottleneck for routine work. 3. Implement proportional gates (Week 3-4): Add automated SAST scanning for all PRs (catches the SQL injection pattern). Add manual threat model requirement only for triggered features. Severity-based deployment gates: critical/high block, medium creates tracked ticket. 4. Measure the balance (Month 2+): Track both metrics: deployment lead time (didn't get slower) and vulnerability escape rate (trending toward zero). If security gates slow deployment by more than 10%, the gates are too heavy — tune them.",
    "commonMistakes": "Overreacting by requiring security review for everything (creates bottleneck, team routes around it). Underreacting because 'it didn't actually get exploited.' Adding a manual gate without automated scanning first (the opposite order of what works). Making security someone else's problem instead of the team's responsibility.",
    "whatGoodLooksLike": "Near-miss treated as learning opportunity, not blame event. Clear security triggers defined within 2 weeks. Automated scanning catching the class of vulnerability that escaped. Deployment velocity maintained while security posture improves. The team views security as 'how we work' rather than 'what slows us down.' Calibration signal: \"Threat modeling practice: 'Required STRIDE threat models for auth and payment features — caught 5 design-level vulner...\" (SIG-213).",
    "mappingNotes": "Security near-miss response and proportional security gates scenario",
    "suggestedMetricIds": [
      "9.1",
      "1.2"
    ]
  },
  {
    "id": "P-C11-4",
    "slug": "scaling-your-interview-process-from-5-to-50-hires-per-year",
    "observableIds": [
      "C11-O1",
      "C11-O9",
      "C11-O11"
    ],
    "capabilityIds": [
      "C11"
    ],
    "title": "Scaling Your Interview Process From 5 to 50 Hires Per Year",
    "context": "Your team went from hiring 1-2 engineers per quarter to needing 10+ per quarter. Your ad-hoc interview process worked fine at low volume but now you're seeing: inconsistent evaluations between interviewers, candidates getting wildly different interview experiences, no data on what predicts success, and your best engineers spending 30% of their time interviewing.",
    "topicsActivated": [
      "Hiring (Process Standardization)",
      "Metrics (Hiring Funnel Analytics)",
      "Culture (Interview Quality at Scale)"
    ],
    "decisionFramework": "1. Design the interview funnel as a system (Week 1): Map each round to the specific, non-overlapping signal it provides. If two rounds test similar skills, consolidate. Target 4-5 total interactions (including recruiter screen) with a 2-week end-to-end target. 2. Build an interviewer training program (Week 2-3): At 50 hires/year, you need 15-20 calibrated interviewers. Create a training program: shadow 2 interviews, reverse-shadow 1, certified after calibration review. Track interviewer load and rotate to prevent burnout. 3. Automate coordination (Month 1): Scheduling at scale is a major bottleneck. Implement automated scheduling, standardized feedback forms with mandatory submission before debrief, and a candidate tracking dashboard. 4. Implement quality controls (Month 1-2): Track per-interviewer pass rates and flag outliers for recalibration. Monitor candidate-stage drop-off rates against industry benchmarks. Run quarterly calibration sessions using recorded mock interviews. 5. Protect candidate experience at scale (Ongoing): High volume creates a temptation to depersonalize. Set maximum response-time SLAs, assign a dedicated hiring coordinator, and run candidate experience surveys after every loop.",
    "commonMistakes": "Letting each interviewer design their own questions and evaluation criteria. Debriefing before individual scores are submitted (anchoring bias). Burning out your best engineers with interview load without tracking or rotating. Using the same 3 interviewers for every candidate instead of building a deep bench. Not automating scheduling — manual coordination becomes the bottleneck at scale. Adding more interview rounds instead of improving existing ones when a bad hire happens.",
    "whatGoodLooksLike": "Rubric adopted within 2 weeks, independent scoring within 3 weeks. Interview experience consistent across candidates. Per-interviewer pass rates converge within 15% after calibration. Funnel data informing process improvements monthly. Time-to-hire decreasing while quality-of-hire stable or improving. GitLab's experience: standardized rubrics with Google Sheets automation to track scoring allowed them to test hypotheses (e.g., 'Do candidates need Vue experience specifically, or does any modern framework predict success?') and verify assumptions with data rather than opinions. Calibration signal: \"Director-level skill: 'Secured [X] headcount by demonstrating [Y] revenue impact / [Z] risk if unfilled'\" (SIG-068).",
    "mappingNotes": "Interview process scaling and standardization scenario",
    "suggestedMetricIds": [
      "7.3"
    ]
  },
  {
    "id": "P-C14-4",
    "slug": "running-your-first-cross-team-performance-calibration",
    "observableIds": [
      "C14-O1",
      "C14-O7"
    ],
    "capabilityIds": [
      "C14"
    ],
    "title": "Running Your First Cross-Team Performance Calibration",
    "context": "You've been asked to lead calibration across 4 engineering teams (~30 engineers). Each manager has their own standards — some are generous, some are strict. There's no shared vocabulary for what 'exceeds expectations' means. You need consistent, fair outcomes.",
    "topicsActivated": [
      "Performance (Cross-Team Consistency)",
      "Leadership (Calibration Facilitation)",
      "Culture (Fairness and Bias Reduction)"
    ],
    "decisionFramework": "1. Pre-calibration prep (2 weeks before): Have each manager prepare evidence portfolios for their reports — specific examples, not vibes. Share the rating rubric with behavioral anchors for each level. Ask managers to pre-rate their reports independently. 2. Run pre-calibration 1:1s (1 week before): Meet each manager individually. Review their ratings. Challenge vague justifications ('she's a strong performer' → 'show me the specific impact'). Flag potential biases: recency bias (are examples only from last 6 weeks?), similarity bias (do all high performers look like the manager?), gender bias (are women rated on past performance while men are rated on potential?). 3. Calibration meeting (2-3 hours): Review by level, not by team — compare all Senior engineers across teams, then all Mids, etc. Focus discussion on borderline cases, not clear performers. Require specific evidence for every rating. Track the distribution across teams — if one manager rates everyone 'exceeds', that's a calibration problem. 4. Post-calibration (Same week): Each manager delivers feedback to their reports within 5 business days. Share one-sentence distillation of the key message — this is the sentence the engineer will remember. Follow up with written summary and development plan within 1 week.",
    "commonMistakes": "Running calibration without pre-calibration prep (becomes a 6-hour meeting). Accepting vague evidence ('strong performer' without specific examples). Not checking for bias patterns across the distribution. Treating calibration as ranking instead of consistency alignment. Delivering feedback weeks after calibration (information goes stale). Writing reviews the engineer will forget — distill the key message into one sentence.",
    "whatGoodLooksLike": "Pre-calibration catches 3-5 inconsistencies before the formal meeting. Calibration meeting takes under 3 hours because managers come prepared. Ratings consistent across teams at the same level. Zero 'surprise' feedback when managers deliver reviews. Distribution looks reasonable — not everyone exceeds expectations. Smruti Patel's recommendation: run pre-calibration sessions to give managers a safe space to practice presenting their cases while you call out bias, subjectivity, and inconsistency before the formal calibration. Calibration signal: \"Core craft skill\" (SIG-075).",
    "mappingNotes": "Cross-team performance calibration facilitation scenario",
    "suggestedMetricIds": [
      "5.1",
      "7.1"
    ]
  },
  {
    "id": "P-C4-4",
    "slug": "mobilizing-your-team-for-a-high-stakes-deadline",
    "observableIds": [
      "C4-O1",
      "C4-O12"
    ],
    "capabilityIds": [
      "C4",
      "C10"
    ],
    "title": "Mobilizing Your Team for a High-Stakes Deadline",
    "context": "Your team has 6 weeks to deliver a major feature that would normally take 12. It's a real business-critical deadline (contract commitment, not artificial urgency). You need to supercharge execution without burning people out.",
    "topicsActivated": [
      "Operational Rhythm (Focused Execution)",
      "Resource Allocation (Temporary Concentration)",
      "Team Health (Sustainable Intensity)"
    ],
    "decisionFramework": "1. Validate the deadline (Day 1): Confirm this is genuinely immovable, not artificial urgency that will repeat. Beast Mode only works if it's rare and real. If it's the third 'critical deadline' this quarter, the problem isn't execution — it's planning. 2. Clear the deck (Day 1-3): Get stakeholder agreement to pause everything else. Every competing priority must be explicitly paused, not 'also done on the side.' If stakeholders won't pause other work, the deadline isn't actually the priority. 3. Structure for focus (Week 1): Split the team into pairs or small squads, each owning a vertical slice of the deliverable. Each squad chooses one task per sprint. Reduce meetings to the minimum: brief daily sync, weekly demo, nothing else. 4. Remove all friction (Ongoing): Your job is clearing blockers in real-time — unblock dependencies, make decisions fast, shield from distractions. You should not be writing code. You are the team's router. 5. Protect sustainability (Ongoing): Set explicit boundaries — no weekend work unless agreed, daily standup includes energy check, EM watches for burnout signals. The goal is focused intensity, not heroics. 6. Wind down (After delivery): Explicit cooldown period. Celebrate the achievement. Retrospective on what worked. Resume normal operations. Don't let Beast Mode become the new baseline.",
    "commonMistakes": "Making every sprint a 'Beast Mode' (it stops working and burns people out). Not pausing other work (team is doing Beast Mode + everything else). EM trying to contribute code instead of clearing blockers. No explicit wind-down — Beast Mode energy becomes expected baseline. Using Beast Mode to compensate for chronic under-resourcing or poor planning.",
    "whatGoodLooksLike": "Everything non-critical paused within 3 days. Team energized by clear focus and urgency. Pairs/squads owning vertical slices with high autonomy. Daily blockers cleared within hours, not days. Feature delivered on time. No burnout — team tired but proud, not exhausted. The LeadDev 'Beast Mode' case study: splitting into autonomous pairs with single-task focus, eliminating all distractions, and providing maximum trust resulted in features built from ground up at unseen speed — slight delay of weeks vs. potential delay of months. Calibration signal: \"EMs define team's communication operating system\" (SIG-060).",
    "mappingNotes": "Time-constrained high-stakes execution scenario",
    "suggestedMetricIds": [
      "1.2",
      "1.4"
    ]
  },
  {
    "id": "P-C12-5",
    "slug": "building-team-culture-through-intentional-rituals",
    "observableIds": [
      "C12-O2",
      "C12-O4",
      "C12-O7"
    ],
    "capabilityIds": [
      "C12"
    ],
    "title": "Building Team Culture Through Intentional Rituals",
    "context": "Your team ships reliably but lacks cohesion. Engineers don't understand business metrics, don't contribute to roadmap discussions, and treat culture as 'something HR does.' Retros feel perfunctory and nobody proposes ambitious improvements. You want to move from a group of individuals to a genuine team.",
    "topicsActivated": [
      "Culture (Ritual Design)",
      "Engagement (Business Context Sharing)",
      "Innovation (Structured Ideation)"
    ],
    "decisionFramework": "1. Start with metrics visibility (Month 1): Establish a monthly 'numbers review' — a 45-minute meeting where engineering, product, design, and relevant business stakeholders review key metrics together. The goal isn't accountability theater; it's giving engineers the context they lack. When engineers understand business metrics, they make better technical trade-offs without being told. 2. Add structured ideation (Month 2): Monthly session where any team member can propose product, marketing, or process improvements. Key: proposals must include a lightweight problem statement and expected impact. This isn't a free-for-all brainstorm; it's a structured channel for bottom-up innovation. Engineers who contribute to the roadmap feel ownership over outcomes. 3. Create a 10X improvement space (Month 3): Bi-monthly session dedicated to ambitious technical improvements — monitoring stacks, automation, architecture evolution. The rule: ideas must aim for 10x improvement, not 10%. This gives engineers permission to think big and surfaces strategic technical investments. 4. Iterate and prune: Not every ritual will land. After 3 months, retrospect on the rituals themselves. Drop what isn't working. The goal is 2-3 high-value rituals, not a packed calendar.",
    "commonMistakes": "Adding rituals without removing anything, creating meeting overload. Making attendance mandatory without explaining purpose. Letting meetings become status updates instead of genuine discussions. Starting all four rituals at once instead of building incrementally. Not involving the team in choosing which rituals to adopt.",
    "whatGoodLooksLike": "Within 3 months: engineers can articulate team KPIs without prompting, at least one bottom-up idea ships per quarter, and the team proactively proposes technical improvements. The LeadDev article on team rituals emphasizes that these aren't add-ons — they replace the context and ownership that agile ceremonies alone don't provide. Calibration signal: \"Intentional culture-building signal: 'Established team charter adopted by [X] engineers, referenced in onboarding and...\" (SIG-086).",
    "mappingNotes": "Based on LeadDev 'rituals that revolutionized engineering teams' — Numberzz, Ideation, and 10X patterns.",
    "suggestedMetricIds": [
      "5.1",
      "6.4"
    ]
  },
  {
    "id": "P-C12-6",
    "slug": "rebuilding-psychological-safety-after-organizational-disruption",
    "observableIds": [
      "C12-O1",
      "C12-O6",
      "C12-O7"
    ],
    "capabilityIds": [
      "C12",
      "C14"
    ],
    "title": "Rebuilding Psychological Safety After Organizational Disruption",
    "context": "Your organization just went through layoffs, a major re-org, or leadership change. The remaining team is disengaged — people avoid speaking up in meetings, nobody challenges decisions, risk-taking has stopped, and your best performers are quietly interviewing. Trust has evaporated.",
    "topicsActivated": [
      "Culture (Psychological Safety Recovery)",
      "People (Trust Rebuilding)",
      "Leadership (Vulnerability and Transparency)"
    ],
    "decisionFramework": "1. Acknowledge reality immediately (Week 1): Do not pretend things are normal. In your first team meeting post-disruption, name what happened directly. Share what you know and what you don't. People's anxiety comes from uncertainty, not from bad news. The worst response is silence. 2. Listen before acting (Week 1-2): Run individual check-ins with every team member. Do not start with 'here's the plan going forward.' Start with 'how are you doing, and what are you worried about?' Write down themes but don't promise fixes yet. 3. Re-establish predictability (Week 2-4): People need to know what won't change. Reaffirm team mission, working agreements, and key processes. Publish a simple 30-60-90 plan. Predictability rebuilds the foundation of safety. 4. Model vulnerability (Ongoing): Share your own uncertainty. 'I don't have all the answers' is more powerful than a confident facade. Thank people publicly for disagreeing or raising concerns. 5. Create low-stakes participation (Month 2+): Start meetings with brief check-ins. Use written-first formats for sensitive discussions. Create anonymous feedback channels. Rebuild contributor safety gradually — people need to see that speaking up is safe before they'll do it.",
    "commonMistakes": "Jumping straight to 'rallying the troops' with false optimism. Avoiding the topic entirely and hoping time heals. Over-communicating strategy when people need emotional acknowledgment first. Treating symptoms (low velocity) instead of root cause (low trust). Expecting psychological safety to return on a timeline you control.",
    "whatGoodLooksLike": "Within 60 days: team members raise concerns in meetings again, at least one person publicly disagrees with a decision constructively, and attrition stabilizes. Within 90 days: team health survey shows improvement. The LeadDev articles on post-layoff leadership emphasize that consistency — not grand gestures — rebuilds trust. Show up, listen, be transparent, repeat. Calibration signal: \"EMs who avoid conflict are a red flag in calibration\" (SIG-044).",
    "mappingNotes": "Based on LeadDev articles on psychological safety after layoffs and the REST framework for post-layoff leadership.",
    "suggestedMetricIds": [
      "5.1",
      "7.8"
    ]
  },
  {
    "id": "P-C10-5",
    "slug": "navigating-your-first-annual-engineering-budget-cycle",
    "observableIds": [
      "C10-O1",
      "C10-O6",
      "C10-O7"
    ],
    "capabilityIds": [
      "C10",
      "C1"
    ],
    "title": "Navigating Your First Annual Engineering Budget Cycle",
    "context": "You've been promoted to a role where you now own budget planning for the first time. You need to present a headcount plan, tooling budget, and infrastructure forecast to finance and your VP. You've never done this before and don't want to show up with a wishlist that gets cut in half.",
    "topicsActivated": [
      "Resource Allocation (Budget Planning Process)",
      "Stakeholder (Cross-Functional Budget Alignment)",
      "Strategy (ROI-Driven Investment Cases)"
    ],
    "decisionFramework": "1. Understand last year first (Month before cycle): Get last year's approved budget, actual spend, and variance. Understand what was approved, what was actually spent, and why they differed. This is your baseline credibility — showing you understand history before proposing the future. 2. Build three tiers (During planning): For every major ask, present three options with quantified trade-offs. Tier 1 (minimum viable): what you need to keep the lights on. Tier 2 (recommended): what you need to hit committed goals. Tier 3 (investment): what would accelerate beyond current targets. Each tier has a number, a deliverable, and a trade-off. 3. Present with product as a united front: Align with your product counterpart before the budget meeting. Non-technical leaders see a joint eng+product plan as alignment, giving them confidence. A solo engineering budget request looks like empire building. 4. Bring data, not opinions: Every headcount request links to a deliverable. Every tooling request includes current cost and projected savings. Don't say 'we need more people' — say 'to deliver X by Q3, we need Y engineers; without them, the timeline extends to Q1 next year.' 5. Plan for what you'll cut: Finance will likely reduce your ask by 15-25%. Decide in advance which tier-3 items you'd sacrifice. Being prepared for the cut earns more trust than fighting it.",
    "commonMistakes": "Submitting a budget in isolation without product alignment. Asking for headcount without linking it to deliverables. Not knowing last year's actual spend. Presenting a single number instead of tiered options. Getting emotional when the budget gets cut instead of having a prepared fallback position.",
    "whatGoodLooksLike": "Budget approved within one revision cycle. Finance trusts your numbers because they're grounded in historical data and linked to deliverables. You get 80-90% of your tier-2 ask. Multiple LeadDev articles on budget planning converge on this: the leaders who get funded are the ones who present as business partners, not cost centers. Calibration signal: \"Directors own org-level headcount story; EMs own team-level justification\" (SIG-124).",
    "mappingNotes": "Based on LeadDev articles on annual budget planning best practices and overcoming budget planning challenges.",
    "suggestedMetricIds": [
      "10.1",
      "10.3"
    ]
  },
  {
    "id": "P-C2-3",
    "slug": "everything-is-priority-zero-and-your-team-is-drowning",
    "observableIds": [
      "C2-O2",
      "C2-O5",
      "C2-O3"
    ],
    "capabilityIds": [
      "C2",
      "C10"
    ],
    "title": "Everything Is Priority Zero and Your Team Is Drowning",
    "context": "Every initiative in flight has been labeled 'highest priority' by a different stakeholder. Your team is context-switching between 5 workstreams, nothing is finishing, and engineers are frustrated. You tried pushing back but each stakeholder explains why theirs is genuinely the most urgent. Sprint commitments are meaningless.",
    "topicsActivated": [
      "Strategy (Ruthless Prioritization)",
      "Stakeholder (Priority Negotiation)",
      "Execution (Focus and Throughput)"
    ],
    "decisionFramework": "1. Make the cost visible (Day 1): List every active workstream with current allocation and expected completion date at current pace. Show stakeholders the math: 5 parallel priorities with 8 engineers means each gets 1.6 engineers and nothing ships for months. Parallelism has a cost and it's visible when you write it down. 2. Force-rank with a framework (Week 1): Use a simple scoring model — business impact, urgency (reversibility of delay), and confidence. Score each initiative. The framework doesn't have to be perfect; its value is making the ranking conversation explicit rather than political. Present the ranked list to leadership and ask: 'Do you agree with this order? If not, which two would you swap and why?' 3. Establish a 'not doing' list (Week 1): Publish what you're explicitly not doing and why. This is more important than the doing list. It forces acknowledgment that trade-offs exist. A highly visible task does not need to become your highest priority the moment you learn about it — but you should communicate where it sits on your list and why. 4. Limit active work (Week 2+): Set a WIP limit: no more than 2-3 active workstreams at a time. New priorities can only enter when something exits. This is not about saying no — it's about saying 'not yet, and here's when.' 5. Review monthly: Priorities shift. What wasn't important yesterday can become urgent tomorrow. Run a monthly priority review with stakeholders. This makes reprioritization a system, not a crisis.",
    "commonMistakes": "Trying to keep everyone happy by spreading the team thin across all priorities. Prioritizing based on who asks loudest rather than business impact. Not publishing the 'not doing' list because it feels confrontational. Using a framework so complex nobody trusts it. Failing to revisit priorities as context changes.",
    "whatGoodLooksLike": "Within 2 weeks: team is focused on 2-3 priorities, shipping tempo increases. Within a month: stakeholders reference the priority list in their own planning. The LeadDev article on the 'priority zero trap' captures it well: if everything is a priority then nothing is, and a prioritization system isn't about finding the perfect order — it's about creating a shared language for trade-offs. Calibration signal: \"The mark of a strong EM/Director: 'What you chose NOT to do, and why' — calibration values this over endless yeses\" (SIG-134).",
    "mappingNotes": "Based on LeadDev 'Avoiding the priority zero trap' and 'Building a prioritization framework.'",
    "suggestedMetricIds": [
      "2.2",
      "2.6",
      "8.4"
    ]
  },
  {
    "id": "P-C3-4",
    "slug": "managing-tech-debt-strategically-without-stopping-the-roadmap",
    "observableIds": [
      "C3-O4",
      "C3-O11",
      "C3-O1"
    ],
    "capabilityIds": [
      "C3",
      "C9"
    ],
    "title": "Managing Tech Debt Strategically Without Stopping the Roadmap",
    "context": "Your team has significant tech debt that's slowing delivery, but you can't stop the roadmap for a multi-month cleanup. Product is sympathetic but won't give you a quarter of dedicated refactoring time. Engineers are frustrated by working around old decisions. Previous 'tech debt sprints' produced feel-good cleanup that didn't move the needle on actual velocity.",
    "topicsActivated": [
      "Architecture (Systematic Debt Management)",
      "Strategy (Capacity Allocation)",
      "Metrics (Impact Quantification)"
    ],
    "decisionFramework": "1. Inventory and score (Week 1-2): Create a lightweight tech debt register. For each item, score three things: severity (how much does it slow us down daily?), strategic impact (does it block roadmap items?), and effort to resolve. Use a simple 1-5 scale. The goal isn't precision — it's creating shared language for comparing debts across the team. 2. Find the leverage points: The insight from experience is that debt isn't solved by scope — it's solved by relevance. Look for the 2-3 debt items blocking the most roadmap work. An e-commerce team found three requested features were blocked by just two architectural decisions from four years ago. Fixing those two items unblocked months of roadmap. 3. Establish a capacity allocation (Month 1): Adopt a 70/20/10 model — 70% on roadmap delivery, 20% on medium-term technical health (the debt items blocking roadmap), and 10% on longer-term cleanup or experiments. This gives product predictability and engineers breathing space. 4. Quantify in business terms (Ongoing): Frame debt as risk and velocity, not engineering purity. 'This service has 47 known issues and our deploy frequency has dropped from daily to weekly' is more compelling to leadership than 'the architecture needs modernizing.' 5. Make progress visible (Ongoing): Track the debt register publicly. Show items resolved, velocity improvements, and incidents prevented. Treat tech debt like a product backlog — prioritized, visible, and regularly groomed.",
    "commonMistakes": "Trying to fix all debt at once instead of targeting highest-leverage items. Framing debt as an engineering concern rather than a business risk. Running 'tech debt sprints' that clean up low-impact items for morale but don't improve velocity. Not tracking the impact of debt reduction, so leadership sees it as overhead. Allocating 0% to tech health because 'we'll get to it after the next launch.'",
    "whatGoodLooksLike": "Within 3 months: measurable improvement in deploy frequency or cycle time from targeted debt reduction. Team has a living debt register that product references during planning. The 70/20/10 allocation is respected in sprint planning. The LeadDev article's key insight resonates: the most effective tech debt strategies focus on relevance, not comprehensiveness. Calibration signal: \"Tech debt tracking explicitly includes AI-generated code debt — monitoring for DRY violations, duplicated logic acros...\" (SIG-303).",
    "mappingNotes": "Based on LeadDev 'How to create a tech debt strategy that works' (70/20/10 model), 'Practical tech-debt prioritization', and tech debt scoring frameworks.",
    "suggestedMetricIds": [
      "2.3",
      "3.2"
    ]
  },
  {
    "id": "P-C7-4",
    "slug": "making-a-high-stakes-technical-decision-under-time-pressure",
    "observableIds": [
      "C7-O1",
      "C7-O10",
      "C7-O11"
    ],
    "capabilityIds": [
      "C7",
      "C3"
    ],
    "title": "Making a High-Stakes Technical Decision Under Time Pressure",
    "context": "You need to decide between two architectural approaches for a critical project. Both have significant trade-offs. Your team is split. Leadership wants an answer by Friday. The wrong choice will be expensive to reverse. You're feeling pressure to either over-analyze (missing the deadline) or decide too fast (regret later). Previous big decisions have been revisited repeatedly because they weren't well-documented.",
    "topicsActivated": [
      "Decision Making (Structured Under Pressure)",
      "Communication (Decision Documentation)",
      "Architecture (Trade-off Analysis)"
    ],
    "decisionFramework": "1. Classify the decision (Hour 1): Is this a one-way door (irreversible, high cost to change) or a two-way door (reversible, low switching cost)? One-way doors deserve deliberation. Two-way doors should be made fast and revisited if needed. If you're not sure, default to two-way and use feature flags to contain blast radius. 2. Structure the options (Day 1): For each option, document three things: what it optimizes for, what it sacrifices, and what assumptions it depends on. Avoid framing as 'good option vs. bad option' — instead, frame as 'Option A optimizes for X at the cost of Y; Option B optimizes for Y at the cost of X.' 3. Seek disconfirming input (Day 1-2): Ask the strongest advocate of the option you're leaning against to make their best case. Run a 15-minute pre-mortem: 'It's 6 months from now and this decision was a disaster — what happened?' This surfaces risks your confidence might be hiding. 4. Decide and document (Day 2-3): Make the call. Write an Architecture Decision Record (ADR): context, options considered, decision, and rationale. The documentation matters because every decision creates a policy — future teams will encounter similar choices and need to understand why you chose what you chose. 5. Plan a decision retro (Schedule for 30/60/90 days out): After 30 days, run a 15-minute retro: 'What worked, what felt unclear, would we make the same call?' This normalizes learning from decisions and reduces the fear of making them.",
    "commonMistakes": "Analysis paralysis — waiting for perfect information that will never come. Making the decision in your head and announcing it without structured input. Treating the decision as purely technical when it has organizational and people implications. Not documenting the rationale, leading to the same debate resurfacing months later. Deciding quickly to relieve pressure without considering trade-offs.",
    "whatGoodLooksLike": "Decision made within timeline. ADR published and referenced by other teams facing similar choices. Team feels heard even if they disagreed. 30-day retro confirms the decision or surfaces learnings for the next one. The LeadDev articles on tough technical decisions converge on this: decision-making isn't about having all the answers — it's about creating structure when things are messy and helping your team feel confident moving forward. Calibration signal: \"Judgment signal: 'Identified [X] issues in first month, addressed critical ones immediately, built 90-day plan for sy...\" (SIG-064).",
    "mappingNotes": "Based on LeadDev 'Mastering tough technical decisions', 'How to make the right decisions under pressure', and 'Every decision creates a policy' (ADR/documentation emphasis).",
    "suggestedMetricIds": [
      "2.2",
      "2.5"
    ]
  },
  {
    "id": "P-C11-5",
    "slug": "your-onboarding-is-broken-and-new-hires-are-struggling",
    "observableIds": [
      "C11-O3",
      "C11-O4"
    ],
    "capabilityIds": [
      "C11",
      "C12"
    ],
    "title": "Your Onboarding Is Broken and New Hires Are Struggling",
    "context": "New engineers take 6+ months to become productive. They report feeling lost in their first weeks, unsure who to ask for help, and overwhelmed by undocumented tribal knowledge. Your most recent hire quietly admitted in their 1:1 that they considered leaving in month two. Meanwhile, your tenured engineers complain they spend too much time answering basic questions.",
    "topicsActivated": [
      "Onboarding (Structured Ramp Program)",
      "Culture (Knowledge Sharing and Belonging)",
      "Metrics (Time-to-Productivity Measurement)"
    ],
    "decisionFramework": "1. Diagnose the gaps (Week 1): Interview your 3 most recent hires about their first 90 days. Ask: 'What did you wish you'd known on day one? When did you first feel productive? What almost made you leave?' Also ask tenured engineers: 'What questions do new hires always ask that shouldn't require asking?' The gap between these two perspectives reveals your onboarding holes. 2. Build progressive structure (Week 2-3): Design a 30/60/90 day plan with specific milestones — not 'learn the codebase' but 'deploy your first change to production by day 10, own a small feature by day 30, lead a design discussion by day 60.' Progressive information introduction works: team, then tech stack, then product domain, then first meaningful project. 3. Assign a buddy (not the manager): Every new hire gets a peer-level buddy for 90 days. The buddy's role is distinct from the manager's: the buddy covers 'where things live, how to ask for help, unwritten team norms.' The manager covers 'expectations, career, feedback.' Early buddy 1:1s dramatically increase comfort in asking questions. 4. Make knowledge discoverable: Audit the most common questions new hires ask and document the answers. This isn't about comprehensive documentation — it's about eliminating the 10 questions that every new hire asks in their first month. 5. Measure and iterate (Quarterly): Track time-to-first-deploy, time-to-first-feature, and run an onboarding exit survey at 90 days. Each cohort should onboard faster than the last. Act on the survey feedback — the fixes are usually small and high-leverage.",
    "commonMistakes": "Throwing new hires at the codebase with 'just read the code.' Assigning the manager as the only point of contact, creating a bottleneck. Having no milestones, so nobody knows if onboarding is going well or not. Blaming the hire for 'not being a good fit' when onboarding set them up to fail. Never collecting or acting on feedback from recent hires.",
    "whatGoodLooksLike": "Time-to-first-deploy under 5 days. 90-day exit survey satisfaction above 8/10. No new hire considers leaving in the first 3 months. Tenured engineers report fewer repeated questions. Top-quartile companies achieve full productivity in 3-4 months versus the 6-7 month average. Each new cohort ramps faster than the last because the program improves from their feedback. Calibration signal: \"Investment in people that calibration notices: 'Buddy program reduced new hire Time-to-first-PR from 2 weeks to 3 days'\" (SIG-070).",
    "mappingNotes": "Based on LeadDev '5 ways onboarding can accelerate engineering efficiency' and 'How to successfully onboard remote engineering staff in four weeks.'",
    "suggestedMetricIds": [
      "7.5",
      "5.1"
    ]
  },
  {
    "id": "P-C13-5",
    "slug": "responding-to-a-critical-dependency-vulnerability",
    "observableIds": [
      "C13-O2",
      "C13-O6",
      "C13-O1"
    ],
    "capabilityIds": [
      "C13",
      "C8"
    ],
    "title": "Responding to a Critical Dependency Vulnerability",
    "context": "A CVE is published for a widely-used open source dependency in your stack (think Log4Shell, XZ Utils-class events). The vulnerability is rated critical. Your team doesn't know how many services are affected, there's no bill of materials, and leadership is asking for a status update. Security is asking when you'll be patched. You're not sure where to start.",
    "topicsActivated": [
      "Security (Supply Chain Vulnerability Response)",
      "Operations (Rapid Dependency Assessment)",
      "Process (SBOM and Dependency Governance)"
    ],
    "decisionFramework": "1. Assess blast radius (Hour 1-4): Before patching anything, answer: where is this dependency? If you have dependency scanning in CI/CD, query it. If not, run a manual search across all repos. Build a list of affected services ranked by exposure (internet-facing > internal > dev-only). This is your battlefield map. 2. Triage by exposure (Hour 4-8): Not everything needs to be patched simultaneously. Internet-facing services with the vulnerable code path active get patched first. Services behind authentication or internal-only are next. Dev/test environments are last. Communicate this triage to security and leadership — they need to see a plan, not just effort. 3. Patch and verify (Day 1-3): For each affected service: update the dependency, run tests, deploy to staging, verify no regression, deploy to production. If a clean upgrade isn't possible, evaluate workarounds (WAF rules, configuration changes, feature disabling) as temporary mitigations. 4. Communicate continuously: Send status updates at fixed intervals (every 4 hours during active response, then daily). Include: services patched, services remaining, blockers, ETA. Leadership doesn't need technical details — they need 'X of Y services patched, ETA for full remediation is Z.' 5. Prevent recurrence (Week 2+): This is where the real work starts. Implement automated dependency scanning in CI/CD with severity-based gates. Create a software bill of materials (SBOM) so you never have to ask 'where is this dependency?' again. Establish vulnerability SLAs by severity (critical: 24-48hrs, high: 7 days, medium: 30 days).",
    "commonMistakes": "Panicking and trying to patch everything at once without triaging by exposure. Not knowing your dependency tree because there's no scanning or SBOM. Patching production without adequate testing, creating a new incident. Over-communicating technical details to leadership instead of status and ETA. Treating it as a one-time fire drill instead of building systematic prevention.",
    "whatGoodLooksLike": "All critical-exposure services patched within 48 hours. Full remediation within 7 days. SBOM established for all production services within 30 days. Dependency scanning in CI/CD within 60 days. Next time a critical CVE drops, blast radius assessment takes minutes instead of hours because the SBOM exists. The 2024 XZ Utils near-miss showed that supply chain attacks are becoming more sophisticated — preparedness is the differentiator. Calibration signal: \"Auditors and security teams track this\" (SIG-025).",
    "mappingNotes": "Based on supply chain security trends, XZ Utils incident analysis, and SBOM best practices.",
    "suggestedMetricIds": [
      "9.2",
      "3.5"
    ]
  },
  {
    "id": "P-C14-5",
    "slug": "putting-an-engineer-on-a-pip-designed-to-succeed",
    "observableIds": [
      "C14-O6",
      "C14-O4",
      "C14-O7"
    ],
    "capabilityIds": [
      "C14"
    ],
    "title": "Putting an Engineer on a PIP Designed to Succeed",
    "context": "An engineer on your team has been underperforming for months despite verbal feedback and informal coaching. You've documented the pattern and HR agrees a formal Performance Improvement Plan is appropriate. You've never run a PIP before and you're unsure how to make it genuinely developmental rather than just a paper trail to termination. The engineer is anxious and the team is watching.",
    "topicsActivated": [
      "Performance (Formal Improvement Process)",
      "People (Developmental Support During PIP)",
      "Process (Documentation and Fairness)"
    ],
    "decisionFramework": "1. Before the PIP — exhaust informal paths (Pre-PIP): Ensure you've delivered clear, specific feedback using SBI (Situation-Behavior-Impact) at least 2-3 times with documented follow-up. If you haven't told someone clearly that their performance is below expectations, you haven't earned the right to put them on a PIP. The PIP should never be the first signal. 2. Define clear, measurable objectives (Day 1): The PIP document must specify exactly what 'meeting expectations' looks like — not 'improve code quality' but 'reduce escaped defects to <2 per sprint and complete code reviews within 24 hours.' Each objective needs a measurable success criterion and a timeline (typically 30-60 days). Ambiguous PIPs fail both the employee and you. 3. Show explicit support (Ongoing): Increase 1:1 frequency to weekly. Assign a peer mentor if appropriate (with their consent and without disclosing the PIP context). Remove obstacles to success — if the engineer needs training, pairing sessions, or reduced scope to focus, provide it. The goal is genuine support, not performative documentation. 4. Check in formally at midpoint: At the halfway mark, provide written feedback on progress against each objective. If the engineer is improving, say so explicitly. If not, be direct about what's still falling short and what needs to change in the remaining time. No surprises at the end. 5. Conclude with integrity: If objectives are met — celebrate genuinely. Remove the PIP, acknowledge the growth, and continue regular performance management. If objectives are not met — the outcome should not surprise the engineer. You've been transparent throughout. Proceed with the agreed consequence (role change, exit) with dignity. Either outcome, do a personal retrospective: could you have intervened earlier or more effectively?",
    "commonMistakes": "Using the PIP as a surprise weapon when feedback has been vague or absent. Writing objectives so ambiguous that success is impossible to prove or disprove. Treating the PIP as a formality to terminate rather than a genuine attempt at improvement. Isolating the engineer — reducing their scope so much they can't demonstrate capability. Not checking in regularly, then delivering a final verdict with no warning. Discussing the PIP with other team members.",
    "whatGoodLooksLike": "Engineer either successfully improves and remains a productive team member, or departs understanding why. No procedural issues raised by HR or the employee. Team observes fair process even if they don't know the details. The LeadDev articles on PIPs emphasize that they should be a last resort, not a first tool, and their effectiveness depends entirely on the manager's genuine commitment to the person's success. Calibration signal: \"HR scrutinizes PIP quality\" (SIG-083).",
    "mappingNotes": "Based on LeadDev 'Driving positive change through performance improvement plans' and 'The relentless rise of the PIP.'",
    "suggestedMetricIds": [
      "5.5",
      "6.5"
    ]
  },
  {
    "id": "P-C7-5",
    "slug": "your-project-is-going-to-miss-its-deadline",
    "observableIds": [
      "C7-O4",
      "C7-O3",
      "C7-O2"
    ],
    "capabilityIds": [
      "C7",
      "C5"
    ],
    "title": "Your Project Is Going to Miss Its Deadline",
    "context": "You're three weeks from a committed deadline and you've realized the team won't make it. The gap isn't small — you're looking at 2-4 weeks of additional work. Your VP has already communicated the date to customers. Product is counting on this for their quarterly goals. You're dreading the conversation but every day you delay makes it worse.",
    "topicsActivated": [
      "Communication (Bad News Delivery)",
      "Stakeholder (Trust Preservation)",
      "Execution (Recovery Planning)"
    ],
    "decisionFramework": "1. Own it before you share it (Hour 1): Before telling anyone, reflect on what led here. Could you have flagged this earlier? What signals did you miss? This isn't self-blame — it's preparation. When you walk into the room, you need to own the situation credibly. Leaders who accept responsibility earn trust even when delivering bad news. 2. Prepare the full picture (Day 1): Before the conversation, have answers to: How late will we be? What's the revised realistic date? What caused the slip? What options exist to reduce scope or parallelize? Don't walk in with just 'we're going to be late' — walk in with 'we're going to be late, here's by how much, here's why, and here are three options.' 3. Deliver early and directly (Day 1): Do not bury the lede. Open with the news: 'We're going to miss the March 15 date. Our realistic delivery is March 28. Here's what happened and what I recommend.' Resist the urge to soften with preamble. Stakeholders will respect directness more than disclosure that feels reluctant. 4. Present options, not just problems: Option A: Ship reduced scope on time. Option B: Ship full scope 2 weeks late. Option C: Add resources to compress by 1 week. Each option has trade-offs — present them clearly and make a recommendation. 5. Manage your emotions and theirs: In tense moments, pause and breathe. Name emotions if needed: 'I understand this is frustrating — this deadline matters and I take the miss seriously.' Don't get defensive. Don't blame the team. After the conversation, communicate the revised plan to your team transparently.",
    "commonMistakes": "Waiting until the deadline to disclose the miss, hoping for a miracle. Blaming the team or external factors instead of owning the situation. Presenting the problem without options or a recovery plan. Over-promising a compressed timeline to reduce tension in the moment, only to miss again. Soft-pedaling the news so stakeholders don't grasp the severity.",
    "whatGoodLooksLike": "Stakeholder is disappointed but trusts you more because you were honest and prepared. Revised timeline holds. Team feels supported, not thrown under the bus. The LeadDev article on delivering bad news captures the core insight: the greatest fear is blame, but owning your mistakes and showing accountability is what preserves trust. Calibration signal: \"Trust is built by how you handle bad news\" (SIG-117).",
    "mappingNotes": "Based on LeadDev 'Getting good at delivering bad news' and deadline management articles.",
    "suggestedMetricIds": [
      "2.1",
      "2.4"
    ]
  },
  {
    "id": "P-C9-5",
    "slug": "introducing-engineering-metrics-to-a-team-that-has-none",
    "observableIds": [
      "C9-O10",
      "C9-O5",
      "C9-O1"
    ],
    "capabilityIds": [
      "C9",
      "C7",
      "C4"
    ],
    "title": "Introducing Engineering Metrics to a Team That Has None",
    "context": "Your engineering organization has no systematic metrics. Leadership makes decisions based on gut feel and anecdotes. When asked 'how is engineering doing?' you struggle to answer beyond 'we shipped X features.' You may not even have the tooling prerequisites — no CI/CD pipeline, manual deployments, no incident tracking — which means jumping straight to DORA or SPACE feels premature. You want to introduce metrics but you're worried about creating a surveillance culture or picking the wrong things to measure.",
    "topicsActivated": [
      "Metrics (Cold Start Strategy)",
      "Culture (Measurement Without Surveillance)",
      "Communication (Data-Driven Storytelling)"
    ],
    "decisionFramework": "1. Assess readiness and start with one metric (Month 1): Before adopting any framework, check your tooling prerequisites — DORA requires automated deployment, version control, and incident tracking as data sources. If those don't exist, DORA metrics will be manually fabricated. If you have to pick one metric, pick cycle time — the elapsed time from first commit to production deploy. It benefits the business, correlates with code quality, and correlates with developer satisfaction. One metric, well-understood, beats a dashboard nobody trusts. 2. Address engineers' problems first (Month 1-2): Introduce metrics that solve problems engineers already feel. If they complain about slow deploys, measure deployment frequency. If they complain about flaky tests, measure test reliability. When metrics address real pain, adoption is natural. Simultaneously, invest in CI/CD and deployment automation as prerequisites for meaningful measurement later. 3. Add process metrics gradually (Month 2-3): Layer in throughput and flow efficiency to identify bottlenecks. These are diagnostic tools, never performance measures for individuals. Be explicit: 'We measure systems, not people.' 4. Graduate to DORA when ready (Month 3-4): Once automated data sources exist, DORA metrics become meaningful. Add deployment frequency, lead time for changes, change failure rate, and MTTR. Connect to business outcomes by mapping engineering metrics to business impact: deployment frequency → feature velocity → time to market; cycle time → iteration speed → customer responsiveness. 5. Evolve beyond DORA (Month 6+): DORA captures throughput and stability but misses developer experience, learning, and innovation. Add developer satisfaction surveys and outcome metrics when DORA is well-established. Resist the temptation to adopt DORA, SPACE, or DX Core 4 on day one — start simple, build baseline data, then evaluate which framework fits your organization's maturity.",
    "commonMistakes": "Launching a comprehensive dashboard on day one that overwhelms everyone. Measuring individual developer output (lines of code, commits, PRs) rather than system flow. Not explaining why metrics are being introduced, triggering fear. Using metrics punitively, destroying the trust needed for honest measurement. Implementing DORA metrics without the tooling to generate accurate data. Manually tracking DORA metrics (defeats the purpose). Telling leadership 'we can't measure anything' instead of proposing a maturity-appropriate starting point.",
    "whatGoodLooksLike": "Within 3 weeks: maturity-appropriate starter metrics live. Within 3 months: team references cycle time in planning discussions, bottlenecks are identified from data rather than anecdotes, CI/CD and deployment automation improved as a side benefit. Within 6 months: leadership receives a monthly engineering health report grounded in data, engineers advocate for the metrics because they solve real problems. Team trusts the metrics because they've been involved in building the measurement system from the start. Nathen Harvey's insight: DORA requires automated data collection as a prerequisite — without it, you're measuring fiction. If you must focus on one metric, focus on cycle time — it benefits everyone. Calibration signal: \"Metrics maturity: 'Started with 3 DORA metrics, retired 1, added developer satisfaction survey — every active metric ...\" (SIG-220).",
    "mappingNotes": "Based on LeadDev 'Introducing engineering metrics to your organization', 'Engineering metrics at every level', and 'The flawed five engineering productivity metrics.'",
    "suggestedMetricIds": [
      "1.1",
      "2.2",
      "2.1",
      "1.2"
    ]
  },
  {
    "id": "P-C2-4",
    "slug": "killing-a-project-that-should-have-died-months-ago",
    "observableIds": [
      "C2-O3",
      "C2-O4",
      "C2-O2"
    ],
    "capabilityIds": [
      "C2",
      "C7"
    ],
    "title": "Killing a Project That Should Have Died Months Ago",
    "context": "A major initiative has been running for 6+ months with growing scope, shifting requirements, and declining team morale. Nobody believes it will deliver the original value, but there's significant sunk cost — engineering hours, leadership credibility, customer promises. You suspect the right move is to kill it, but everyone is afraid to be the one to say it.",
    "topicsActivated": [
      "Strategy (Sunk Cost Decision Making)",
      "Communication (Delivering Project Cancellation)",
      "People (Team Morale After Cancellation)"
    ],
    "decisionFramework": "1. Separate sunk cost from future value (Day 1): The money and time already spent are gone regardless. The only question is: 'Starting from today, is the remaining investment justified by the expected outcome?' If the answer is no, the project should stop. This is harder than it sounds because teams conflate 'we've invested so much' with 'therefore we should continue.' Name the sunk cost fallacy explicitly. 2. Build the kill case with data (Week 1): Document the project's trajectory — original timeline vs. current, original scope vs. current, team velocity trend, customer feedback signal. Compare the remaining investment to alternative uses of the same capacity. What else could this team ship in the same timeframe? Present this as an investment decision, not a failure acknowledgment. 3. Get alignment before announcing (Week 1-2): Have private conversations with product, your VP, and key stakeholders before any public announcement. The conversation is: 'Given what we know now, continuing costs X and delivers Y. Stopping frees capacity for Z. I recommend stopping.' Give them space to process. Nobody likes killing something they championed. 4. Communicate with dignity (Day of announcement): To the team: acknowledge their effort explicitly. 'The work you did was good. The decision to stop reflects changed circumstances, not your execution.' Name what was learned and how it applies going forward. Nothing is more demoralizing than having months of work dismissed as wasted. 5. Redirect immediately (Week 2+): Don't leave the team in limbo. Have the next mission ready — ideally something with visible, near-term impact to rebuild momentum. Run a brief retrospective: what signals should have triggered the stop decision earlier? Use the answer to build kill criteria into future projects.",
    "commonMistakes": "Continuing because of sunk cost rather than future value. Framing cancellation as failure rather than strategic reallocation. Announcing the kill without preparing stakeholders privately. Not acknowledging the team's effort, leaving them feeling their work was meaningless. Failing to redirect the team to meaningful work immediately after.",
    "whatGoodLooksLike": "Project stops within 2 weeks of the decision. Team is redirected to high-impact work and morale recovers within a month. The freed capacity delivers visible value in the next quarter. Leadership credits you for making a hard call, not for wasting resources. Future projects include explicit checkpoints for go/no-go reassessment. The LeadDev article on product pivots captures a key insight: take time to celebrate what the team did learn, even when the project didn't succeed. Calibration signal: \"Decision-making under ambiguity: 'In fast-moving market with incomplete data, classified decisions as one-way vs\" (SIG-285).",
    "mappingNotes": "Based on sunk cost decision frameworks and LeadDev 'Leading your engineering team through an unexpected product pivot.'",
    "suggestedMetricIds": [
      "8.3",
      "8.4"
    ]
  },
  {
    "id": "P-C11-6",
    "slug": "your-top-candidate-just-declined-winning-in-competitive-market",
    "observableIds": [
      "C11-O6",
      "C11-O10",
      "C11-O5"
    ],
    "capabilityIds": [
      "C11"
    ],
    "title": "Your Top Candidate Just Declined — Winning in a Competitive Talent Market",
    "context": "You've lost three of your last five top candidates to competing offers. Your interview process is solid, candidates reach the offer stage enthusiastic, but they take another offer at the last moment. Your employer brand isn't strong enough, your compensation is competitive but not leading, and your closing process is too slow. Leadership is frustrated by open reqs that stay unfilled for months.",
    "topicsActivated": [
      "Hiring (Competitive Offer Strategy)",
      "Employer Brand (Value Proposition)",
      "Process (Closing Velocity)"
    ],
    "decisionFramework": "1. Diagnose why candidates decline (Week 1): Reach out to recent decliners with a brief, non-defensive follow-up: 'We're working to improve our process — would you share what led to your decision?' Common reasons: compensation gap, slower process than competitors, stronger team/mission fit elsewhere, or remote work flexibility. Don't guess — ask. 2. Compress your timeline (Week 2): If your process takes 3+ weeks end-to-end, you're losing to companies that close in 10 days. Map your pipeline and identify bottlenecks: scheduling delays, too many interview rounds, slow hiring committee approvals. Set a target: 10 business days from first screen to offer. Speed is a competitive advantage. 3. Differentiate your value proposition (Ongoing): If you can't win on total compensation, win on something else. What can you offer that FAANG can't? Impact per person (smaller team, more ownership), learning opportunity (harder problems, more autonomy), mission alignment, work-life balance, or career progression speed. Articulate this clearly in every interaction — candidates who choose you for the right reasons retain better. 4. Improve the closing experience (Per candidate): Assign a 'closing partner' (hiring manager or team member) for final-stage candidates. This person maintains warm contact between offer and acceptance: answers questions, arranges team meet-and-greets, shares exciting team updates. The close isn't the offer letter — it's the entire period until day one. 5. Build pipeline depth (Ongoing): The best way to handle candidate declines is to never depend on a single candidate. Maintain a sourcing pipeline through referrals, technical content, open source contributions, and conference presence. When one candidate declines, you should have another strong option within a week, not start sourcing from scratch.",
    "commonMistakes": "Matching counter-offers reactively instead of building a differentiated value proposition. Blaming candidates for making the wrong choice instead of fixing your process. Extending the interview process to 'be more thorough' when speed is the problem. Competing only on compensation when impact, growth, and culture matter more to many engineers. Not asking declined candidates why they declined.",
    "whatGoodLooksLike": "Offer acceptance rate above 80%. Time-to-offer under 2 weeks. Candidate experience feedback consistently positive even from those who decline. Pipeline depth ensures no single decline stalls hiring by more than a week. The LeadDev articles on hiring paradox and trends emphasize that the bar for talent is rising — the winners are teams that move fast, articulate their unique value, and treat hiring as a product, not a process. Calibration signal: \"Competitive hiring: 'Won 4 of 5 competing offers against FAANG by differentiating on career velocity and impact — all...\" (SIG-215).",
    "mappingNotes": "Based on LeadDev 'The great engineer hiring paradox', 'Rethinking your engineer hiring strategy 2024', and '6 engineering hiring trends 2025.'",
    "suggestedMetricIds": [
      "5.1",
      "5.2"
    ]
  },
  {
    "id": "P-C13-6",
    "slug": "your-vp-wants-engineering-to-own-security-but-nobody-has-expertise",
    "observableIds": [
      "C13-O5",
      "C13-O1",
      "C13-O7"
    ],
    "capabilityIds": [
      "C13",
      "C6"
    ],
    "title": "Your VP Wants Engineering to 'Own Security' But Nobody Has Expertise",
    "context": "Your VP has decided that security should shift left into engineering. There's no dedicated security team, no AppSec engineer, and nobody on your team has deep security expertise. A recent incident — a feature shipped with a known SQL injection vulnerability because 'security didn't review it in time' — has made this an urgent priority. You're expected to embed security into the development lifecycle, but your engineers see security as 'not my job' and compliance requirements are growing.",
    "topicsActivated": [
      "Security (Building Capability From Scratch)",
      "People (Security Champion Program)",
      "Process (Automated Security Integration)"
    ],
    "decisionFramework": "1. Start with automation, not training (Month 1): Don't begin by telling engineers to 'think about security more.' Begin by adding automated security scanning to your CI/CD pipeline — dependency scanning, SAST, secret detection. These tools catch the easy wins (known vulnerabilities, hardcoded secrets, common patterns) without requiring engineers to become security experts. The automation creates a baseline without adding cognitive load. 2. Designate security champions (Month 1-2): Identify 1-2 engineers per team who are curious about security. This isn't a full-time role — it's 10-15% of their time. Their job: review security scanner output, triage findings, learn enough to advise the team, and be the bridge to external security resources. Invest in their development — security training, conference attendance, cross-training with any security staff you do have. 3. Build threat modeling into design (Month 2-3): For any feature touching sensitive data, authentication, or external integrations, add a lightweight threat modeling step to the design phase. It doesn't have to be formal STRIDE analysis — start with three questions: 'What could go wrong? What's the worst case? How do we detect it?' This embeds security thinking without requiring deep expertise. 4. Establish severity-based response SLAs (Month 3): Define how quickly different severity vulnerabilities must be addressed: critical in 48 hours, high in 7 days, medium in 30 days. This gives the team clear expectations without asking them to prioritize security against every other demand subjectively. 5. Build the case for dedicated investment (Month 4+): After 3 months of data from automated scanning, you'll have visibility into your security posture that you didn't have before. Use this data to make the case for what you need next — whether that's a dedicated AppSec hire, a security contractor for audit prep, or a deeper investment in training.",
    "commonMistakes": "Starting with mandatory security training before engineers understand why it matters. Expecting engineers to become security experts overnight. Trying to implement everything at once (team will rebel). Making security a gate that blocks deploys without clear ownership of remediation. Not tuning scanning tools (alert fatigue kills adoption). Treating security as punishment rather than practice.",
    "whatGoodLooksLike": "Within 3 months: automated scanning catches vulnerabilities before production, security champions are triaging findings, and threat modeling happens for high-risk features. Within 6 months: vulnerability SLA compliance >90%, vulnerability backlog is trending down, security catches shifting left (found in development, not production), and the team has data to justify next security investment. Team sees security as 'how we work', not 'extra overhead'. Google's BeyondCorp model integrates security into every layer of the development and deployment pipeline, making security a structural property rather than an add-on review step. Calibration signal: \"Threat modeling practice: 'Required STRIDE threat models for auth and payment features — caught 5 design-level vulner...\" (SIG-213).",
    "mappingNotes": "Based on LeadDev 'Shifting left on security: Five steps to transformation' and 'Born-left security: The new approach taking over shift-left.'",
    "suggestedMetricIds": [
      "9.2",
      "3.5",
      "9.1"
    ]
  },
  {
    "id": "P-C12-7",
    "slug": "driving-culture-change-against-organizational-resistance",
    "observableIds": [
      "C12-O1",
      "C12-O2",
      "C12-O8"
    ],
    "capabilityIds": [
      "C12"
    ],
    "title": "Driving Culture Change Against Organizational Resistance",
    "context": "You've been hired or promoted to improve engineering culture, but the broader organization actively resists change. Legacy norms, hostile leadership, or institutional inertia work against your efforts. Your skip-level doesn't see culture as a priority, peers are indifferent or defensive, and your team is cynical from failed past initiatives. You believe the culture needs to change, but you lack the organizational power to mandate it.",
    "topicsActivated": [
      "Culture (Grassroots Change Strategy)",
      "Stakeholder (Coalition Building)",
      "People (Team as Microculture)"
    ],
    "decisionFramework": "1. Assess the landscape honestly (Week 1-2): Before trying to change anything, understand the system you're in. Map the power dynamics: who benefits from the current culture, who suffers, who has influence. Identify the 2-3 most damaging cultural norms and the 2-3 people who might be natural allies. Be honest about your scope of influence — you can likely change your team's microculture, you may be able to influence your org, and you probably cannot change the company alone. 2. Build your team as a proof-of-concept (Month 1-2): Create the culture you want within your team first. Establish explicit norms (team charter), model the behavior you want to see, and protect your team from the worst of the broader culture. When your team starts outperforming, you have evidence, not just arguments. This is 'seeding' — growing something healthy in a controlled environment. 3. Build coalitions, not crusades (Month 2-4): Find 2-3 peer managers who share your concerns. Start informal alliances — shared rituals, cross-team norms, joint retros. Coalition-building is slower than top-down mandates but more durable. A single manager asking for change is a complaint; five managers asking together is a movement. 4. Choose your battles deliberately (Ongoing): You cannot fix everything. Pick the 1-2 cultural changes that would have the most impact and are most achievable given your influence. Focus your energy there. Let smaller issues go — fighting every battle guarantees you win none. The battles worth fighting are the ones that affect psychological safety and retention. 5. Know your limits and your exit criteria (Ongoing): Not every culture can be saved from inside. Set a personal timeline: if after 6-12 months of genuine effort, the organizational culture hasn't improved or has gotten worse, consider whether staying serves your team or just enables the dysfunction. The hardest leadership decision is sometimes 'I cannot fix this from here.' That's not failure — it's clarity.",
    "commonMistakes": "Going it alone without building coalitions — the 'lone crusader' burns out. Trying to change everything at once instead of focusing on 1-2 high-impact norms. Expecting organizational culture to change on your timeline rather than building incrementally. Confusing protecting your team with isolating them — your team needs to function within the broader org, not as an island. Staying too long in a genuinely toxic environment out of misplaced loyalty or sunk cost.",
    "whatGoodLooksLike": "Your team has a visibly healthier culture than the surrounding org. Peer managers start asking how you did it and adopting similar practices. At least one cultural norm you championed spreads beyond your team. You have honest clarity about what you can and cannot change. If you leave, the cultural improvements you built persist because they're embedded in team practices, not dependent on your presence. Jason Wong's LeadDev talk on culture change in hostile environments captures the key insight: know the limits of what you can change, and invest your energy where it can actually make a difference. Calibration signal: \"EMs who avoid conflict are a red flag in calibration\" (SIG-044).",
    "mappingNotes": "Based on LeadDev 'Culture change in a hostile environment' (Jason Wong, ActBlue). Fills a genuine gap — existing C12 playbooks assume cooperative organizational context.",
    "suggestedMetricIds": [
      "6.1",
      "6.2"
    ]
  }
]