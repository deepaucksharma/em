[
  {
    "id": "P-C6-1",
    "slug": "your-top-performer-gives-2-weeks-notice",
    "observableIds": [
      "C6-O3",
      "C6-O4"
    ],
    "capabilityIds": [
      "C6"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Your Top Performer Gives 2 Weeks Notice",
    "context": "Your best senior engineer has an offer from a competitor. They seem decided but are giving you a courtesy conversation.",
    "topicsActivated": [
      "Team Health (Retention)",
      "Performance (Managing High Performers)",
      "Stakeholder Mgmt (Delivering Bad News)"
    ],
    "decisionFramework": "1. Listen: understand root cause (comp? growth? culture? manager?). 2. Assess: is this fixable? Be honest with yourself. 3. If fixable: make specific, genuine offer (not panicked counter). 4. If not fixable: wish them well, plan transition. 5. Inform your manager immediately. 6. Plan knowledge transfer. 7. Communicate to team. 8. Post-mortem: what could you have done differently?",
    "commonMistakes": "Panicked counter-offer that doesn't address root cause (buys 6 months at best). Guilt-tripping. Not telling your manager immediately. Not planning knowledge transfer. Ignoring the signal for the rest of the team.",
    "whatGoodLooksLike": "Within 24 hours: root cause conversation completed; honest assessment of fixability. Within 48 hours: if counter-offering — specific offer that addresses root cause, approved by leadership; if not fixable — graceful exit with knowledge transfer plan started; team communicated. Within 1 week: retrospective on retention practices — what signals did you miss? Measured by: voluntary attrition rate (metric 7.1), retention rate for top performers (metric 7.2).",
    "mappingNotes": "Top performer retention scenario",
    "suggestedMetricIds": [
      "7.1",
      "7.2",
      "5.1"
    ]
  },
  {
    "id": "P-C8-1",
    "slug": "major-production-incident-during-your-vacation",
    "observableIds": [
      "C8-O1",
      "C8-O2",
      "C8-O4"
    ],
    "capabilityIds": [
      "C8"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Major Production Incident During Your Vacation",
    "context": "You're on vacation. A Sev1 incident fires and your team is handling it. Your manager pings you asking for status.",
    "topicsActivated": [
      "Operational Risk (Incident Response)",
      "Team Health (Trust)",
      "Stakeholder Mgmt (Managing Up)"
    ],
    "decisionFramework": "1. Check: is the team handling it? If yes, trust them. 2. If team is struggling: provide guidance, don't take over. 3. Respond to manager with what you know. 4. Post-vacation: review incident response. 5. If team handled it well: celebrate. If not: identify gaps for training/process/runbooks.",
    "commonMistakes": "Taking over incident from vacation (undermines team, proves you haven't delegated). Going completely dark when your manager needs a response. Blaming team for not handling it perfectly. Using it as evidence that you can't take vacation.",
    "whatGoodLooksLike": "Within 5 minutes: on-call IC contacted and confirmed; EM notified but not required to take command. Within 1 hour: incident mitigated by on-call team using existing runbooks. Within 24 hours: EM reviews incident response quality and coaches on gaps. Outcome: team handles incidents independently; EM's vacation proves the process works. Measured by: MTTR during EM absence vs. presence, team confidence survey.",
    "mappingNotes": "Production incident during vacation",
    "suggestedMetricIds": [
      "1.3",
      "3.5",
      "3.3",
      "3.6"
    ]
  },
  {
    "id": "P-C5-1",
    "slug": "pm-wants-to-ship-a-feature-you-know-will-create-tech-debt",
    "observableIds": [
      "C5-O1",
      "C3-O4"
    ],
    "capabilityIds": [
      "C5",
      "C3"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "PM Wants to Ship a Feature You Know Will Create Tech Debt",
    "context": "PM has committed to a deadline with stakeholders. Meeting the deadline requires cutting corners that will cost the team months of debt later.",
    "topicsActivated": [
      "Cross-Functional Partnership (PM Disagreement)",
      "Decision Making (Trade-offs)",
      "Strategic Alignment (Trade-off Communication)"
    ],
    "decisionFramework": "1. Quantify the debt: specifically what gets cut, what's the future cost (hours, risk, incidents). 2. Present options: (A) Ship on time with known debt + remediation plan, (B) Extend deadline by X to do it right, (C) Reduce scope to hit deadline without debt. 3. Make trade-off PM's decision with full information. 4. If PM chooses debt: document the plan to pay it back. 5. Never: passive-aggressively build it 'right' and miss deadline.",
    "commonMistakes": "Just saying 'no' without alternatives. Passive compliance then complaining about tech debt later. Building it your way and missing the deadline. Not quantifying the actual cost of debt. Escalating without first trying to align directly with PM.",
    "whatGoodLooksLike": "Within 1 day: 3 clear options with honest trade-offs presented. Within 1 week: PM makes informed decision; if debt is accepted, remediation plan is written with capacity allocated and payback timeline agreed. Ongoing: no resentment — it was a joint decision with eyes open; all trade-offs documented in writing, not just discussed. Measured by: tech debt ratio (metric 8.6), engineering investment mix (metric 8.4).",
    "mappingNotes": "PM tech debt conflict",
    "suggestedMetricIds": [
      "8.6",
      "8.4",
      "1.2"
    ]
  },
  {
    "id": "P-C12-1",
    "slug": "two-of-your-engineers-are-in-conflict",
    "observableIds": [
      "C12-O1",
      "C12-O2"
    ],
    "capabilityIds": [
      "C12"
    ],
    "relevantLevels": [
      1,
      2,
      3
    ],
    "title": "Two of Your Engineers Are in Conflict",
    "context": "Two senior engineers disagree on a technical approach. It's gotten personal — code reviews are hostile, standup is tense, team morale is affected.",
    "topicsActivated": [
      "Team Health (Constructive Conflict, Psych Safety)",
      "Performance (Feedback)",
      "Engineering Culture (Inclusive Environment)"
    ],
    "decisionFramework": "1. Talk to each privately: understand their perspective and concerns. 2. Separate the technical from the personal. 3. Resolve the technical disagreement: use RFC process, get external reviewer, make a decision. 4. Address the personal: specific feedback to each on behavior (not opinions). 5. Mediated conversation if needed. 6. Clear expectations going forward. 7. Follow up in 2 weeks.",
    "commonMistakes": "Ignoring it ('they'll work it out'). Taking sides publicly. Making it about who's right technically while ignoring interpersonal damage. Punishing both equally. Moving one to another team without addressing the root cause.",
    "whatGoodLooksLike": "Within 1 week: technical decision made through structured process (not by who argues louder); both engineers receive specific, private feedback on behavior. Within 2 weeks: both acknowledge impact on team; clear working agreement going forward. At 4 weeks: follow-up confirms improvement; team morale recovers. Key principle: how teams handle conflict is a key differentiator of effective teams — structured coaching through interpersonal conflict without taking sides. Measured by: team engagement (metric 5.1), psychological safety survey.",
    "mappingNotes": "Engineer conflict resolution",
    "suggestedMetricIds": [
      "7.9",
      "5.1",
      "7.8"
    ]
  },
  {
    "id": "P-C1-1",
    "slug": "you-inherit-a-low-performing-team",
    "observableIds": [
      "C1-O8",
      "C1-O7",
      "C6-O2"
    ],
    "capabilityIds": [
      "C1",
      "C6"
    ],
    "relevantLevels": [
      1,
      2,
      3
    ],
    "title": "You Inherit a Low-Performing Team",
    "context": "You join as EM. In your first month you discover: 2 engineers should probably be on a PIP, the team's metrics are worst in the org, morale is low, and the previous EM left because of burnout.",
    "topicsActivated": [
      "First 90 Days (Assessment, Credibility)",
      "Performance (Underperformance, PIPs)",
      "Team Health (Psych Safety)",
      "Stakeholder Mgmt (Managing Up)"
    ],
    "decisionFramework": "1. Month 1: Listen and assess. Build trust. Understand WHY performance is low (people? process? scope? leadership?). 2. Month 2: Quick wins (fix one obvious pain point). Start 1:1 cadence. Begin addressing most critical performance gap. 3. Month 3: Comprehensive plan to leadership — honest assessment + proposed path. Begin formal performance processes where needed. 4. Communicate to team: 'here's what I've observed, here's what we're going to change, here's how I'll support you.'",
    "commonMistakes": "Making changes in week 1 before understanding context. Putting people on PIP immediately (you weren't there, you don't have evidence yet). Blaming previous EM publicly. Promising everything will be better without a plan. Sugar-coating the situation to leadership.",
    "whatGoodLooksLike": "Within 30 days: honest assessment delivered to leadership (with plan, not just problems). Within 60 days: trust built through listening and 2-3 quick wins on acknowledged pain points. Within 90 days: measurable improvement in team metrics; performance conversations started with clear expectations and genuine support; team feels heard and sees a path forward.",
    "mappingNotes": "Inherit low-performing team",
    "suggestedMetricIds": [
      "5.1",
      "7.1",
      "7.9",
      "1.2",
      "1.4"
    ]
  },
  {
    "id": "P-C1-2",
    "slug": "layoffs-are-coming-and-you-know-before-your-team",
    "observableIds": [
      "C1-O4",
      "C7-O3"
    ],
    "capabilityIds": [
      "C1",
      "C7"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Layoffs Are Coming and You Know Before Your Team",
    "context": "Your Director told you confidentially: 20% RIF next week. You know who's affected on your team. You can't tell anyone yet.",
    "topicsActivated": [
      "Team Health (Leading Through Layoffs)",
      "Stakeholder Mgmt (Managing Up)",
      "Engineering Culture (Trust)"
    ],
    "decisionFramework": "1. Prepare: talking points for affected and remaining team members. 2. Coordinate: logistics (access revocation, equipment, benefits information). 3. Day of: tell affected people first, in private, with humanity. 4. Same day: address remaining team honestly. 5. Following weeks: over-communicate stability, re-plan roadmap, address survivor guilt. 6. Don't pretend nothing happened.",
    "commonMistakes": "Leaking the news early. Being cold or scripted with affected people. Not being available for remaining team. Immediately jumping to 'let's focus on the work' without acknowledging grief. Not re-planning the roadmap (pretending you can do the same work with fewer people).",
    "whatGoodLooksLike": "Day 1: affected people treated with dignity — they remember how they were treated. Within 1 week: 1:1s with every remaining team member completed; remaining team hears honesty about what happened, what's changing, what's not. Within 2 weeks: roadmap re-planned publicly (team sees you're realistic). Within 1 quarter: zero additional voluntary attrition. Measured by: voluntary attrition rate post-event, team sentiment survey, roadmap delivery against revised plan.",
    "mappingNotes": "Layoffs incoming — pre-disclosure",
    "suggestedMetricIds": [
      "7.1",
      "5.1",
      "7.9"
    ]
  },
  {
    "id": "P-C5-2",
    "slug": "your-new-vp-wants-to-change-everything",
    "observableIds": [
      "C5-O10",
      "C1-O4"
    ],
    "capabilityIds": [
      "C5",
      "C1"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Your New VP Wants to Change Everything",
    "context": "New VP joins. They want to re-org, change tech stack, adopt new processes, and reset all roadmaps. Your team is anxious.",
    "topicsActivated": [
      "Team Health (New Leadership, Managing Through Adversity)",
      "Stakeholder Mgmt (Managing Up)",
      "Strategic Alignment (Planning)"
    ],
    "decisionFramework": "1. Listen to VP's vision — understand the intent behind changes. 2. Identify: which changes are reasonable? Which are destructive? 3. Push back constructively on changes that would hurt (with data). 4. Shield team from unnecessary churn while supporting legitimate changes. 5. Communicate to team: what's changing, what's not, what you're advocating for. 6. Build trust with new VP through early results.",
    "commonMistakes": "Immediately complying with everything (your team suffers). Immediately resisting everything (you get replaced). Badmouthing the VP to your team. Letting team anxiety fester without addressing it. Not adapting at all to new leadership's style.",
    "whatGoodLooksLike": "Within 30 days: VP trust earned through early results on their priorities. Within 60 days: team protected from unnecessary churn; legitimate changes well-communicated and well-executed. Within 6 months: VP trusts you, team trusts you, things are stable. Outcome: team sees you as their advocate who also adapts to new reality. Measured by: developer satisfaction (metric 5.1), OKR achievement rate (metric 8.1).",
    "mappingNotes": "New VP changes everything",
    "suggestedMetricIds": [
      "1.2",
      "3.5",
      "5.1",
      "8.4"
    ]
  },
  {
    "id": "P-C6-2",
    "slug": "you-discover-a-senior-engineer-is-quietly-job-searching",
    "observableIds": [
      "C6-O3",
      "C6-O8"
    ],
    "capabilityIds": [
      "C6"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "You Discover a Senior Engineer Is Quietly Job Searching",
    "context": "Through the grapevine (or obvious calendar gaps), you realize one of your senior engineers is interviewing elsewhere. They haven't told you.",
    "topicsActivated": [
      "Team Health (Flight Risk, Stay Interviews)",
      "Performance (Managing High Performers, Career Conversations)"
    ],
    "decisionFramework": "1. Don't confront them about job searching (it's their right). 2. Create opportunity for honest career conversation: 'How are you feeling about your growth here? What would make the next year exciting for you?' 3. Listen for the underlying need. 4. If you can address it: propose specific changes (scope, comp, growth path). 5. If you can't: be honest about constraints. 6. Either way: plan for potential departure (knowledge transfer, succession).",
    "commonMistakes": "Confronting them directly about interviewing (violation of trust). Panic counter-offer out of nowhere. Ignoring the signals hoping they'll stay. Punishing them with reduced access or scope. Telling their teammates.",
    "whatGoodLooksLike": "Within 1 week: proactive career conversation that feels natural, not surveillance-triggered; underlying needs identified. Within 2 weeks: if addressable — specific changes proposed (scope, comp, growth path); if not — departure planning started quietly. Outcome: if they stay, it's because root cause was addressed; if they leave, you had time to plan. Either way: trust preserved. Measured by: voluntary attrition rate (metric 7.1), manager effectiveness score (metric 7.8).",
    "mappingNotes": "Senior engineer quietly job searching",
    "suggestedMetricIds": [
      "7.1",
      "5.1",
      "7.8"
    ]
  },
  {
    "id": "P-C3-1",
    "slug": "your-team-needs-to-deliver-a-critical-migration-under-tight-deadline",
    "observableIds": [
      "C3-O6",
      "C4-O2"
    ],
    "capabilityIds": [
      "C3",
      "C4"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Your Team Needs to Deliver a Critical Migration Under Tight Deadline",
    "context": "Leadership committed to migrating off a legacy system in Q3. Your team owns it. The deadline is real (vendor contract expiration). Scope is larger than estimated.",
    "topicsActivated": [
      "Technical Strategy (Migration Strategy)",
      "Engineering Excellence (Delivery Practices)",
      "Stakeholder Mgmt (Trade-off Communication, Delivering Bad News)",
      "Team Health (Capacity Planning)"
    ],
    "decisionFramework": "1. Re-estimate honestly: what's actually required? What's the gap vs. plan? 2. Present options: (A) Full migration by deadline with X% overtime risk, (B) Phased migration — critical paths by deadline, remaining by Q4, (C) Full migration with additional resources. 3. Communicate gap EARLY — don't wait until deadline is missed. 4. If proceeding under pressure: protect team health (no sustained overtime), cut scope aggressively, track weekly.",
    "commonMistakes": "Hiding the gap ('we'll figure it out'). Asking team to work overtime for months. Promising delivery you know isn't realistic. Not flagging the risk until it's a crisis. Blaming the estimate instead of managing the situation.",
    "whatGoodLooksLike": "Within 1 week: risk flagged with clear options document. Within 2 weeks: leadership approves trade-off (scope, timeline, or resources). Ongoing: weekly progress visible to stakeholders; team executes at sustainable pace. Outcome: if phased, phase 1 delivered on deadline with clear plan for phase 2; team emerges without burnout. Measured by: deployment frequency (metric 1.2), change failure rate (metric 1.4), team burnout signals (metric 2.5).",
    "mappingNotes": "Critical migration under deadline",
    "suggestedMetricIds": [
      "1.2",
      "1.4",
      "2.5",
      "3.5"
    ]
  },
  {
    "id": "P-C14-1",
    "slug": "calibration-committee-wants-to-downgrade-your-engineers-rating",
    "observableIds": [
      "C14-O2",
      "C14-O3"
    ],
    "capabilityIds": [
      "C14"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Calibration Committee Wants to Downgrade Your Engineer's Rating",
    "context": "You submitted 'Exceeds Expectations' for a strong senior engineer. In calibration, peer managers are pushing for 'Meets' based on cross-team comparison. You disagree.",
    "topicsActivated": [
      "Performance (Calibration Preparation, Ratings Distribution)",
      "Stakeholder Mgmt (Influence Without Authority)"
    ],
    "decisionFramework": "1. Present evidence calmly: specific impact examples, peer feedback, cross-team comparison data you prepared. 2. Listen to counter-arguments: is there a valid point you're missing? 3. If your case is strong: hold firm with evidence. 4. If their case has merit: gracefully accept and adjust. 5. Either way: communicate outcome to your engineer honestly. 6. Never: throw the committee under the bus.",
    "commonMistakes": "Caving without defending your case. Getting emotional or combative. Not preparing comparison data in advance. Telling your engineer 'I fought for you but they overruled me' (blames the system, undermines trust in the process). Inflating evidence to win the argument.",
    "whatGoodLooksLike": "Strong, evidence-based case that even skeptics find credible. If you win: earned through preparation, not politics. If you lose: you understand why and can explain it honestly to your engineer. Your credibility in the room is intact or strengthened regardless of outcome. Within 1 cycle: calibration cases include 3+ specific examples per rating dimension with pre-built counter-arguments grounded in observable behavior. Measured by: rating revision rate in calibration (metric 5.1), calibration credibility over time (metric 7.8).",
    "mappingNotes": "Calibration downgrade fight",
    "suggestedMetricIds": [
      "5.1",
      "7.8"
    ]
  },
  {
    "id": "P-C1-3",
    "slug": "a-cross-team-initiative-is-stalled-because-no-one-owns-it",
    "observableIds": [
      "C1-O2",
      "C5-O7"
    ],
    "capabilityIds": [
      "C1",
      "C5"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "A Cross-Team Initiative Is Stalled Because No One Owns It",
    "context": "An important cross-org project has been discussed for months but nobody is driving it. Multiple teams would benefit but each expects the other to lead.",
    "topicsActivated": [
      "Cross-Functional (Dependency Negotiation)",
      "Stakeholder Mgmt (Driving Alignment, Political Capital)",
      "Decision Making (First Principles)"
    ],
    "decisionFramework": "1. Assess: should your team own this? (Impact on your goals, capacity, strategic value) 2. If yes: volunteer to drive. Write a proposal with scope, timeline, resource needs. 3. Get buy-in from other teams (pre-wire 1:1). 4. Establish DACI. 5. Drive to completion with regular cross-team syncs. 6. If no: identify who should own it and facilitate that conversation.",
    "commonMistakes": "Waiting for someone else to step up (it's been months — they won't). Volunteering without capacity (good intentions, failed execution). Trying to drive without explicit buy-in from participating teams. Not getting DACI clear upfront.",
    "whatGoodLooksLike": "Within 1 week: single-threaded owner identified (you or someone else) with explicit DACI roles. Within 2 weeks: cross-team execution plan with milestones shared with all teams. Within 1 quarter: initiative ships; your role visible to leadership. Measured by: initiative completion, cross-team dependency resolution time, leadership visibility of your contribution.",
    "mappingNotes": "Stalled cross-team initiative",
    "suggestedMetricIds": [
      "2.6",
      "1.2",
      "8.1"
    ]
  },
  {
    "id": "P-C14-2",
    "slug": "your-manager-asks-you-to-rate-everyone-meets-expectations",
    "observableIds": [
      "C14-O3",
      "C14-O4"
    ],
    "capabilityIds": [
      "C14"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Your Manager Asks You to Rate Everyone 'Meets Expectations'",
    "context": "Your Director says the org needs to compress ratings this cycle. They want fewer 'Exceeds' ratings. You have 3 engineers who genuinely exceeded.",
    "topicsActivated": [
      "Performance (Ratings Distribution, Delivering Feedback)",
      "Stakeholder Mgmt (Managing Up)",
      "Decision Making (First Principles)"
    ],
    "decisionFramework": "1. Understand the constraint: is this a hard mandate or guidance? What's driving it? 2. Present your strongest case: 'I can compress 2 of the 3, but this one engineer delivered [X] — here's the evidence.' 3. If Director insists across the board: push back with data one more time, clearly. 4. If overruled: you own the outcome — deliver feedback honestly to your engineers. 5. Never: blame your Director to your team.",
    "commonMistakes": "Silently complying and giving unfair ratings. Openly fighting your Director in calibration (burning political capital). Telling your engineers 'I wanted to give you Exceeds but my boss said no.' Distributing the pain randomly instead of based on evidence.",
    "whatGoodLooksLike": "You advocate clearly with evidence. You compress where reasonable and hold firm where it matters most. If compromised: you deliver honest feedback to affected engineers ('here's what you accomplished, here's the context for the rating, here's your growth path'). Trust preserved in both directions. Within 1 week: affected engineers receive honest context for their ratings with clear growth paths. Measured by: employee fairness perception in post-review survey (metric 7.1), retention of high performers (metric 7.9).",
    "mappingNotes": "Forced ratings compression",
    "suggestedMetricIds": [
      "7.1",
      "7.9",
      "5.1"
    ]
  },
  {
    "id": "P-C11-1",
    "slug": "you-need-to-build-a-new-team-from-scratch",
    "observableIds": [
      "C11-O1",
      "C11-O2",
      "C11-O4"
    ],
    "capabilityIds": [
      "C11"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "You Need to Build a New Team From Scratch",
    "context": "You've been given headcount to build a new team for a greenfield project. You have 6 months to hire 6 engineers and deliver first milestone.",
    "topicsActivated": [
      "Talent Acquisition (Hiring Loops, Bar, Onboarding)",
      "Org Design (Team Structure)",
      "Technical Strategy (Vision)",
      "Team Health (First 90 Days)"
    ],
    "decisionFramework": "1. First: define team mission, scope, and technical vision (BEFORE hiring). 2. Hire tech lead first (partner on architecture and subsequent hiring). 3. Hiring loop: calibrated, structured, bar raiser. Don't lower bar for speed. 4. Onboard in waves (don't hire all 6 simultaneously). 5. Establish team charter, principles, and rituals from day 1. 6. First milestone should be deliberately achievable — team needs an early win.",
    "commonMistakes": "Hiring all 6 simultaneously (onboarding chaos). Not defining technical direction before hiring (building a team without a mission). Hiring fast at the expense of quality. Not establishing culture intentionally (letting it happen by accident). Setting unrealistic first milestone.",
    "whatGoodLooksLike": "Within 1 month: tech lead hired and contributing to architecture and hiring decisions. Within 3 months: engineers hired in waves of 2, each wave onboarded before next; team charter established. Within 6 months: functioning, cohesive team with clear identity; first milestone delivered on time. Key principle: small autonomous teams with single-threaded leadership; structured onboarding with mentorship pairing and progressive ramp-up milestones. Measured by: time-to-productivity (metric 6.3), team engagement (metric 5.1).",
    "mappingNotes": "Build team from scratch",
    "suggestedMetricIds": [
      "7.3",
      "7.4",
      "7.5"
    ]
  },
  {
    "id": "P-C6-3",
    "slug": "one-of-your-ems-is-struggling-director-scenario",
    "observableIds": [
      "C6-O6",
      "C6-O7"
    ],
    "capabilityIds": [
      "C6"
    ],
    "relevantLevels": [
      4,
      5
    ],
    "title": "One of Your EMs Is Struggling (Director Scenario)",
    "context": "One of your EMs is underperforming: their team's metrics are declining, skip-level feedback is concerning, and the EM seems overwhelmed.",
    "topicsActivated": [
      "Managing Managers (Coaching EMs, Skip-Levels)",
      "Performance (Underperformance)",
      "Team Health (First 90 Days for the EM's team)"
    ],
    "decisionFramework": "1. Diagnose the root cause first — don't assume: is this a skill gap (can't do it), will gap (won't do it), or situation gap (set up to fail — wrong scope, inadequate resources, inherited dysfunction)? Each requires a fundamentally different intervention. 2. If skill gap: build a structured coaching plan with specific milestones — weekly 1:1s focused on the exact skills needed (e.g., running effective team meetings, giving direct feedback, technical credibility). See P-C6-4 for detailed coaching frameworks. 3. If will gap: have a direct, honest conversation about expectations and mutual fit. Some people don't want to be managers — creating a path back to IC can be a win for everyone. 4. If situation gap: fix the situation before judging the person. Reduce scope, add a TL, remove a toxic team member, clarify expectations with your leadership. 5. Set a clear 60-90 day improvement window with explicit success criteria. Document what improvement looks like so both of you can measure it objectively.",
    "commonMistakes": "Ignoring it hoping they'll figure it out — the EM's team suffers while you wait. Taking over their responsibilities yourself — they never develop and you become the bottleneck. Not diagnosing the root cause (skill/will/situation) and applying the wrong intervention. Waiting too long to act while the team's best engineers start interviewing elsewhere. Replacing them without any genuine coaching attempt, which signals to your other EMs that they have no safety net.",
    "whatGoodLooksLike": "Within 1 week: honest root-cause diagnosis (skill/will/situation) shared with the EM; specific coaching plan with measurable milestones. Within 60 days: if skill gap — EM shows measurable improvement on targeted behaviors; if situation gap — environment fixed and EM thriving; if will gap — managed transition with dignity, team protected throughout, replacement ready. Ongoing: use structured skip-level feedback (semi-annually) to identify struggling managers early; apply same coaching rigor to EMs as to engineers, using written feedback with specific behavioral examples. Measured by: team engagement scores (metric 5.1), attrition rate (metric 7.1), manager effectiveness score (metric 7.9).",
    "mappingNotes": "Struggling EM (Director scenario)",
    "suggestedMetricIds": [
      "5.1",
      "7.1",
      "7.9",
      "1.2"
    ]
  },
  {
    "id": "P-C8-2",
    "slug": "production-outage-caused-by-your-teams-deploy",
    "observableIds": [
      "C8-O1",
      "C8-O4"
    ],
    "capabilityIds": [
      "C8"
    ],
    "relevantLevels": [
      1,
      2,
      3,
      4
    ],
    "title": "Production Outage Caused by Your Team's Deploy",
    "context": "Your team deployed a change that caused a major production incident. Customers are affected. Leadership is asking what happened.",
    "topicsActivated": [
      "Operational Risk (Incident Response, Post-Mortems)",
      "Stakeholder Mgmt (Delivering Bad News, Status Communication)",
      "Engineering Excellence (Release Management, Testing)"
    ],
    "decisionFramework": "1. First: resolve the incident (rollback, mitigation). 2. Communicate: status updates to stakeholders every 30min during incident. 3. Take ownership immediately with leadership: 'our deploy caused this, here's what we're doing.' 4. Post-incident: blameless post-mortem within 48hrs. 5. Action items with owners and deadlines. 6. Prevention: what process/test/check would have caught this? 7. Communicate lessons learned broadly.",
    "commonMistakes": "Pointing fingers at the engineer who deployed. Hiding or minimizing the impact. Not communicating status during the incident. Post-mortem that's actually a blame session. Promising 'it won't happen again' without systemic changes.",
    "whatGoodLooksLike": "Within 15 minutes: incident detected and IC assigned. Within 1 hour: mitigation applied (rollback, feature flag, or fix-forward). Within 48 hours: blameless post-mortem completed with root cause identified. Within 2 weeks: all action items assigned, tracked, and >90% completed. Outcome: the deploy failure becomes a process improvement catalyst, not a blame event. Measured by: MTTR, post-mortem action completion rate, change failure rate trend.",
    "mappingNotes": "Outage caused by your deploy",
    "suggestedMetricIds": [
      "1.4",
      "1.3",
      "3.3",
      "3.5"
    ]
  },
  {
    "id": "P-C5-3",
    "slug": "youre-asked-to-take-on-scope-that-doesnt-fit-your-team",
    "observableIds": [
      "C5-O3",
      "C2-O2"
    ],
    "capabilityIds": [
      "C5",
      "C2"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "You're Asked to Take on Scope That Doesn't Fit Your Team",
    "context": "Your VP wants your team to own a new area that doesn't align with your team's expertise or mission. It's a political hot potato nobody else wants.",
    "topicsActivated": [
      "Org Design (Cognitive Load, Single-Threaded Ownership)",
      "Stakeholder Mgmt (Managing Up, Framing Asks)",
      "Decision Making (Saying No, Trade-offs)"
    ],
    "decisionFramework": "1. Understand the real ask: why your team? What's the actual need? 2. Assess honestly: can your team absorb this without degrading current work? What's the cost? 3. If no: propose alternatives (which team should own this? new team needed?). 4. If yes with conditions: 'we can take this if we deprioritize X or get Y headcount.' 5. Never: silently absorb scope that will crush your team.",
    "commonMistakes": "Accepting without understanding the cost. Declining without offering alternatives. Complaining to your team about leadership decisions. Absorbing it and burning out your team to prove you can handle it.",
    "whatGoodLooksLike": "Within 1 week: clear-eyed assessment presented to VP with specific trade-offs. If accepted: explicit agreement on deprioritized work or additional resources. If declined: alternative proposal that solves the VP's actual problem. Outcome: you're seen as thoughtful and solutions-oriented; teams own what they can cognitively manage, not what politics assigns. Measured by: engineering investment mix (metric 8.4), sprint commitment accuracy (metric 2.5).",
    "mappingNotes": "Asked to take misfit scope",
    "suggestedMetricIds": [
      "8.4",
      "2.5",
      "2.6"
    ]
  },
  {
    "id": "P-C5-4",
    "slug": "a-key-platform-dependency-is-unreliable-and-the-platform-team-wont-prioritize-fixes",
    "observableIds": [
      "C5-O3",
      "C5-O4"
    ],
    "capabilityIds": [
      "C5"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "A Key Platform Dependency Is Unreliable and the Platform Team Won't Prioritize Fixes",
    "context": "Your team's reliability is suffering because of an upstream platform dependency. The platform team acknowledges the issue but has higher priorities.",
    "topicsActivated": [
      "Cross-Functional (Dependency Negotiation, Platform Partnership)",
      "Technical Strategy (Dependency Management)",
      "Operational Risk (Dependency Risk)"
    ],
    "decisionFramework": "1. Quantify impact: how many incidents? Customer impact? Eng hours spent on workarounds? 2. Build relationship with platform EM (understand their constraints). 3. Propose mutual solutions: can you contribute a fix? Can you implement a fallback? 4. If blocked: escalate with data to Director level. 5. Parallel: build resilience against this dependency (circuit breakers, fallbacks, caching).",
    "commonMistakes": "Complaining without data. Escalating without first trying direct partnership. Building a competing solution in secret. Blaming the platform team publicly. Doing nothing and accepting the pain.",
    "whatGoodLooksLike": "Within 1 week: data-driven conversation with platform team; mutual problem-solving started. Within 2 weeks: fallback strategy implemented regardless of platform timeline. If escalation needed: both EMs present jointly to leadership with shared facts. Ongoing: relationship preserved; explicit SLOs and error budgets defined for platform dependencies, giving concrete data for future escalation. Measured by: SLO compliance rate (metric 3.5), lead time for changes (metric 1.2).",
    "mappingNotes": "Unreliable platform dependency",
    "suggestedMetricIds": [
      "3.5",
      "3.3",
      "1.2"
    ]
  },
  {
    "id": "P-C12-2",
    "slug": "your-team-has-zero-diversity-and-youre-told-to-fix-it",
    "observableIds": [
      "C12-O5",
      "C11-O3"
    ],
    "capabilityIds": [
      "C12",
      "C11"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Your Team Has Zero Diversity and You're Told to Fix It",
    "context": "HR/leadership points out your team is homogeneous. You're asked to 'improve diversity' in your next hires.",
    "topicsActivated": [
      "Talent Acquisition (Sourcing, Hiring Loops)",
      "Engineering Culture (Inclusive Environment)",
      "Decision Making (First Principles)"
    ],
    "decisionFramework": "1. Audit the full pipeline: where are candidates dropping off? (Sourcing? Screen? Onsite? Offer?) 2. Fix the pipeline, not the outcome: diverse sourcing channels, structured interviews with rubrics, diverse interview panels, bias training. 3. Fix retention too: is your team environment actually inclusive? Would diverse hires thrive? 4. Never: lower the bar or hire for optics. 5. This is a sustained effort, not a one-quarter initiative.",
    "commonMistakes": "Token hiring (one diverse hire into unwelcoming environment — they leave). Lowering the bar (insulting and counterproductive). Treating it as a checkbox exercise. Only fixing sourcing without fixing culture. Delegating entirely to recruiting team.",
    "whatGoodLooksLike": "Within 1 quarter: pipeline audited and fixed at each stage; interview process restructured with structured rubrics and diverse panels; culture assessed honestly. Ongoing: sustained effort over multiple quarters; diversity improves AND inclusion improves AND bar maintained; hiring committees review for bias patterns across interview cycles. Measured by: pipeline diversity metrics, inclusion survey scores, hiring bar consistency.",
    "mappingNotes": "Zero diversity fix mandate",
    "suggestedMetricIds": [
      "7.7",
      "5.1",
      "7.4"
    ]
  },
  {
    "id": "P-C7-1",
    "slug": "you-strongly-disagree-with-a-decision-made-above-you",
    "observableIds": [
      "C7-O1",
      "C7-O7"
    ],
    "capabilityIds": [
      "C7"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "You Strongly Disagree with a Decision Made Above You",
    "context": "Your Director/VP made a strategic decision you think is wrong. It directly affects your team's direction.",
    "topicsActivated": [
      "Stakeholder Mgmt (Managing Up)",
      "Decision Making (Reversible vs Irreversible, First Principles)",
      "Team Health (Trust)"
    ],
    "decisionFramework": "1. Understand the full context (you might not have it). Ask questions, not accusations. 2. If you still disagree after understanding context: make your case once, clearly, with data. 3. If overruled: disagree and commit. Execute fully. 4. Never: undermine the decision through passive resistance or complaining to your team. 5. If the decision fails: don't say 'I told you so.' Help fix it.",
    "commonMistakes": "Going along silently (your perspective was valuable and you withheld it). Fighting the same battle repeatedly after the decision is made. Telling your team 'I disagree but we have to do it' (undermines the decision and your leadership above). Sabotaging through half-hearted execution.",
    "whatGoodLooksLike": "Within 1 week: you made your case clearly and respectfully with data. After the decision: you commit fully and execute as if it were your own idea; your team sees you supporting the direction. If the decision proves wrong: you help course-correct without blame. Outcome: your Director remembers you raised the concern constructively; silence is treated as agreement, so you spoke up when it mattered. Measured by: OKR achievement rate (metric 8.1), engineering investment mix (metric 8.4).",
    "mappingNotes": "Disagree with decision above you",
    "suggestedMetricIds": [
      "8.1",
      "8.2",
      "8.4"
    ]
  },
  {
    "id": "P-C5-5",
    "slug": "youre-preparing-for-your-own-promotion-to-director",
    "observableIds": [
      "C5-O5",
      "C5-O6",
      "C14-O2"
    ],
    "capabilityIds": [
      "C5",
      "C14"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "You're Preparing for Your Own Promotion to Director",
    "context": "You've been a strong EM for 2+ years and want to make the jump to Director. What do you actually need to demonstrate?",
    "topicsActivated": [
      "Performance (Promo Packets, Career Growth)",
      "Stakeholder Mgmt (Sponsor Relationships, Executive Communication)",
      "Managing Managers (all principles)",
      "Org Design (all principles)"
    ],
    "decisionFramework": "1. Map current scope to Director-level rubric — identify gaps honestly. 2. Build evidence of Director-level work: cross-team impact, org-level strategy, developing other managers. 3. Get a sponsor (not just a mentor) at VP+ level. 4. Demonstrate you can operate at Director altitude while still delivering as EM. 5. Your promo case should be obvious before you submit it.",
    "commonMistakes": "Asking for the title without doing the job. Focusing on managing your team well (necessary but not sufficient — Directors operate across teams). Not building sponsor relationships. Waiting for your manager to nominate you instead of driving your own career. Neglecting your current team while chasing Director scope.",
    "whatGoodLooksLike": "6+ months before promo discussion: operating at Director level with documented cross-team impact. At least one future EM developed. A sponsor at VP+ level who advocates for you. Your current team continues to thrive. Outcome: the promo feels like recognition of reality, not aspiration. Measured by: cross-team impact evidence, leadership scope beyond own team, EMs' independence under your guidance.",
    "mappingNotes": "Preparing own Director promotion",
    "suggestedMetricIds": [
      "1.2",
      "3.5",
      "7.1",
      "8.4",
      "8.2"
    ]
  },
  {
    "id": "P-C9-1",
    "slug": "dora-metrics-show-decline-but-team-feels-fine",
    "observableIds": [
      "C9-O1",
      "C9-O3"
    ],
    "capabilityIds": [
      "C9"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "DORA Metrics Show Decline But Team Feels Fine",
    "context": "Your DORA metrics have been trending down for 2 quarters — deploy frequency dropped 40%, lead time increased 3x. But team morale is fine and nobody is complaining. Your VP saw the dashboard and is asking questions.",
    "topicsActivated": [
      "Metrics Measurement (DORA analysis)",
      "Activity Metrics (diagnostic vs punitive)",
      "Stakeholder Mgmt (framing to VP)"
    ],
    "decisionFramework": "1. Investigate root cause: is it real degradation or measurement artifact (team grew, monolith split, deploy definition changed)? 2. Correlate with outcomes: are incidents up? Is velocity actually down? Customer impact? 3. If real: identify bottleneck (CI pipeline? review queue? test suite? environment contention?). 4. If artifact: fix the measurement, communicate context to VP. 5. Either way: present findings with context, not just numbers. 6. Create improvement plan with team ownership, not top-down targets.",
    "commonMistakes": "Setting DORA targets as team KPIs (Goodhart's Law). Gaming deploy frequency by splitting deploys. Presenting raw numbers to VP without context. Blaming the team instead of investigating systemic causes. Ignoring the signal because \"team feels fine.\"",
    "whatGoodLooksLike": "Within 1 week: stage-level decomposition of delivery pipeline completed. Within 2 weeks: specific bottleneck identified with data. Within 1 month: targeted intervention showing measurable improvement. Outcome: DORA metrics improving and team understands why. Measured by: stage-by-stage lead time, deployment frequency trend, team perception alignment with data.",
    "mappingNotes": "DORA decline investigation scenario — metrics as diagnostic",
    "suggestedMetricIds": [
      "1.2",
      "1.4",
      "1.1",
      "5.1",
      "2.5"
    ]
  },
  {
    "id": "P-C10-1",
    "slug": "budget-cut-lose-20-headcount-keep-critical-deliverables",
    "observableIds": [
      "C10-O1",
      "C10-O2",
      "C10-O5"
    ],
    "capabilityIds": [
      "C10"
    ],
    "relevantLevels": [
      4,
      5
    ],
    "title": "Budget Cut: Lose 20% Headcount, Keep Critical Deliverables",
    "context": "Company announces 20% headcount reduction. You have 25 engineers across 4 teams. You need to identify 5 positions to cut while maintaining delivery on your top-3 commitments. You have 2 weeks before the announcement.",
    "topicsActivated": [
      "Resource Allocation (headcount planning)",
      "Strategic Prioritization (ruthless triage)",
      "People Management (layoff execution)",
      "Stakeholder Mgmt (managing up during crisis)"
    ],
    "decisionFramework": "1. Map all current work to business impact tiers: critical (revenue/compliance), important (strategic), nice-to-have (quality-of-life). 2. Identify what STOPS — be explicit, write it down. 3. Assess team composition: performance, criticality of knowledge, single points of failure. 4. Optimize for org survival: protect critical path, maintain minimum viable team per area. 5. Prepare talking points for affected individuals (with your HR partner). 6. Tell affected people FIRST, with humanity. 7. Re-plan roadmap with remaining team within 1 week. 8. Communicate revised plan to stakeholders within 2 weeks.",
    "commonMistakes": "Spreading cuts evenly across teams (peanut butter approach). Cutting based solely on performance ratings. Not communicating what stops. Protecting pet projects over business-critical work. Delaying the conversation with affected people. Not re-planning the roadmap after cuts.",
    "whatGoodLooksLike": "Within days: clear-eyed impact analysis delivered to leadership with explicit Tier 1/2/3 prioritization. Within 1 week: affected individuals told with dignity and support; remaining team has revised, achievable roadmap. Within 3-4 weeks: team productive with new scope; stakeholders know what's changing. Ongoing: zero regrettable attrition from survivors. Measured by: engineering investment mix (metric 8.4), engineering cost efficiency (metric 8.3), attrition rate (metric 7.1).",
    "mappingNotes": "Headcount reduction scenario — resource allocation under constraint",
    "suggestedMetricIds": [
      "8.4",
      "8.2",
      "7.1",
      "1.2"
    ]
  },
  {
    "id": "P-C13-1",
    "slug": "security-audit-finding-critical-vulnerability-in-production",
    "observableIds": [
      "C13-O1",
      "C13-O2",
      "C13-O4"
    ],
    "capabilityIds": [
      "C13"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Security Audit Finding: Critical Vulnerability in Production",
    "context": "Internal security audit found a critical vulnerability (e.g., unencrypted PII in logs, open S3 bucket, SQL injection vector) in your team's production service. Security team has given you a 72-hour remediation window. Your team is mid-sprint on a high-priority feature launch.",
    "topicsActivated": [
      "Security Compliance (vulnerability SLAs)",
      "Operational Risk (incident-adjacent)",
      "Stakeholder Mgmt (communicating priority shift)",
      "Strategic Prioritization (competing demands)"
    ],
    "decisionFramework": "1. Assess severity and blast radius immediately — what data is exposed, who is affected, what's the regulatory implication? 2. Communicate to your manager and stakeholders SAME DAY: feature timeline is shifting. 3. Assign your best engineer to the fix — this is not junior work. 4. Implement fix + write regression test + add to automated scanning to prevent recurrence. 5. Document root cause: how did this get to production? What process failed? 6. Close the loop with security team within SLA. 7. Post-fix: update threat model, add to security review checklist, share learning broadly.",
    "commonMistakes": "Treating it as security team's problem. Trying to fix it quietly without telling stakeholders about the feature delay. Assigning it to whoever is \"free\" instead of your most capable engineer. Fixing the symptom without addressing how it got to production. Not communicating the feature timeline impact upward.",
    "whatGoodLooksLike": "Within 72 hours: vulnerability patched; root cause documented. Within 1 week: process gap identified and closed (new automated check, updated review checklist); stakeholders informed proactively about feature delay. Ongoing: security team relationship strengthened; similar vulnerability never recurs. Key principle: use pre-defined severity tiers with SLA-based response timelines; mandatory security reviews catch vulnerabilities before launch, not after audit. Measured by: vulnerability remediation time (metric 9.1), change failure rate.",
    "mappingNotes": "Critical security finding response scenario",
    "suggestedMetricIds": [
      "9.1",
      "9.3",
      "9.2"
    ]
  },
  {
    "id": "P-C3-2",
    "slug": "your-teams-tech-stack-becomes-deprecated-company-wide",
    "observableIds": [
      "C3-O4",
      "C3-O6"
    ],
    "capabilityIds": [
      "C3"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Your Team's Tech Stack Becomes Deprecated Company-Wide",
    "context": "Platform team announces your primary framework/language is sunset. Migration timeline is 18 months. You have 100K lines of code in the deprecated stack, active feature development in flight, and no one on your team has deep experience with the target stack.",
    "topicsActivated": [
      "Technical Strategy (Migration Planning, Tech Debt)",
      "Team Health (Skill Development, Morale)",
      "Stakeholder Mgmt (Timeline Negotiation)",
      "Engineering Excellence (Incremental Migration)"
    ],
    "decisionFramework": "1. Assess blast radius: how much of your codebase is affected? What's the dependency graph? Which services are most critical? 2. Evaluate team readiness: who has experience with the target stack? What training is needed? Budget learning time explicitly. 3. Negotiate timeline: 18 months may be unrealistic for 100K LOC — present data-driven counter-proposal if needed. 4. Choose migration strategy: strangler fig (incremental) vs. big bang rewrite. Almost always choose strangler fig. 5. Create migration backlog: prioritize by risk (most critical services first) and coupling (least coupled first for early wins). 6. Run feature development and migration in parallel — do NOT freeze features for 18 months. 7. Establish migration metrics: % migrated, velocity per sprint, blockers. Report weekly to platform team and leadership. 8. Negotiate for dedicated migration capacity (at least 30% of team bandwidth).",
    "commonMistakes": "Attempting a big-bang rewrite (historically fails at 100K+ LOC scale). Freezing all feature work to focus on migration (business won't tolerate it). Not investing in team upskilling early (migration stalls in month 3 when nobody knows the new stack). Treating migration as purely technical without communicating business impact upward. Waiting until month 12 to flag that the timeline is unrealistic. Not negotiating for dedicated capacity — trying to squeeze migration into 'spare time.'",
    "whatGoodLooksLike": "Within 2 weeks: strangler fig migration plan published. Within 4 weeks: team upskilling started (pairing sessions, training budget). Within 6 weeks: first service migrated as proof of concept. Ongoing: feature development at 70% capacity; weekly migration metrics visible. Outcome: zero production incidents from migration; team has new skills and stronger architecture. Measured by: % services migrated per sprint, deployment frequency (metric 1.2), incident rate during migration, tech debt ratio (metric 8.6).",
    "mappingNotes": "Tech stack deprecation and large-scale migration planning",
    "suggestedMetricIds": [
      "8.4",
      "8.6",
      "1.2",
      "3.5"
    ]
  },
  {
    "id": "P-C1-4",
    "slug": "merging-two-teams-after-acquisition-re-org",
    "observableIds": [
      "C1-O3",
      "C1-O14"
    ],
    "capabilityIds": [
      "C1"
    ],
    "relevantLevels": [
      4,
      5
    ],
    "title": "Merging Two Teams After Acquisition/Re-org",
    "context": "Company acquired a startup. You're responsible for merging their 8-person team with your 12-person team. Different tech stacks (they use Python/Django, you use Java/Spring), different cultures (startup speed vs. enterprise process), different processes (they have no sprint planning, you run 2-week sprints with full ceremonies). Some role overlap exists. Both teams are anxious about who stays, who leads, and what changes.",
    "topicsActivated": [
      "Org Design (Team Mergers, Culture Integration)",
      "Team Health (Psychological Safety, Trust Building)",
      "Technical Strategy (Stack Consolidation)",
      "Stakeholder Mgmt (Managing Up, Managing Expectations)"
    ],
    "decisionFramework": "1. Week 1: Meet every person on the acquired team individually — understand their role, concerns, what they value about their current culture. Listen more than you talk. 2. Week 2: Identify overlaps and complementary skills. Map who owns what. Do NOT make org changes yet. 3. Weeks 3-4: Establish shared rituals — joint standup, shared Slack channel, cross-team pairing sessions. Let cultures cross-pollinate before forcing convergence. 4. Month 2: Make org structure decisions — be transparent about rationale. If roles overlap, decide based on skill and contribution, not 'which team they came from.' 5. Month 2-3: Consolidate processes gradually — take the best of both. Don't force the acquired team to adopt all your processes wholesale. 6. Months 3-6: Tech stack convergence plan — decide on target stack with input from both teams. This is a multi-quarter effort. 7. Throughout: over-communicate. Ambiguity breeds anxiety and attrition.",
    "commonMistakes": "Treating the acquired team as subordinate ('you're joining us, adopt our ways'). Making org changes in week 1 before understanding the people. Forcing immediate tech stack convergence (creates resentment and risk). Ignoring cultural differences and expecting instant alignment. Losing key acquired talent because they feel undervalued. Not addressing role overlap directly — letting it fester into political infighting. Assuming your processes are better because you're the acquirer.",
    "whatGoodLooksLike": "Within 2 weeks: every acquired team member feels heard through individual 1:1s. Within 1 month: org structure decisions made transparently with clear rationale; best practices adopted from BOTH teams. Within 3 months: tech stack convergence plan is data-driven with buy-in from both sides. Within 6 months: zero regrettable attrition from either team; shared team identity emerged; it's one team, not 'us and them.' Measured by: regrettable attrition within 90 days, time to full productivity (<4 weeks target), acquired team sentiment survey.",
    "mappingNotes": "Post-acquisition team merger and cultural integration",
    "suggestedMetricIds": [
      "1.2",
      "5.1",
      "7.1",
      "2.5"
    ]
  },
  {
    "id": "P-C8-3",
    "slug": "managing-through-a-major-security-incident-data-breach",
    "observableIds": [
      "C8-O1",
      "C8-O2",
      "C13-O1",
      "C13-O2"
    ],
    "capabilityIds": [
      "C8",
      "C13"
    ],
    "relevantLevels": [
      4,
      5
    ],
    "title": "Managing Through a Major Security Incident (Data Breach)",
    "context": "Security team discovers unauthorized data access to your team's service. Customer PII (names, emails, hashed passwords) may be compromised. The scope is unclear — could be hundreds or millions of records. Legal, PR, and the exec team are involved. Your team owns the affected service. The incident is already escalated to the CEO. Clock is ticking on regulatory notification requirements (72 hours under GDPR).",
    "topicsActivated": [
      "Operational Risk (Incident Command, War Room)",
      "Security Compliance (Breach Response, Regulatory Notification)",
      "Stakeholder Mgmt (Exec Communication, Cross-Functional Coordination)",
      "Team Health (Crisis Management, Protecting Engineers)"
    ],
    "decisionFramework": "1. Immediate (Hour 0-4): Establish incident command structure. Your role is EM-level incident commander for the technical response. Contain the breach — revoke compromised credentials, patch the vulnerability, preserve forensic evidence (do NOT destroy logs). 2. Hour 4-12: Scope the impact — how many records, what data types, what time period. Work with security team on forensic analysis. Provide regular updates to exec war room (every 2 hours). 3. Hour 12-48: Support legal team with technical facts for regulatory notification. Prepare customer-facing technical explanation (work with PR). Continue forensic analysis — understand attack vector completely. 4. Day 2-7: Implement permanent fix (not just patch). Conduct thorough security review of adjacent systems. Begin post-incident review. 5. Week 2-4: Full post-mortem with systemic improvements. Security architecture review. Team well-being check — breaches are extremely stressful. 6. Throughout: protect your engineers from exec pressure. They need to focus on the technical response, not fielding questions from 15 VPs.",
    "commonMistakes": "Destroying evidence in the rush to fix (deleting logs, wiping servers). Downplaying severity to leadership ('it's probably not that bad'). Letting every executive directly ping your engineers for status. Not involving legal immediately (regulatory clock is ticking). Promising a root cause before forensics are complete. Blaming the engineer who wrote the vulnerable code. Not checking on your team's well-being during and after the crisis. Treating the post-mortem as optional because 'we already fixed it.'",
    "whatGoodLooksLike": "Breach contained within hours, not days. Forensic evidence preserved from minute one. Exec team gets regular, honest updates (not optimistic guesses). Legal has what they need for regulatory notification within 72 hours. Customer communication is transparent and accurate. Root cause identified and systemic fix implemented (not just a patch). Security review of adjacent systems completed. Post-mortem produces architectural improvements that prevent this class of vulnerability. Your team is recognized for their response, not blamed for the breach. No one burns out from the crisis. Measured by: breach containment time, regulatory notification compliance (metric 9.1), post-incident architectural improvements, team well-being scores.",
    "mappingNotes": "Major security incident and data breach response",
    "suggestedMetricIds": [
      "9.1",
      "9.3",
      "1.3",
      "3.5"
    ]
  },
  {
    "id": "P-C6-4",
    "slug": "your-skip-level-reports-are-unhappy-with-their-em",
    "observableIds": [
      "C6-O6",
      "C6-O11"
    ],
    "capabilityIds": [
      "C6"
    ],
    "relevantLevels": [
      4,
      5
    ],
    "title": "Your Skip-Level Reports Are Unhappy With Their EM",
    "context": "In skip-level 1:1s, multiple engineers (3 out of 7) express frustration with their EM. Recurring themes: lack of career development conversations, micromanagement on technical decisions, poor communication about team priorities, and feeling unheard. The EM has been in role for 18 months and was promoted from within. Their self-assessment is that things are going well.",
    "topicsActivated": [
      "Managing Managers (Skip-Level Feedback, Coaching EMs)",
      "Performance (Underperformance, Feedback Delivery)",
      "Team Health (Psychological Safety, Trust)"
    ],
    "decisionFramework": "1. Validate the signal: are 3 engineers saying the same thing independently, or is this a clique? Are the themes consistent? Is there performance data that corroborates (retention risk, engagement survey scores, velocity trends)? 2. Do NOT relay skip-level feedback verbatim to the EM — that destroys skip-level trust. Instead, synthesize themes: 'I'm hearing signals that career development and communication could be stronger on your team.' 3. Have a direct coaching conversation with the EM: share observed themes (not attributing to specific people), ask for their perspective, listen for self-awareness. 4. Co-create a development plan: specific actions on career conversations (monthly with each report), delegation framework (decide which technical decisions they truly need to own vs. delegate), communication cadence (weekly team priorities update). 5. Set a 60-day checkpoint. Increase your own 1:1 cadence with this EM to weekly. 6. Follow up with skip-level reports in 30 and 60 days — is it improving? 7. If no improvement after 60 days with genuine coaching: this may be a performance conversation, not a coaching conversation.",
    "commonMistakes": "Sharing skip-level feedback directly with attribution ('Alex said you're micromanaging') — destroys skip-level trust permanently. Ignoring the signal because the EM's self-assessment is positive. Taking over the EM's responsibilities instead of coaching them. Moving engineers to other teams without addressing the root cause (the EM's behavior). Giving vague feedback ('you need to communicate better') without specific, actionable guidance. Waiting too long — 3 unhappy engineers become 5, then attrition starts.",
    "whatGoodLooksLike": "Within 1 week: EM receives synthesized, actionable feedback without knowing who said what; development plan has specific, measurable actions. Within 30 days: skip-level reports notice early improvement signals. Within 60 days: if EM improves — you developed a manager and built trust in both directions; if EM doesn't improve — you have documented pattern and can make a fair decision. Either way: skip-level reports trust that their feedback was heard and acted upon. Measured by: team engagement scores (metric 5.1), manager effectiveness score (metric 7.8), attrition rate (metric 7.1).",
    "mappingNotes": "Skip-level dissatisfaction with EM — coaching and intervention",
    "suggestedMetricIds": [
      "5.1",
      "7.8",
      "7.1"
    ]
  },
  {
    "id": "P-C10-2",
    "slug": "budget-is-frozen-but-commitments-remain",
    "observableIds": [
      "C10-O2",
      "C10-O5",
      "C2-O2"
    ],
    "capabilityIds": [
      "C10",
      "C2"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Budget Is Frozen But Commitments Remain",
    "context": "Mid-year budget freeze announced. No new hires (you had 4 open reqs), no new vendor contracts, limited travel and conference budget. But your Q3-Q4 commitments haven't been adjusted. Leadership expects delivery on the original roadmap. Your team is already at capacity, and two of the open reqs were for a critical new initiative that starts next quarter.",
    "topicsActivated": [
      "Resource Allocation (Constraint-Based Planning)",
      "Strategic Prioritization (Ruthless Triage)",
      "Stakeholder Mgmt (Managing Up, Resetting Expectations)",
      "Delivery Management (Doing More With Less)"
    ],
    "decisionFramework": "1. Immediate: map every commitment to required capacity (people-weeks). Total it up. Compare to available capacity without the 4 missing hires. Quantify the gap precisely (e.g., 'we're 35% short of committed capacity'). 2. Tier your commitments: Tier 1 (must deliver — revenue, contractual, compliance), Tier 2 (should deliver — strategic, high-value), Tier 3 (could deliver — quality-of-life, nice-to-have). 3. Present the gap to leadership with options: (A) Cut Tier 3 entirely and reduce Tier 2 scope — deliver Tier 1 on time. (B) Delay Tier 2 to Q1 next year — deliver Tier 1 + reduced Tier 3. (C) Deliver everything with explicit quality/reliability trade-offs (not recommended). 4. Force the prioritization decision upward — do not silently absorb an impossible scope. 5. Communicate to team: 'here's what changed, here's what we're focusing on, here's what we're NOT doing.' 6. Explore creative solutions: internal transfers, contractor budget (if not frozen), automation of manual work, scope reduction on individual projects.",
    "commonMistakes": "Silently accepting the original commitments and burning out your team trying to deliver. Not quantifying the capacity gap with data (just saying 'we can't do it all'). Spreading the team thin across everything instead of cutting scope decisively. Waiting until Q3 delivery misses to raise the alarm. Not exploring creative alternatives (internal mobility, automation, scope negotiation). Complaining to your team about leadership instead of actively managing upward.",
    "whatGoodLooksLike": "Within 1 week: capacity gap quantified and presented to leadership with tiered options. Within 2 weeks: team has clear, achievable plan — they know what they're doing and what they're NOT doing; creative alternatives explored. Ongoing: Tier 1 commitments delivered on time; team morale stable because expectations are realistic. Measured by: engineering investment mix (metric 8.4), engineering cost efficiency (metric 8.3), sprint commitment accuracy (metric 2.5), deployment frequency (metric 1.2).",
    "mappingNotes": "Budget freeze with unchanged commitments — constraint-based planning",
    "suggestedMetricIds": [
      "8.4",
      "8.3",
      "2.5",
      "1.2"
    ]
  },
  {
    "id": "P-C10-3",
    "slug": "leading-a-cost-optimization-initiative-finops",
    "observableIds": [
      "C10-O3",
      "C3-O9"
    ],
    "capabilityIds": [
      "C10",
      "C3"
    ],
    "relevantLevels": [
      4,
      5
    ],
    "title": "Leading a Cost Optimization Initiative (FinOps)",
    "context": "CFO mandates 30% cloud cost reduction across engineering. You're asked to lead the initiative for your org (4 teams, ~40 engineers). Current monthly spend is $2M across AWS services. No one has a clear picture of what's driving costs. Teams have never been held accountable for infrastructure spend. Finance wants a plan in 2 weeks and results within the quarter.",
    "topicsActivated": [
      "Financial Management (Cloud Cost Optimization, FinOps)",
      "Technical Strategy (Infrastructure Efficiency)",
      "Cross-Functional Partnership (Finance/Engineering Alignment)",
      "Stakeholder Mgmt (Driving Org-Wide Initiative)"
    ],
    "decisionFramework": "1. Week 1 — Visibility: Get cost breakdown by team, service, and environment. Tag everything. Identify top 10 cost drivers (usually 80/20 rule applies — a few services drive most spend). Distinguish production from non-production costs. 2. Week 2 — Quick wins: Identify low-hanging fruit (unused resources, oversized instances, non-prod environments running 24/7, unattached EBS volumes, idle load balancers). These often yield 10-15% savings with minimal effort. 3. Week 2 — Plan: Present tiered savings plan: Tier 1 (quick wins, 10-15%, 2 weeks), Tier 2 (right-sizing and reserved instances, 10-15%, 6-8 weeks), Tier 3 (architectural changes — caching, serverless migration, data tiering, 5-10%, 1-2 quarters). 4. Execution: Assign cost owners per team. Create cost dashboards visible to all engineers. Set up weekly cost review cadence. Implement automated alerts for spend anomalies. 5. Sustainability: Bake cost awareness into architecture reviews and PR reviews. Make cloud cost a standing item in team retrospectives. Celebrate wins publicly. 6. Report to CFO monthly with progress against target, broken down by initiative.",
    "commonMistakes": "Promising 30% savings purely from quick wins (architectural changes are needed for the last 10-15%). Not tagging resources properly (you can't optimize what you can't measure). Making engineers feel punished for using cloud resources (kills innovation). Cutting non-prod environments that teams actually need for testing. Not setting up sustainable practices (costs creep back within 6 months). Treating it as a one-time project instead of an ongoing discipline. Over-optimizing and causing production reliability issues (saving $50K/month but causing outages that cost more).",
    "whatGoodLooksLike": "Within 2 weeks: full cost visibility with per-team and per-service attribution. Within 1 month: quick wins (10-15%) delivered. Within 1 quarter: 30% target achieved through quick wins, right-sizing, and architectural improvements; cost dashboards used by engineers proactively. Ongoing: FinOps practices embedded in team culture — cost considered in architecture decisions; no production reliability degradation; CFO sees engineering as a partner in financial discipline. Measured by: cloud cost per transaction (metric 10.1), AI tooling ROI (metric 10.5).",
    "mappingNotes": "Cloud cost optimization and FinOps leadership",
    "suggestedMetricIds": [
      "10.1",
      "10.2",
      "8.4"
    ]
  },
  {
    "id": "P-C11-2",
    "slug": "managing-through-a-hiring-freeze",
    "observableIds": [
      "C11-O1",
      "C11-O5",
      "C6-O4"
    ],
    "capabilityIds": [
      "C11",
      "C6"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Managing Through a Hiring Freeze",
    "context": "6-month hiring freeze announced company-wide. You have 3 open reqs mid-pipeline — one candidate just passed the onsite, another is in final round, and a third is at phone screen stage. Your team of 9 is already stretched across 3 projects. Two senior engineers are showing signs of burnout. One mid-level engineer recently mentioned they've been getting recruiter outreach. Attrition risk is real and you can't backfill.",
    "topicsActivated": [
      "Talent Acquisition (Pipeline Management During Freeze)",
      "Team Health (Retention, Burnout Prevention)",
      "Performance (Capacity Planning)",
      "Stakeholder Mgmt (Priority Negotiation)"
    ],
    "decisionFramework": "1. Candidates in pipeline: Fight for exceptions for the candidate who passed onsite — they're the hardest to re-recruit later. Present the cost of losing them (recruiting fees, ramp time, offer expiration). For earlier-stage candidates: communicate honestly and keep them warm if possible. 2. Scope reduction: Immediately re-prioritize. With 9 people instead of planned 12, you cannot run 3 full projects. Present leadership with options: (A) 2 projects at full speed + 1 paused, (B) 3 projects at reduced scope, (C) rank order and leadership decides. 3. Retention: Individual conversations with each team member within 1 week. Understand their concerns. Invest in what you CAN offer — stretch assignments, learning opportunities, conference talks, visibility projects, flexible work arrangements. 4. Burnout mitigation: Reduce meeting load, protect focus time, be explicit about what's NOT getting done. No hero culture. 5. Internal mobility: Can you borrow people from teams with lower priority work? Propose temporary reallocation. 6. Plan for freeze end: Keep job descriptions updated, maintain recruiter relationships, have a priority order for reqs when hiring resumes.",
    "commonMistakes": "Accepting the freeze without fighting for in-pipeline candidates (especially post-onsite — you'll never get them back). Not adjusting scope to match reduced team (expecting 9 people to do the work of 12). Ignoring retention risk because 'where would they go in this market?' (good engineers always have options). Canceling all development investment (training, conferences, hackathons) — this is exactly when you need retention tools most. Burning out your best people by giving them the extra work. Not planning for freeze end — scrambling when hiring reopens wastes months.",
    "whatGoodLooksLike": "Within 1 week: exception attempted for post-onsite candidate with VP-level advocacy; scope explicitly reduced and communicated to stakeholders; every team member had a 1:1 addressing concerns and growth path. Ongoing: burnout signals monitored actively; workload redistributed; low-priority work paused; zero regrettable attrition during freeze. When freeze ends: reqs ready, pipeline warm, team hits ground running — maintain hiring committee relationships and interviewer calibration across hiring cycles. Measured by: attrition rate (metric 7.1), team engagement (metric 5.1).",
    "mappingNotes": "Hiring freeze management and retention during constraints",
    "suggestedMetricIds": [
      "7.3",
      "7.5",
      "5.1",
      "7.9"
    ]
  },
  {
    "id": "P-C3-3",
    "slug": "your-product-is-being-sunset-deprecated",
    "observableIds": [
      "C3-O6",
      "C1-O15",
      "C7-O4"
    ],
    "capabilityIds": [
      "C3",
      "C1",
      "C7"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Your Product Is Being Sunset/Deprecated",
    "context": "Leadership decides to sunset your team's product — a service with 50K active users. Revenue doesn't justify continued investment. Users need to be migrated to an alternative (internal or third-party). Timeline is 9 months. Your team of 10 engineers built this product over 3 years. They're anxious about their futures — will they be laid off, reassigned, or forgotten?",
    "topicsActivated": [
      "Technical Strategy (Deprecation Planning, User Migration)",
      "Org Design (Team Dissolution/Reassignment)",
      "Team Health (Morale, Career Anxiety)",
      "Stakeholder Mgmt (Communicating Difficult Decisions)",
      "Change Management (Product Lifecycle End)"
    ],
    "decisionFramework": "1. People first (Day 1-3): Before talking about migration plans, address your team's concerns. Be transparent about what you know and don't know. Commit to advocating for every team member's next role. If you have guarantees about no layoffs, say so. If you don't, be honest about the uncertainty. 2. Migration planning (Week 1-2): Assess user segmentation — which users migrate to what alternative? Create migration tiers: self-serve users (automated migration tools), mid-tier (guided migration with docs), enterprise/high-touch (white-glove migration support). 3. Communication plan (Week 2): Draft user communication in partnership with PM and marketing. Give users maximum notice. Provide clear timelines, migration guides, and support channels. 4. Technical execution (Month 1-8): Build migration tooling, maintain product stability during wind-down (no new features, but fix critical bugs and security issues), decommission infrastructure in phases. 5. Team placement (Month 1-6): Work with your manager and HR to identify landing spots for every team member. Match skills and interests to open roles. Start conversations early — don't wait until month 8. 6. Knowledge transfer (Month 6-9): Document institutional knowledge, transfer ownership of any shared components, archive code and documentation. 7. Sunset (Month 9): Final user cutover, infrastructure decommission, retrospective on the product lifecycle.",
    "commonMistakes": "Not addressing team anxiety immediately (people start job searching externally on Day 1 if they feel uncertain). Treating migration as an afterthought (50K users migrating poorly creates customer support nightmares and brand damage). Letting product quality degrade during wind-down (users still depend on it until they're migrated). Not fighting for your team members' placement (they remember who advocated for them). Waiting until month 7 to start placing people (best roles are taken). Pretending the product was a failure (it served users for 3 years — honor that). Not doing a retrospective (valuable lessons about product-market fit, technical decisions, and lifecycle management).",
    "whatGoodLooksLike": "By month 6: every team member knows their next role — zero involuntary departures. By month 9: 90%+ users migrated with minimal support tickets. Throughout: product quality maintained until cutover; user communication transparent and empathetic. Outcome: team members report the process was handled well; NPS impact minimized. Measured by: user migration completion rate, support ticket volume, team retention, user satisfaction.",
    "mappingNotes": "Product sunset, user migration, and team reassignment",
    "suggestedMetricIds": [
      "8.2",
      "8.4",
      "1.2"
    ]
  },
  {
    "id": "P-C2-1",
    "slug": "quarterly-planning-product-engineering-disagree",
    "observableIds": [
      "C2-O1",
      "C2-O2"
    ],
    "capabilityIds": [
      "C2",
      "C5"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Quarterly Planning When Product and Engineering Disagree on Priorities",
    "context": "It's planning season. Product wants to invest heavily in new features for a key customer segment. Engineering wants to pay down critical tech debt that's causing weekly incidents. Both sides have compelling data. You need to facilitate alignment.",
    "topicsActivated": [
      "Strategic Prioritization (Investment Balance)",
      "Cross-Functional Influence (PM Partnership)",
      "Decision Framing (Trade-off Communication)"
    ],
    "decisionFramework": "1. Quantify both sides: feature revenue impact vs. incident cost (engineer time, customer impact, pager fatigue). 2. Find the false dichotomy: can scope be split? Can debt be paid incrementally alongside features? 3. Use a capacity allocation framework (e.g., 70/20/10 — see P-C3-4 for detailed implementation): 70% features, 20% debt, 10% exploration — adjust ratios with data. 4. Create a shared artifact (investment portfolio view) both sides can see. 5. Agree on metrics to evaluate next quarter whether the split was right. 6. Escalate only if you've genuinely tried to align and can articulate the trade-off crisply.",
    "commonMistakes": "Picking a side instead of facilitating trade-off analysis. Using vague 'tech debt' framing without specific incidents/costs. Letting loudest voice win. Deferring entirely to product ('they own the roadmap'). Committing to both without acknowledging capacity constraints.",
    "whatGoodLooksLike": "Within 1 week: shared investment portfolio document with clear ratios. Within 2 weeks: both product and engineering feel heard; trade-offs are explicit and documented. Ongoing: success metrics defined for both feature and debt investments; re-evaluation cadence agreed upon. Measured by: investment mix ratio adherence (metric 8.4), OKR achievement rate (metric 8.1).",
    "mappingNotes": "Strategic prioritization with cross-functional tension",
    "suggestedMetricIds": [
      "8.4",
      "8.1",
      "8.3",
      "1.2"
    ]
  },
  {
    "id": "P-C2-2",
    "slug": "asked-to-build-conflicting-with-okrs",
    "observableIds": [
      "C2-O2",
      "C2-O3"
    ],
    "capabilityIds": [
      "C2",
      "C10"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Asked to Build Something That Conflicts with Your OKRs",
    "context": "A VP from another org asks your team to take on an urgent project that doesn't align with your quarterly OKRs. Your manager seems supportive of helping. Taking this on means dropping or delaying a committed OKR.",
    "topicsActivated": [
      "Strategic Prioritization (Saying No)",
      "Resource Allocation (Scope Management)",
      "Stakeholder Mgmt (Managing Up and Across)"
    ],
    "decisionFramework": "1. Clarify the ask: scope, timeline, why your team specifically. 2. Map impact: what gets dropped or delayed if you say yes. 3. Present trade-off to your manager explicitly: 'We can do X, but Y slips by Z weeks.' 4. If manager says do both: push back with data on capacity. 5. If you take it on: renegotiate OKRs formally — don't silently absorb. 6. Document the decision and who made it.",
    "commonMistakes": "Silently absorbing the work and burning out the team. Saying yes without surfacing what gets dropped. Saying no without offering alternatives (different team, reduced scope, later timeline). Not looping in your manager before committing. Treating OKRs as immutable when the business context has changed.",
    "whatGoodLooksLike": "Within 24 hours: trade-off conversation with manager. Within 48 hours: clear written communication to requesting VP with options. Within 1 week: OKRs formally adjusted if work is accepted; team understands why priorities shifted. Outcome: no hero-mode — scope is right-sized to capacity; reversible commitments flex, irreversible ones get escalated. Measured by: OKR achievement rate (metric 8.1), engineering investment mix (metric 8.4).",
    "mappingNotes": "Priority conflict with cross-org pressure",
    "suggestedMetricIds": [
      "8.1",
      "8.4",
      "8.3"
    ]
  },
  {
    "id": "P-C4-1",
    "slug": "team-delivery-predictability-collapsed",
    "observableIds": [
      "C4-O1",
      "C4-O2",
      "C4-O8"
    ],
    "capabilityIds": [
      "C4",
      "C9"
    ],
    "relevantLevels": [
      2,
      3
    ],
    "title": "Your Team's Delivery Predictability Has Collapsed",
    "context": "Over the past two quarters, your team has missed 70% of committed sprint goals. Stakeholders are losing trust. The team feels demoralized and says estimates are impossible given constant interruptions and changing requirements.",
    "topicsActivated": [
      "Operational Leadership (Delivery Cadence)",
      "Metrics & Measurement (Predictability)",
      "Team Health (Morale)"
    ],
    "decisionFramework": "1. Diagnose: categorize why work was missed — scope creep, interrupts, underestimation, dependencies, or changing priorities? 2. Measure interrupt load: what percentage of sprint capacity goes to unplanned work? 3. Right-size commitments: commit to less, deliver consistently. 4. Protect the sprint: create an interrupt buffer (e.g., 20% unplanned capacity) or designate an interrupt handler rotation. 5. Shorten planning horizon if needed (2-week sprints → 1-week). 6. Rebuild trust: show 3 consecutive on-target deliveries before expanding scope.",
    "commonMistakes": "Adding more process (longer planning, more estimation ceremonies) without addressing root cause. Blaming the team for bad estimates when the real problem is scope creep. Committing to aggressive goals to 'make up' for missed ones. Not addressing the interrupt source (often another team or on-call). Tracking velocity as a performance metric instead of a planning tool.",
    "whatGoodLooksLike": "Within 1 week: root cause analysis completed. Within 2 weeks: interrupt load measured and buffer created; commitments reduced to 70% of historical capacity. Within 6 weeks: 3 consecutive sprints hitting >80% of commitments. Outcome: stakeholders see improving trend and regain trust; team morale improves as they start hitting goals. Measured by: sprint commitment accuracy (metric 2.5), flow time (metric 2.1), carryover rate.",
    "mappingNotes": "Delivery predictability recovery",
    "suggestedMetricIds": [
      "2.5",
      "1.2",
      "2.6",
      "1.4",
      "5.1"
    ]
  },
  {
    "id": "P-C4-2",
    "slug": "inherit-team-no-operating-cadence",
    "observableIds": [
      "C4-O1",
      "C4-O3",
      "C4-O9"
    ],
    "capabilityIds": [
      "C4"
    ],
    "relevantLevels": [
      1,
      2,
      3
    ],
    "title": "You Inherit a Team with No Operating Cadence",
    "context": "You've just taken over a team that has no regular standup, no sprint planning, no retrospectives, and no documented on-call process. Work gets done through Slack threads and ad-hoc requests. The team is productive but chaotic, and knowledge is siloed.",
    "topicsActivated": [
      "Operational Leadership (Cadence Design)",
      "Developer Experience (Process)",
      "Culture (Norms)"
    ],
    "decisionFramework": "1. Observe first: spend 2 weeks understanding how work actually flows before changing anything. 2. Identify the biggest pain point (not your biggest concern — theirs). 3. Introduce ONE ceremony at a time. Start with a weekly sync — lowest friction, highest visibility. 4. Add planning/retro after the team sees value in the sync. 5. Document decisions and on-call in a shared runbook. 6. Don't over-process: match cadence to team size and maturity. A 4-person team doesn't need SAFe.",
    "commonMistakes": "Introducing 5 new meetings in week 1. Copying your previous team's exact process. Not asking the team what's actually painful. Making process compliance the goal instead of outcomes. Assuming chaos means dysfunction — sometimes small teams work fine with minimal process.",
    "whatGoodLooksLike": "Week 1-2: observation period before changes. Week 3-4: team co-designs operating cadence; one ceremony introduced per 2-week cycle. Within 30 days: written runbook for on-call. Within 60 days: team reports less chaos and fewer knowledge silos. Outcome: process is lightweight and valued, not resented. Measured by: meeting purpose clarity (can every attendee state the purpose?), knowledge silo reduction, team satisfaction.",
    "mappingNotes": "Operating cadence bootstrapping",
    "suggestedMetricIds": [
      "1.2",
      "1.4",
      "2.5",
      "5.1"
    ]
  },
  {
    "id": "P-C12-3",
    "slug": "your-team-culture-is-deteriorating-after-rapid-growth",
    "observableIds": [
      "C12-O1",
      "C12-O2",
      "C12-O6"
    ],
    "capabilityIds": [
      "C12"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Your Team Culture Is Deteriorating After Rapid Growth",
    "context": "Your team doubled from 6 to 12 in 3 months. Skip-level feedback reveals new hires feel excluded, original team members feel their culture is gone, and collaboration norms are breaking down.",
    "topicsActivated": [
      "Team Health (Culture Continuity)",
      "Onboarding (Cultural Integration)",
      "Communication (Scaling Norms)"
    ],
    "decisionFramework": "1. Diagnose: run anonymous team health survey to quantify the problem. 2. Acknowledge: in team meeting, name the challenge — growth is hard, culture erosion is natural, and you're going to address it intentionally. 3. Rebuild: facilitate team charter session with ALL members (not just originals) to co-create norms. 4. Pair: create culture buddy system pairing new hires with veterans. 5. Formalize: document onboarding culture guide. 6. Monitor: monthly culture pulse checks for 2 quarters. 7. Adjust: iterate on norms based on feedback.",
    "commonMistakes": "Assuming culture will 'rub off' naturally. Only listening to original team members. Treating it as a one-time off-site exercise. Not documenting norms (they live in people's heads). Blaming new hires for 'not getting it'. Trying to preserve old culture exactly instead of evolving it.",
    "whatGoodLooksLike": "Within 4 weeks: co-created team charter with all members (not just originals); culture buddy program running. Within 2 months: monthly pulse checks showing improvement. Within 60 days: new hires report feeling integrated. Key principle: use Health Checks to make cultural dimensions explicit and discussable, enabling rapid intervention when scores decline. Measured by: team engagement (metric 5.1), new hire integration survey.",
    "mappingNotes": "Culture deterioration during growth scenario",
    "suggestedMetricIds": [
      "7.1",
      "7.2",
      "5.1"
    ]
  },
  {
    "id": "P-C12-4",
    "slug": "addressing-a-toxic-team-member-senior-engineer-others-avoid",
    "observableIds": [
      "C12-O1",
      "C12-O8"
    ],
    "capabilityIds": [
      "C12",
      "C6"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Addressing a Toxic Senior Engineer the Team Works Around",
    "context": "Your most technically skilled senior engineer is dismissive in code reviews, dominates discussions, and makes junior engineers afraid to speak up. The team has learned to 'work around' this person, but you're losing good people.",
    "topicsActivated": [
      "Team Health (Psychological Safety)",
      "Performance (Behavioral Standards)",
      "Culture (Norm Enforcement)"
    ],
    "decisionFramework": "1. Document: collect specific behavioral examples with dates and impact (code review comments, meeting behaviors, peer feedback). 2. Private 1:1: deliver feedback using SBI framework — specific behavior, its impact on team and business, expected change. 3. Set timeline: clear behavioral expectations with 30-day checkpoint. 4. Support: provide coaching or resources (communication training, mentoring). 5. Monitor: check with team members (without naming) about improvement. 6. Follow through: if no improvement at 30 days, escalate to formal performance process. 7. Communicate: when behavior improves, acknowledge it; if person exits, address team about culture standards.",
    "commonMistakes": "Avoiding the conversation because of the person's technical value. Giving vague feedback like 'be nicer'. Addressing it publicly instead of privately first. Not documenting specific examples. Treating it as a personality issue rather than behavioral impact. Waiting for an HR complaint instead of acting proactively.",
    "whatGoodLooksLike": "Within 1 week: feedback delivered with specific behavioral examples cited. Within 30 days: clear improvement plan with timeline; psychological safety survey administered before and after intervention. Outcome: either behavioral improvement or managed exit; team knows culture standards are enforced regardless of seniority. Key principle: brilliant jerks are a net negative — technical skill doesn't exempt anyone from behavioral standards. Measured by: psychological safety survey delta, team engagement (metric 5.1).",
    "mappingNotes": "Toxic senior engineer scenario",
    "suggestedMetricIds": [
      "7.1",
      "5.1",
      "7.2"
    ]
  },
  {
    "id": "P-C10-4",
    "slug": "asked-to-do-more-with-fewer-people-after-layoffs",
    "observableIds": [
      "C10-O2",
      "C10-O6",
      "C10-O8"
    ],
    "capabilityIds": [
      "C10"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Asked to Do More With Fewer People After Layoffs",
    "context": "After a round of layoffs, your team lost 3 of 10 engineers. Leadership still expects the same deliverables. Team morale is low, survivors are anxious, and you need to figure out what's actually possible.",
    "topicsActivated": [
      "Resource Allocation (Capacity Planning)",
      "Stakeholder Mgmt (Expectation Setting)",
      "Team Health (Post-Layoff Recovery)"
    ],
    "decisionFramework": "1. Acknowledge first, plan second (Week 1): Before any roadmap discussion, hold honest conversations with your team. Name what happened, honor the people who left, and address survivor anxiety directly. 'We lost good people, and this affects us all. Here's what I know, here's what I don't.' 2. Assess capacity and emotional readiness (Week 1-2): Map remaining capacity honestly, but also assess team morale. A team in shock can't deliver at 100% of mathematical capacity — plan for 70-80% utilization in the first quarter. 3. Present tiered options to leadership (Week 2-3): Go to leadership with clear trade-offs — (A) what's deliverable with current team at sustainable pace, (B) what requires timeline extensions, (C) what needs to stop entirely. Frame this as protecting the investment in the remaining team, not complaining about headcount. 4. Rebuild team identity (Month 1-2): The team's identity has been disrupted. Redistribute ownership thoughtfully, create new rituals, and invest in the remaining team's growth to signal commitment. 5. Monitor for delayed attrition (Ongoing): The biggest risk post-layoffs is losing people you intended to keep. Increase 1:1 frequency, watch for disengagement signals, and proactively discuss career growth with top performers.",
    "commonMistakes": "Immediately jumping to roadmap replanning without acknowledging the emotional impact. Treating this as a purely resource-allocation problem when it's also a trust and morale crisis. Being vague about what stops — 'we'll try our best' is not a plan. Not renegotiating timelines with external stakeholders. Burning out remaining team trying to prove the layoffs weren't needed. Ignoring survivor guilt and anxiety in skip-level conversations.",
    "whatGoodLooksLike": "Within 2 weeks: honest capacity assessment completed; tiered options presented to leadership with quantified trade-offs; at least 30% of previous scope explicitly deprioritized or stopped. Within 6 weeks: team morale stabilized (measured via pulse survey). Within 1 quarter: no additional regrettable departures. Key principle: radical scope reduction proportional to headcount reduction — don't expect remaining team to absorb the load. Measured by: attrition rate (metric 7.1), team engagement (metric 5.1), engineering investment mix (metric 8.4).",
    "mappingNotes": "Post-layoff resource management scenario",
    "suggestedMetricIds": [
      "7.1",
      "5.1",
      "1.2"
    ]
  },
  {
    "id": "P-C7-2",
    "slug": "communicating-a-major-technical-decision-that-not-everyone-agrees-with",
    "observableIds": [
      "C7-O1",
      "C7-O6",
      "C7-O8"
    ],
    "capabilityIds": [
      "C7"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Communicating a Major Technical Decision Not Everyone Agrees With",
    "context": "After an RFC process, you've decided to migrate from microservices back to a modular monolith. Senior engineers are split — some are excited, others feel their microservices expertise is being devalued. You need to communicate the decision without creating factions.",
    "topicsActivated": [
      "Decision Framing (Controversial Decisions)",
      "Communication (Technical Audience)",
      "Culture (Constructive Disagreement)"
    ],
    "decisionFramework": "1. Document the decision fully: what was decided, why, what alternatives were considered and why they were rejected. 2. Acknowledge dissent: name the strongest counter-arguments and explain specifically why you decided differently. 3. Communicate in writing first: publish the decision doc before the meeting so people can process it. 4. Hold a Q&A: let people ask questions and express concerns without relitigating. 5. Commit explicitly: 'This is the decision. If you disagree, I respect that, and I need you to commit to execution.' 6. Follow up individually: talk to the loudest dissenters 1:1 to address concerns and check for commitment.",
    "commonMistakes": "Announcing without explaining the reasoning. Dismissing disagreement as 'not understanding'. Not documenting the alternative options considered. Relitigating the decision in every meeting. Making it personal — 'I decided' vs. 'we evaluated'. Not following up with dissenters who might sabotage silently.",
    "whatGoodLooksLike": "Within 1 week: decision doc published with alternatives analysis; team can articulate the reasoning even if they disagreed. Within 2 weeks: execution begins; no underground resistance; dissenters feel heard even though they didn't 'win'. At 90 days: team retrospective evaluates whether the decision is working. Measured by: deployment frequency (metric 1.2), engineering investment mix (metric 8.4).",
    "mappingNotes": "Controversial technical decision communication scenario",
    "suggestedMetricIds": [
      "1.2",
      "8.4"
    ]
  },
  {
    "id": "P-C7-3",
    "slug": "your-team-has-communication-debt-decisions-not-documented",
    "observableIds": [
      "C7-O6",
      "C7-O9"
    ],
    "capabilityIds": [
      "C7",
      "C4"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Your Team Has Communication Debt — Decisions Aren't Documented",
    "context": "Your team has grown from 5 to 15 people. Key technical decisions live in Slack threads, hallway conversations, and individual memories. New hires can't find context. The same decisions are relitigated monthly. You're drowning in 'quick syncs'.",
    "topicsActivated": [
      "Communication (Documentation Practices)",
      "Decision Making (Process Design)",
      "Operational Rhythm (Scaling)"
    ],
    "decisionFramework": "1. Audit: List the top 10 decisions made in the last quarter — can you find the documentation? If not, you have communication debt. 2. Triage: retroactively document the most impactful undocumented decisions (tech choices, process decisions, ownership assignments). 3. Introduce lightweight RFC process: not bureaucratic — a simple template for decisions that affect >2 people. 4. Create decision log: a single page listing recent decisions with links to context. 5. Establish norms: 'If it was decided in Slack, it didn't happen until it's in the decision log.' 6. Review: monthly audit of decision documentation completeness.",
    "commonMistakes": "Making the RFC process too heavy (kills adoption). Not retroactively documenting the most critical past decisions. Expecting the team to change overnight (habits take weeks). Documenting everything (leads to documentation fatigue). Not having a single searchable location for decisions. Confusing meeting notes with decision documentation.",
    "whatGoodLooksLike": "Within 6 weeks: lightweight RFC template adopted; decision log created and maintained; new hires can find context for major decisions without asking. Within 3 months: same-decision relitigations drop measurably; communication overhead per person decreases as team scales. Ongoing: every significant decision has a single documented source of truth accessible to the entire team and stakeholders. Measured by: design doc count per quarter (metric 2.3), sprint velocity trend (metric 2.5).",
    "mappingNotes": "Communication debt and documentation scaling scenario",
    "suggestedMetricIds": [
      "2.3",
      "2.5"
    ]
  },
  {
    "id": "P-C14-3",
    "slug": "high-performer-wants-promotion-but-isnt-ready",
    "observableIds": [
      "C14-O5",
      "C14-O8"
    ],
    "capabilityIds": [
      "C14",
      "C6"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "High Performer Wants Promotion But Isn't Ready",
    "context": "Your best engineer is pushing hard for promotion to Senior. They're technically excellent and deliver consistently, but they lack the influence, mentoring, and scope leadership expected at the next level. They're getting impatient and you're worried about retention.",
    "topicsActivated": [
      "Performance (Promotion Readiness)",
      "Coaching (Development Planning)",
      "Retention (Managing Expectations)"
    ],
    "decisionFramework": "1. Validate: Review the level expectations document and honestly assess where the engineer is vs. where they need to be. 2. Prepare the conversation: document specific gaps with examples (not 'you need more leadership' but 'at Senior level, you'd need to have led a cross-team initiative end-to-end'). 3. Deliver with care: acknowledge their strengths, name the specific gaps, and frame it as 'here's what we're building toward' not 'you're not good enough'. 4. Co-create a plan: identify 2-3 specific stretch opportunities over the next 6 months that address the gaps. 5. Commit to support: regular check-ins on progress, sponsor for the right opportunities, provide visibility. 6. Manage retention: separate promo from retention — explore comp adjustment, scope expansion, or recognition to address flight risk without premature promotion.",
    "commonMistakes": "Promoting to retain (creates level inflation and sets them up to fail). Vague feedback like 'you need more scope'. Not separating retention from promotion — they're different problems. Saying 'maybe next cycle' without a concrete plan. Comparing them to others who were promoted. Not sponsoring opportunities that would fill the gaps. Over-promising timeline for promotion.",
    "whatGoodLooksLike": "Within 2 weeks: specific gap analysis delivered with rubric-aligned evidence gaps identified. Within 30 days: at least one stretch assignment initiated targeting the highest-priority gap. Within 6 months: development plan with concrete milestones co-created; monthly check-ins on progress; running promotion document updated monthly with gap analysis. Engineer feels invested in (not rejected). If comp is the real issue, address it separately. Measured by: promotion success rate (metric 5.1), retention of high performers (metric 7.1).",
    "mappingNotes": "Promotion readiness gap management scenario",
    "suggestedMetricIds": [
      "5.1",
      "7.1"
    ]
  },
  {
    "id": "P-C11-3",
    "slug": "your-interview-process-is-rejecting-good-candidates",
    "observableIds": [
      "C11-O1",
      "C11-O9"
    ],
    "capabilityIds": [
      "C11"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Your Interview Process Is Rejecting Good Candidates",
    "context": "Your team has been hiring for 4 months with zero offers extended. Recruiting says candidate quality is high, but your interviewers keep saying 'not a strong enough signal.' You suspect the interview process itself is the problem.",
    "topicsActivated": [
      "Hiring (Interview Process Design)",
      "Metrics (Conversion Rates)",
      "Culture (Bias in Assessment)"
    ],
    "decisionFramework": "1. Diagnose (Week 1): Pull conversion rates by interview stage. Where's the biggest drop-off? Is it consistent across interviewers? 2. Audit (Week 1-2): Review recent interview scorecards. Are scores specific (behavioral evidence) or vague ('didn't seem senior enough')? Are rubrics being used? Is scoring happening before debrief? 3. Calibrate (Week 2-3): Run a calibration session with all interviewers using a mock candidate. Identify scoring discrepancies. 4. Fix (Week 3-4): Revise rubrics with specific behavioral anchors. Retrain interviewers. Implement independent scoring before debrief. 5. Monitor (Ongoing): Track conversion rates weekly, interviewer agreement rates, and candidate feedback scores.",
    "commonMistakes": "Blaming recruiting for candidate quality without examining your interview process. Using vague rubrics that different interviewers interpret differently. Not collecting candidate feedback on the interview experience. Lowering the bar instead of fixing the process. Assuming the process works because some good candidates get through.",
    "whatGoodLooksLike": "Within 2 weeks: root cause identified from funnel data. Within 4 weeks: rubrics revised with specific behavioral anchors; interviewer calibration complete. Within 6 weeks: conversion rate improving. Within 8 weeks: hire made with improved process. Key principle: predefined rubrics with behavioral anchors and independent scoring achieve consistent evaluation at scale. Measured by: offer acceptance rate, interviewer agreement rate, candidate experience scores.",
    "mappingNotes": "Broken interview process diagnosis scenario",
    "suggestedMetricIds": [
      "7.3"
    ]
  },
  {
    "id": "P-C4-3",
    "slug": "your-team-is-agile-in-name-only",
    "observableIds": [
      "C4-O1",
      "C4-O3"
    ],
    "capabilityIds": [
      "C4"
    ],
    "relevantLevels": [
      1,
      2,
      3
    ],
    "title": "Your Team Is Agile in Name Only",
    "context": "Your team runs 'sprints' but nothing is really iterative. Sprint planning is task assignment by the EM. Retros generate action items nobody completes. Standup is a status report to you. The team jokes about 'waterfall with standups.'",
    "topicsActivated": [
      "Process (Agile Implementation)",
      "Team Health (Ownership)",
      "Operational Rhythm (Ceremony Design)"
    ],
    "decisionFramework": "1. Honest audit (Week 1): For each ceremony, ask 'What decision did this change last month?' If nothing, it's cargo cult. 2. Strip to essentials (Week 2): Cancel all ceremonies except retro and planning. Let the team feel the difference. 3. Rebuild from need (Week 3-4): Ask the team 'What problems do we have that a recurring meeting could solve?' Reintroduce only ceremonies the team wants. 4. Fix ownership (Month 2): Shift standup from status-report-to-EM to team-coordination. Let the team run planning. Make retro action items the first item on next planning. 5. Measure (Ongoing): Track ceremony participation, action item completion, and team satisfaction with process.",
    "commonMistakes": "Adding more process to fix a process problem. Adopting SAFe/LeSS/Scrum-of-Scrums when basic Scrum isn't working. Making it about the framework instead of the outcomes. Not letting the team own their process. Running retros without ever completing the action items. Treating agile as a noun instead of an adjective.",
    "whatGoodLooksLike": "Within 6 weeks: team owns their ceremonies; retro action items >80% completion rate; standup is team coordination not status report; sprint commitments are team-made not EM-assigned. Outcome: team describes their process as 'what works for us' rather than 'what we're supposed to do.' Measured by: retro action item completion rate, sprint commitment accuracy, team ownership of ceremonies.",
    "mappingNotes": "Cargo cult agile diagnosis and fix scenario",
    "suggestedMetricIds": [
      "2.5",
      "2.4"
    ]
  },
  {
    "id": "P-C9-2",
    "slug": "leadership-wants-engineering-productivity-metrics-yesterday",
    "observableIds": [
      "C9-O1",
      "C9-O9",
      "C9-O10"
    ],
    "capabilityIds": [
      "C9"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Leadership Wants Engineering Productivity Metrics Yesterday",
    "context": "Your VP asks for a 'developer productivity dashboard' by next month. They want to see engineering ROI. Your team currently has no metrics infrastructure. You know that naive productivity metrics can be destructive.",
    "topicsActivated": [
      "Metrics (Framework Selection)",
      "Stakeholder Mgmt (Managing Expectations)",
      "Team Health (Avoiding Surveillance)"
    ],
    "decisionFramework": "1. Understand the real need (Week 1): What decision is leadership trying to make? 'See engineering ROI' could mean many things. Ask: 'If you had this dashboard, what would you do differently?' 2. Propose a phased approach (Week 1): Phase 1 (Month 1): 3 core DORA metrics from existing tooling. Phase 2 (Month 2): Add developer satisfaction survey. Phase 3 (Month 3): Add business outcome connection (features → impact). 3. Set guardrails (Week 2): Metrics for team-level trends, not individual surveillance. Paired metrics (speed + quality). No metrics as performance targets. 4. Build Phase 1 (Month 1): Deploy basic DORA dashboard from CI/CD data. 5. Iterate (Ongoing): Add metrics only when previous ones drive decisions. Retire metrics that don't inform action.",
    "commonMistakes": "Giving leadership lines-of-code or tickets-closed metrics (gameable and misleading). Building a complex dashboard nobody uses. Using metrics for individual performance evaluation. Not asking what decisions the metrics should inform. Building everything at once instead of iterating. Not pairing speed metrics with quality metrics.",
    "whatGoodLooksLike": "Within 1 week: existing measurement state assessed. Within 2 weeks: 3-5 metrics proposed with clear decision linkage. Within 1 month: first metrics review conducted with leadership. Outcome: leadership gets useful data without weaponizing metrics against engineers. Measured by: leadership satisfaction with visibility, zero complaints about surveillance, decisions driven by data.",
    "mappingNotes": "Engineering productivity metrics rollout scenario",
    "suggestedMetricIds": [
      "2.1",
      "2.2",
      "1.2"
    ]
  },
  {
    "id": "P-C5-6",
    "slug": "product-and-engineering-have-separate-roadmaps",
    "observableIds": [
      "C5-O1"
    ],
    "capabilityIds": [
      "C5",
      "C2"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Product and Engineering Have Separate Roadmaps",
    "context": "You discover that PM has a roadmap they share with stakeholders, and your engineering team has a separate technical roadmap. The two don't align. PM is frustrated that engineering is 'doing their own thing.' Engineering is frustrated that PM doesn't account for technical investment.",
    "topicsActivated": [
      "Cross-Functional Partnership (Alignment)",
      "Strategic Prioritization (Unified Planning)",
      "Communication (Shared Artifacts)"
    ],
    "decisionFramework": "1. Diagnose (Week 1): Compare both roadmaps. What's overlapping? What's unique to each? Where are the conflicts? 2. Align on reality (Week 2): Joint session with PM to reconcile. Present engineering investment needs (tech debt, platform, reliability) as business investments with expected ROI. 3. Unify (Week 3): Create a single roadmap with explicit capacity allocation — e.g., 60% product features, 20% tech debt/platform, 10% reliability, 10% experimentation. 4. Share (Week 4): Publish unified roadmap to all stakeholders. Both PM and Engineering present it together. 5. Sustain (Ongoing): Joint planning sessions quarterly. Weekly triad syncs. Any roadmap changes require mutual agreement.",
    "commonMistakes": "Engineering building their own roadmap in secret. PM not allocating any capacity for engineering-initiated work. Treating it as a power struggle instead of an alignment problem. Not quantifying the business impact of technical investment. Agreeing on a unified roadmap but reverting to separate ones within a quarter.",
    "whatGoodLooksLike": "Within 4 weeks: single unified roadmap with explicit capacity allocation visible to all stakeholders. Ongoing: both PM and Engineering can articulate the full roadmap (not just their part); no more 'surprise' engineering work or 'surprise' PM commitments; shared OKRs make separate roadmaps structurally impossible. Measured by: lead time for changes (metric 1.2), engineering investment mix (metric 8.4).",
    "mappingNotes": "Separate roadmaps alignment scenario",
    "suggestedMetricIds": [
      "1.2",
      "8.4"
    ]
  },
  {
    "id": "P-C6-5",
    "slug": "your-new-engineering-manager-is-struggling-with-the-transition-from-ic",
    "observableIds": [
      "C6-O1",
      "C6-O13"
    ],
    "capabilityIds": [
      "C6"
    ],
    "relevantLevels": [
      4,
      5
    ],
    "title": "Your New Engineering Manager Is Struggling With the IC-to-EM Transition",
    "context": "You promoted your best senior engineer to EM 3 months ago. They're still writing code 50% of the time, 1:1s are technical problem-solving sessions, and skip-level feedback reveals reports feel unsupported. Your new EM is frustrated that 'management is keeping me from real work.'",
    "topicsActivated": [
      "Coaching (New Manager Development)",
      "Career Development (IC to EM Transition)",
      "Team Health (Report Experience)"
    ],
    "decisionFramework": "1. Validate (Week 1): Confirm the problem through skip-level feedback and 1:1 with the new EM. Understand their perspective without judgment. 2. Redefine success (Week 1-2): Have an explicit conversation about what 'good EM work' looks like. It's not coding. It's growing people, removing blockers, setting direction. 3. Create structure (Week 2-3): Set explicit time allocation targets (code: <10%, 1:1s: 25%, strategy/planning: 25%, cross-functional: 20%, admin: 20%). Pair them with an experienced EM mentor. 4. Build skills (Month 2): Coaching on feedback delivery, 1:1 structure, career conversations. Shadow their 1:1s (with consent) and provide feedback. 5. Check and adjust (Month 3): Review skip-level feedback. If improving, continue coaching. If not, have honest conversation about whether management is the right path. 6. Offer the exit ramp (If needed): Make return to IC genuinely safe and unstigmatized.",
    "commonMistakes": "Expecting the transition to happen naturally without coaching. Not reducing their IC responsibilities explicitly. Criticizing their management without teaching alternatives. Not offering a genuine return-to-IC path. Promoting again too soon (next person) without providing better transition support.",
    "whatGoodLooksLike": "Within 90 days: EM spending <20% time on code, 1:1s focused on career and growth, skip-level feedback improving. Within 6 months: EM has found their management identity, reports feel supported, EM is invested in management (not pining for IC work). Or: EM has returned to IC track gracefully, and someone better suited has taken the role — both outcomes are wins. Measured by: team engagement scores (metric 5.1), attrition rate (metric 7.1).",
    "mappingNotes": "IC-to-EM transition support scenario",
    "suggestedMetricIds": [
      "5.1",
      "7.1"
    ]
  },
  {
    "id": "P-C8-4",
    "slug": "building-incident-readiness-before-your-first-major-outage",
    "observableIds": [
      "C8-O1",
      "C8-O7"
    ],
    "capabilityIds": [
      "C8"
    ],
    "relevantLevels": [
      1,
      2,
      3
    ],
    "title": "Building Incident Readiness Before Your First Major Outage",
    "context": "Your team owns a growing service but has never had a serious incident. There are no runbooks, no defined on-call rotation, and no incident response process. You know it's a matter of when, not if.",
    "topicsActivated": [
      "Reliability (Incident Preparation)",
      "Operational Rhythm (On-Call Design)",
      "Risk Management (Pre-Incident Planning)"
    ],
    "decisionFramework": "1. Inventory (Week 1): List the 5 most likely failure modes for your service. For each, write a one-page playbook: what happens, how you detect it, how you mitigate it, who to notify. 2. Set up basics (Week 2): Define on-call rotation, escalation path, incident severity levels, and communication channels. 3. Practice (Month 2): Run a tabletop exercise — walk through a simulated incident using the playbooks. Identify gaps. 4. Graduate (Month 3): Run a controlled game day — actually inject a failure in staging, have the on-call respond using the playbook. 5. Improve (Ongoing): After each real incident, update playbooks. Review runbook accuracy quarterly. The process of building playbooks is itself a prevention tool — the conversations surface risks you would have missed.",
    "commonMistakes": "Trying to write runbooks for every possible failure (write for the 5 most likely first). Making on-call purely punitive instead of a learning rotation. Not practicing — having runbooks nobody has ever used. Writing runbooks at the wrong altitude (too abstract or too detailed). Not updating runbooks after incidents reveal gaps.",
    "whatGoodLooksLike": "Within 2 weeks: ICS roles defined and documented. Within 1 month: first game day completed; runbook gaps identified and fixed. Within 1 quarter: team has handled at least one real incident using the new process. Outcome: team is prepared before the first major outage, not during it. Measured by: game day gap count, runbook coverage, team incident readiness confidence.",
    "mappingNotes": "Pre-incident readiness building scenario",
    "suggestedMetricIds": [
      "3.3",
      "3.5"
    ]
  },
  {
    "id": "P-C8-5",
    "slug": "your-on-call-rotation-is-burning-people-out",
    "observableIds": [
      "C8-O2",
      "C8-O8"
    ],
    "capabilityIds": [
      "C8",
      "C4"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Your On-Call Rotation Is Burning People Out",
    "context": "Your on-call engineers are getting paged 10+ times per week. The same alerts fire repeatedly. Morale is cratering. Two engineers have asked to be removed from the rotation. You're losing people to teams with better operational hygiene.",
    "topicsActivated": [
      "Reliability (Alert Quality)",
      "Team Health (On-Call Sustainability)",
      "Operational Rhythm (Toil Reduction)"
    ],
    "decisionFramework": "1. Quantify (Week 1): Pull alert data for the last 30 days — frequency, repeat offenders, pages that required action vs. noise, time to resolve. 2. Triage alerts (Week 1-2): Categorize every alert: actionable (keep), informational (convert to ticket), noise (delete), duplicate (consolidate). 3. Fix top offenders (Week 2-4): Take the top 5 most frequent alerts and either fix the underlying issue, tune the threshold, or convert to non-paging. 4. Redesign rotation (Month 2): Ensure on-call burden is shared equitably, build in compensatory time off, pair junior on-call with experienced backup. 5. Set SLOs (Month 2-3): Define what 'healthy on-call' looks like — target <2 actionable pages per shift, <30 min mean time to engage. 6. Sustain (Ongoing): Review alert quality monthly, track on-call happiness, treat recurring alerts as bugs to fix, not toil to endure.",
    "commonMistakes": "Adding more people to the rotation instead of fixing alert quality (spreads suffering, doesn't fix it). Accepting alert fatigue as 'just how on-call works'. Not tracking alert-to-action ratio. Removing people from rotation when they complain instead of fixing the root cause. Compensating with on-call bonus instead of reducing toil.",
    "whatGoodLooksLike": "Within 2 weeks: on-call health metrics dashboarded (page volume, off-hours pages, false positive rate). Within 1 month: noise reduction sprint completed; top 5 noisy alerts fixed or silenced. Within 1 quarter: off-hours pages <2 per night; on-call satisfaction survey shows improvement. Outcome: on-call viewed as sustainable, not punitive; zero requests to leave the rotation. Measured by: on-call burden (metric 5.6), alert-to-action ratio, on-call satisfaction.",
    "mappingNotes": "On-call burnout and alert fatigue scenario",
    "suggestedMetricIds": [
      "3.3",
      "7.1"
    ]
  },
  {
    "id": "P-C9-3",
    "slug": "your-metrics-are-being-gamed-and-everyone-knows-it",
    "observableIds": [
      "C9-O1",
      "C9-O9"
    ],
    "capabilityIds": [
      "C9"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Your Metrics Are Being Gamed and Everyone Knows It",
    "context": "Leadership set a deployment frequency target. Your team is now deploying tiny changes to hit the number, but cycle time is actually worse and code review quality has dropped. Engineers are cynical about measurement.",
    "topicsActivated": [
      "Metrics (Gaming Prevention)",
      "Culture (Measurement Trust)",
      "Leadership (Incentive Design)"
    ],
    "decisionFramework": "1. Acknowledge (Week 1): Name the problem openly with leadership and the team. Gaming is a rational response to misaligned incentives — it's a system design failure, not a character failure. 2. Diagnose (Week 1-2): Identify which metrics became targets rather than diagnostics. For each, find the paired metric that would have caught the gaming (deployment frequency should be paired with change failure rate or PR size). 3. Redesign (Week 2-3): Replace single-metric targets with paired metrics. Present speed metrics alongside quality counterparts. Stop setting metrics as individual or team performance targets — use them as diagnostic tools only. 4. Rebuild trust (Month 2): Communicate the new approach: metrics exist to help the team improve, not to judge them. Involve engineers in choosing which metrics matter. 5. Monitor (Ongoing): Ask quarterly: 'Which metrics drove a real decision? Which are being ignored or gamed?' Retire any metric that isn't informing action.",
    "commonMistakes": "Punishing the gaming instead of fixing the incentive structure. Replacing gamed metrics with more metrics (creates an arms race). Using metrics as individual performance targets (guarantees gaming). Not involving the team in metric selection. Reacting to metric fluctuations without investigating context.",
    "whatGoodLooksLike": "Within 1 week: gaming behavior identified with evidence. Within 2 weeks: metric pairs introduced to prevent single-metric optimization. Within 1 month: team re-engaged with metrics as diagnostic tools. Outcome: metrics reflect reality; gaming stops because paired metrics make it self-defeating. Measured by: metric-outcome alignment, team trust in metrics (survey).",
    "mappingNotes": "Metrics gaming diagnosis and correction scenario",
    "suggestedMetricIds": [
      "2.1",
      "2.2"
    ]
  },
  {
    "id": "P-C13-3",
    "slug": "your-team-keeps-failing-security-audits-with-the-same-findings",
    "observableIds": [
      "C13-O3",
      "C13-O6"
    ],
    "capabilityIds": [
      "C13"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Your Team Keeps Failing Security Audits With the Same Findings",
    "context": "For the third year in a row, your external audit surfaces the same categories of findings: stale access permissions, unpatched dependencies, and missing encryption at rest. Your team scrambles before each audit but nothing sticks. Leadership is frustrated.",
    "topicsActivated": [
      "Security (Continuous Compliance vs. Audit Cramming)",
      "Process (Automated Evidence Collection)",
      "Culture (Security Ownership)"
    ],
    "decisionFramework": "1. Diagnose the pattern (Week 1): Map every repeat finding to a root cause. Stale permissions → no automated access review. Unpatched deps → no dependency scanning in CI. Missing encryption → no infrastructure-as-code enforcement. The findings are symptoms; the root causes are missing automation. 2. Automate evidence (Week 2-4): For each repeat finding, implement continuous automated checks: scheduled access reviews with auto-expiration, dependency scanning in CI/CD with severity gates, IaC policies that enforce encryption. The goal: compliance evidence generated continuously, not reconstructed annually. 3. Make it visible (Month 2): Create a compliance dashboard showing real-time posture — not a spreadsheet updated once a year. When something drifts, the team sees it within hours, not months. 4. Shift ownership (Month 2-3): Rotate a security champion role. Make compliance part of the definition of done, not a separate review step. 5. Prepare for next audit with confidence (Ongoing): If evidence is generated continuously, audit prep becomes a reporting exercise, not a fire drill.",
    "commonMistakes": "Treating audit prep as a periodic sprint instead of fixing root causes. Blaming the auditors for 'not understanding our context.' Adding headcount to manually check things that should be automated. Confusing compliance (checking boxes) with security (actually being secure). Building a compliance team instead of embedding compliance into engineering.",
    "whatGoodLooksLike": "Within 90 days: automated scanning for all previous repeat findings; compliance dashboard live; access reviews automated. Next audit: zero repeat findings; audit prep takes days not weeks. Diagnostic question: when a vulnerability is found, is the response pathological (fingers pointed), bureaucratic (heads roll), or generative (seen as opportunity to improve)? Key principle: treat audit findings as engineering bugs with SLOs for remediation time — recurring findings trigger architectural reviews rather than point fixes; build compliance into default tooling so teams achieve audit readiness as a side effect. Measured by: compliance audit readiness (metric 9.3), repeat finding count.",
    "mappingNotes": "Recurring security audit failure scenario",
    "suggestedMetricIds": [
      "9.1",
      "9.3"
    ]
  },
  {
    "id": "P-C13-4",
    "slug": "balancing-security-risk-with-shipping-speed-after-a-close-call",
    "observableIds": [
      "C13-O1",
      "C13-O7"
    ],
    "capabilityIds": [
      "C13",
      "C8"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Balancing Security Risk With Shipping Speed After a Close Call",
    "context": "A near-miss security incident revealed that a feature shipped without a threat model — a misconfigured API endpoint was exposing customer PII to unauthenticated requests, caught in production monitoring before data was exfiltrated. Leadership wants to know how this happened and what you are doing to prevent it.",
    "topicsActivated": [
      "Security (Shift-Left Practices)",
      "Risk Management (Near-Miss Response)",
      "Process (Security Gates Without Bottlenecks)"
    ],
    "decisionFramework": "1. Treat the near-miss like an incident (Week 1): Run a blameless post-mortem on the near-miss. How did a feature touching user input ship without a security review? Map the failure in the process — not who to blame, but what structural gap allowed it. 2. Define security-sensitive triggers (Week 2): Not every feature needs a threat model. Define clear triggers: features touching authentication, user input, payments, PII, or external APIs require one. Everything else doesn't. This prevents security from becoming a bottleneck for routine work. 3. Implement proportional gates (Week 3-4): Add automated SAST scanning for all PRs (catches the SQL injection pattern). Add manual threat model requirement only for triggered features. Severity-based deployment gates: critical/high block, medium creates tracked ticket. 4. Measure the balance (Month 2+): Track both metrics: deployment lead time (didn't get slower) and vulnerability escape rate (trending toward zero). If security gates slow deployment by more than 10%, the gates are too heavy — tune them.",
    "commonMistakes": "Overreacting by requiring security review for everything (creates bottleneck, team routes around it). Underreacting because 'it didn't actually get exploited.' Adding a manual gate without automated scanning first (the opposite order of what works). Making security someone else's problem instead of the team's responsibility.",
    "whatGoodLooksLike": "Within 2 weeks: near-miss treated as learning opportunity; clear security triggers defined; automated scanning catching the class of vulnerability that escaped. Ongoing: deployment velocity maintained while security posture improves; team views security as 'how we work' rather than 'what slows us down'. Key principle: catching vulnerabilities before code is written is orders of magnitude cheaper than post-deploy remediation — require threat models for any feature touching authentication, authorization, or PII. Measured by: vulnerability escape rate, deployment lead time, security review coverage.",
    "mappingNotes": "Security near-miss response and proportional security gates scenario",
    "suggestedMetricIds": [
      "9.1",
      "1.2"
    ]
  },
  {
    "id": "P-C11-4",
    "slug": "scaling-your-interview-process-from-5-to-50-hires-per-year",
    "observableIds": [
      "C11-O1",
      "C11-O9",
      "C11-O11"
    ],
    "capabilityIds": [
      "C11"
    ],
    "relevantLevels": [
      4,
      5
    ],
    "title": "Scaling Your Interview Process From 5 to 50 Hires Per Year",
    "context": "Your team went from hiring 1-2 engineers per quarter to needing 10+ per quarter. Your ad-hoc interview process worked fine at low volume but now you're seeing: inconsistent evaluations between interviewers, candidates getting wildly different interview experiences, no data on what predicts success, and your best engineers spending 30% of their time interviewing.",
    "topicsActivated": [
      "Hiring (Process Standardization)",
      "Metrics (Hiring Funnel Analytics)",
      "Culture (Interview Quality at Scale)"
    ],
    "decisionFramework": "1. Design the interview funnel as a system (Week 1): Map each round to the specific, non-overlapping signal it provides. If two rounds test similar skills, consolidate. Target 4-5 total interactions (including recruiter screen) with a 2-week end-to-end target. 2. Build an interviewer training program (Week 2-3): At 50 hires/year, you need 15-20 calibrated interviewers. Create a training program: shadow 2 interviews, reverse-shadow 1, certified after calibration review. Track interviewer load and rotate to prevent burnout. 3. Automate coordination (Month 1): Scheduling at scale is a major bottleneck. Implement automated scheduling, standardized feedback forms with mandatory submission before debrief, and a candidate tracking dashboard. 4. Implement quality controls (Month 1-2): Track per-interviewer pass rates and flag outliers for recalibration. Monitor candidate-stage drop-off rates against industry benchmarks. Run quarterly calibration sessions using recorded mock interviews. 5. Protect candidate experience at scale (Ongoing): High volume creates a temptation to depersonalize. Set maximum response-time SLAs, assign a dedicated hiring coordinator, and run candidate experience surveys after every loop.",
    "commonMistakes": "Letting each interviewer design their own questions and evaluation criteria. Debriefing before individual scores are submitted (anchoring bias). Burning out your best engineers with interview load without tracking or rotating. Using the same 3 interviewers for every candidate instead of building a deep bench. Not automating scheduling — manual coordination becomes the bottleneck at scale. Adding more interview rounds instead of improving existing ones when a bad hire happens.",
    "whatGoodLooksLike": "Within 2 weeks: rubric adopted with behavioral anchors. Within 3 weeks: independent scoring before debrief enforced. Within 1 quarter: per-interviewer pass rates converge within 15% after calibration; funnel data informing process improvements monthly; time-to-hire decreasing while quality-of-hire stable or improving. Key principles: standardized rubrics enable hypothesis testing (e.g., 'does specific framework experience predict success?'); no single interviewer blocks or advances a candidate alone; independent evaluators prevent hiring standard drift. Measured by: inter-rater reliability, time-to-hire, quality-of-hire metrics.",
    "mappingNotes": "Interview process scaling and standardization scenario",
    "suggestedMetricIds": [
      "7.3"
    ]
  },
  {
    "id": "P-C14-4",
    "slug": "running-your-first-cross-team-performance-calibration",
    "observableIds": [
      "C14-O1",
      "C14-O7"
    ],
    "capabilityIds": [
      "C14"
    ],
    "relevantLevels": [
      4,
      5
    ],
    "title": "Running Your First Cross-Team Performance Calibration",
    "context": "You've been asked to lead calibration across 4 engineering teams (~30 engineers). Each manager has their own standards — some are generous, some are strict. There's no shared vocabulary for what 'exceeds expectations' means. You need consistent, fair outcomes.",
    "topicsActivated": [
      "Performance (Cross-Team Consistency)",
      "Leadership (Calibration Facilitation)",
      "Culture (Fairness and Bias Reduction)"
    ],
    "decisionFramework": "1. Pre-calibration prep (2 weeks before): Have each manager prepare evidence portfolios for their reports — specific examples, not vibes. Share the rating rubric with behavioral anchors for each level. Ask managers to pre-rate their reports independently. 2. Run pre-calibration 1:1s (1 week before): Meet each manager individually. Review their ratings. Challenge vague justifications ('she's a strong performer' → 'show me the specific impact'). Flag potential biases: recency bias (are examples only from last 6 weeks?), similarity bias (do all high performers look like the manager?), gender bias (are women rated on past performance while men are rated on potential?). 3. Calibration meeting (2-3 hours): Review by level, not by team — compare all Senior engineers across teams, then all Mids, etc. Focus discussion on borderline cases, not clear performers. Require specific evidence for every rating. Track the distribution across teams — if one manager rates everyone 'exceeds', that's a calibration problem. 4. Post-calibration (Same week): Each manager delivers feedback to their reports within 5 business days. Share one-sentence distillation of the key message — this is the sentence the engineer will remember. Follow up with written summary and development plan within 1 week.",
    "commonMistakes": "Running calibration without pre-calibration prep (becomes a 6-hour meeting). Accepting vague evidence ('strong performer' without specific examples). Not checking for bias patterns across the distribution. Treating calibration as ranking instead of consistency alignment. Delivering feedback weeks after calibration (information goes stale). Writing reviews the engineer will forget — distill the key message into one sentence.",
    "whatGoodLooksLike": "Pre-calibration catches 3-5 inconsistencies before the formal meeting. Calibration meeting takes under 3 hours because managers come prepared. Ratings consistent across teams at the same level. Zero 'surprise' feedback when managers deliver reviews. Distribution looks reasonable — not everyone exceeds expectations. Run pre-calibration sessions to give managers a safe space to practice presenting their cases while you call out bias, subjectivity, and inconsistency before the formal calibration. Measured by: rating revision rate post-calibration (metric 5.1), post-review fairness survey (metric 7.1).",
    "mappingNotes": "Cross-team performance calibration facilitation scenario",
    "suggestedMetricIds": [
      "5.1",
      "7.1"
    ]
  },
  {
    "id": "P-C4-4",
    "slug": "mobilizing-your-team-for-a-high-stakes-deadline",
    "observableIds": [
      "C4-O1",
      "C4-O12"
    ],
    "capabilityIds": [
      "C4",
      "C10"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Mobilizing Your Team for a High-Stakes Deadline",
    "context": "Your team has 6 weeks to deliver a major feature that would normally take 12. It's a real business-critical deadline (contract commitment, not artificial urgency). You need to supercharge execution without burning people out.",
    "topicsActivated": [
      "Operational Rhythm (Focused Execution)",
      "Resource Allocation (Temporary Concentration)",
      "Team Health (Sustainable Intensity)"
    ],
    "decisionFramework": "1. Validate the deadline (Day 1): Confirm this is genuinely immovable, not artificial urgency that will repeat. Beast Mode only works if it's rare and real. If it's the third 'critical deadline' this quarter, the problem isn't execution — it's planning. 2. Clear the deck (Day 1-3): Get stakeholder agreement to pause everything else. Every competing priority must be explicitly paused, not 'also done on the side.' If stakeholders won't pause other work, the deadline isn't actually the priority. 3. Structure for focus (Week 1): Split the team into pairs or small squads, each owning a vertical slice of the deliverable. Each squad chooses one task per sprint. Reduce meetings to the minimum: brief daily sync, weekly demo, nothing else. 4. Remove all friction (Ongoing): Your job is clearing blockers in real-time — unblock dependencies, make decisions fast, shield from distractions. You should not be writing code. You are the team's router. 5. Protect sustainability (Ongoing): Set explicit boundaries — no weekend work unless agreed, daily standup includes energy check, EM watches for burnout signals. The goal is focused intensity, not heroics. 6. Wind down (After delivery): Explicit cooldown period. Celebrate the achievement. Retrospective on what worked. Resume normal operations. Don't let Beast Mode become the new baseline.",
    "commonMistakes": "Making every sprint a 'Beast Mode' (it stops working and burns people out). Not pausing other work (team is doing Beast Mode + everything else). EM trying to contribute code instead of clearing blockers. No explicit wind-down — Beast Mode energy becomes expected baseline. Using Beast Mode to compensate for chronic under-resourcing or poor planning.",
    "whatGoodLooksLike": "Within 3 days: everything non-critical paused. During sprint: pairs/squads own vertical slices with high autonomy; daily blockers cleared within hours. Outcome: feature delivered on time; no burnout — team tired but proud. Measured by: on-time delivery, team burnout signals post-sprint, zero weekend work.",
    "mappingNotes": "Time-constrained high-stakes execution scenario",
    "suggestedMetricIds": [
      "1.2",
      "1.4"
    ]
  },
  {
    "id": "P-C12-5",
    "slug": "building-team-culture-through-intentional-rituals",
    "observableIds": [
      "C12-O2",
      "C12-O4",
      "C12-O7"
    ],
    "capabilityIds": [
      "C12"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Building Team Culture Through Intentional Rituals",
    "context": "Your team ships reliably but lacks cohesion. Engineers don't understand business metrics, don't contribute to roadmap discussions, and treat culture as 'something HR does.' Retros feel perfunctory and nobody proposes ambitious improvements. You want to move from a group of individuals to a genuine team.",
    "topicsActivated": [
      "Culture (Ritual Design)",
      "Engagement (Business Context Sharing)",
      "Innovation (Structured Ideation)"
    ],
    "decisionFramework": "1. Start with metrics visibility (Month 1): Establish a monthly 'numbers review' — a 45-minute meeting where engineering, product, design, and relevant business stakeholders review key metrics together. The goal isn't accountability theater; it's giving engineers the context they lack. When engineers understand business metrics, they make better technical trade-offs without being told. 2. Add structured ideation (Month 2): Monthly session where any team member can propose product, marketing, or process improvements. Key: proposals must include a lightweight problem statement and expected impact. This isn't a free-for-all brainstorm; it's a structured channel for bottom-up innovation. Engineers who contribute to the roadmap feel ownership over outcomes. 3. Create a 10X improvement space (Month 3): Bi-monthly session dedicated to ambitious technical improvements — monitoring stacks, automation, architecture evolution. The rule: ideas must aim for 10x improvement, not 10%. This gives engineers permission to think big and surfaces strategic technical investments. 4. Iterate and prune: Not every ritual will land. After 3 months, retrospect on the rituals themselves. Drop what isn't working. The goal is 2-3 high-value rituals, not a packed calendar.",
    "commonMistakes": "Adding rituals without removing anything, creating meeting overload. Making attendance mandatory without explaining purpose. Letting meetings become status updates instead of genuine discussions. Starting all four rituals at once instead of building incrementally. Not involving the team in choosing which rituals to adopt.",
    "whatGoodLooksLike": "Within 3 months: engineers can articulate team KPIs without prompting; at least one bottom-up idea ships per quarter; team proactively proposes technical improvements. Key principle: these rituals aren't add-ons — they replace the context and ownership that agile ceremonies alone don't provide; explicit team charters covering mission, working agreements, and decision-making authority should be reviewed quarterly and referenced during onboarding. Measured by: team engagement (metric 5.1), bottom-up idea count, technical improvement proposals.",
    "mappingNotes": "Based on LeadDev 'rituals that revolutionized engineering teams' — Numberzz, Ideation, and 10X patterns.",
    "suggestedMetricIds": [
      "5.1",
      "6.4",
      "7.8"
    ]
  },
  {
    "id": "P-C12-6",
    "slug": "rebuilding-psychological-safety-after-organizational-disruption",
    "observableIds": [
      "C12-O1",
      "C12-O6",
      "C12-O7"
    ],
    "capabilityIds": [
      "C12",
      "C14"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Rebuilding Psychological Safety After Organizational Disruption",
    "context": "Your organization just went through layoffs, a major re-org, or leadership change. The remaining team is disengaged — people avoid speaking up in meetings, nobody challenges decisions, risk-taking has stopped, and your best performers are quietly interviewing. Trust has evaporated.",
    "topicsActivated": [
      "Culture (Psychological Safety Recovery)",
      "People (Trust Rebuilding)",
      "Leadership (Vulnerability and Transparency)"
    ],
    "decisionFramework": "1. Acknowledge reality immediately (Week 1): Do not pretend things are normal. In your first team meeting post-disruption, name what happened directly. Share what you know and what you don't. People's anxiety comes from uncertainty, not from bad news. The worst response is silence. 2. Listen before acting (Week 1-2): Run individual check-ins with every team member. Do not start with 'here's the plan going forward.' Start with 'how are you doing, and what are you worried about?' Write down themes but don't promise fixes yet. 3. Re-establish predictability (Week 2-4): People need to know what won't change. Reaffirm team mission, working agreements, and key processes. Publish a simple 30-60-90 plan. Predictability rebuilds the foundation of safety. 4. Model vulnerability (Ongoing): Share your own uncertainty. 'I don't have all the answers' is more powerful than a confident facade. Thank people publicly for disagreeing or raising concerns. 5. Create low-stakes participation (Month 2+): Start meetings with brief check-ins. Use written-first formats for sensitive discussions. Create anonymous feedback channels. Rebuild contributor safety gradually — people need to see that speaking up is safe before they'll do it.",
    "commonMistakes": "Jumping straight to 'rallying the troops' with false optimism. Avoiding the topic entirely and hoping time heals. Over-communicating strategy when people need emotional acknowledgment first. Treating symptoms (low velocity) instead of root cause (low trust). Expecting psychological safety to return on a timeline you control.",
    "whatGoodLooksLike": "Within 60 days: team members raise concerns in meetings again; at least one person publicly disagrees constructively; attrition stabilizes. Within 90 days: team health survey shows improvement. Key principle: consistency — not grand gestures — rebuilds trust; psychological safety is the single strongest predictor of high-performing teams; teams where members feel safe to take interpersonal risks outperform teams optimized for individual talent. Measured by: team health survey scores, attrition rate (metric 7.1), qualitative indicators of speaking up.",
    "mappingNotes": "Based on LeadDev articles on psychological safety after layoffs and the REST framework for post-layoff leadership.",
    "suggestedMetricIds": [
      "5.1",
      "7.8"
    ]
  },
  {
    "id": "P-C10-5",
    "slug": "navigating-your-first-annual-engineering-budget-cycle",
    "observableIds": [
      "C10-O1",
      "C10-O6",
      "C10-O7"
    ],
    "capabilityIds": [
      "C10",
      "C1"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Navigating Your First Annual Engineering Budget Cycle",
    "context": "You've been promoted to a role where you now own budget planning for the first time. You need to present a headcount plan, tooling budget, and infrastructure forecast to finance and your VP. You've never done this before and don't want to show up with a wishlist that gets cut in half.",
    "topicsActivated": [
      "Resource Allocation (Budget Planning Process)",
      "Stakeholder (Cross-Functional Budget Alignment)",
      "Strategy (ROI-Driven Investment Cases)"
    ],
    "decisionFramework": "1. Understand last year first (Month before cycle): Get last year's approved budget, actual spend, and variance. Understand what was approved, what was actually spent, and why they differed. This is your baseline credibility — showing you understand history before proposing the future. 2. Build three tiers (During planning): For every major ask, present three options with quantified trade-offs. Tier 1 (minimum viable): what you need to keep the lights on. Tier 2 (recommended): what you need to hit committed goals. Tier 3 (investment): what would accelerate beyond current targets. Each tier has a number, a deliverable, and a trade-off. 3. Present with product as a united front: Align with your product counterpart before the budget meeting. Non-technical leaders see a joint eng+product plan as alignment, giving them confidence. A solo engineering budget request looks like empire building. 4. Bring data, not opinions: Every headcount request links to a deliverable. Every tooling request includes current cost and projected savings. Don't say 'we need more people' — say 'to deliver X by Q3, we need Y engineers; without them, the timeline extends to Q1 next year.' 5. Plan for what you'll cut: Finance will likely reduce your ask by 15-25%. Decide in advance which tier-3 items you'd sacrifice. Being prepared for the cut earns more trust than fighting it.",
    "commonMistakes": "Submitting a budget in isolation without product alignment. Asking for headcount without linking it to deliverables. Not knowing last year's actual spend. Presenting a single number instead of tiered options. Getting emotional when the budget gets cut instead of having a prepared fallback position.",
    "whatGoodLooksLike": "Within 1 revision cycle: budget approved. Finance trusts your numbers because they're grounded in historical data and linked to deliverables. You get 80-90% of your tier-2 ask. Key principle: present headcount proposals with explicit ROI narratives — each role justified by revenue impact, risk mitigation, or strategic initiative enablement; use productivity models that project feature velocity per engineer to quantify capacity gaps. Measured by: cloud cost efficiency (metric 10.1), engineering investment mix (metric 8.4).",
    "mappingNotes": "Based on LeadDev articles on annual budget planning best practices and overcoming budget planning challenges.",
    "suggestedMetricIds": [
      "10.1",
      "10.3"
    ]
  },
  {
    "id": "P-C2-3",
    "slug": "everything-is-priority-zero-and-your-team-is-drowning",
    "observableIds": [
      "C2-O2",
      "C2-O5",
      "C2-O3"
    ],
    "capabilityIds": [
      "C2",
      "C10"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Everything Is Priority Zero and Your Team Is Drowning",
    "context": "Every initiative in flight has been labeled 'highest priority' by a different stakeholder. Your team is context-switching between 5 workstreams, nothing is finishing, and engineers are frustrated. You tried pushing back but each stakeholder explains why theirs is genuinely the most urgent. Sprint commitments are meaningless.",
    "topicsActivated": [
      "Strategy (Ruthless Prioritization)",
      "Stakeholder (Priority Negotiation)",
      "Execution (Focus and Throughput)"
    ],
    "decisionFramework": "1. Make the cost visible (Day 1): List every active workstream with current allocation and expected completion date at current pace. Show stakeholders the math: 5 parallel priorities with 8 engineers means each gets 1.6 engineers and nothing ships for months. Parallelism has a cost and it's visible when you write it down. 2. Force-rank with a framework (Week 1): Use a simple scoring model — business impact, urgency (reversibility of delay), and confidence. Score each initiative. The framework doesn't have to be perfect; its value is making the ranking conversation explicit rather than political. Present the ranked list to leadership and ask: 'Do you agree with this order? If not, which two would you swap and why?' 3. Establish a 'not doing' list (Week 1): Publish what you're explicitly not doing and why. This is more important than the doing list. It forces acknowledgment that trade-offs exist. A highly visible task does not need to become your highest priority the moment you learn about it — but you should communicate where it sits on your list and why. 4. Limit active work (Week 2+): Set a WIP limit: no more than 2-3 active workstreams at a time. New priorities can only enter when something exits. This is not about saying no — it's about saying 'not yet, and here's when.' 5. Review monthly: Priorities shift. What wasn't important yesterday can become urgent tomorrow. Run a monthly priority review with stakeholders. This makes reprioritization a system, not a crisis.",
    "commonMistakes": "Trying to keep everyone happy by spreading the team thin across all priorities. Prioritizing based on who asks loudest rather than business impact. Not publishing the 'not doing' list because it feels confrontational. Using a framework so complex nobody trusts it. Failing to revisit priorities as context changes.",
    "whatGoodLooksLike": "Within 2 weeks: team is focused on 2-3 priorities, shipping tempo increases. Within 1 month: stakeholders reference the priority list in their own planning; 'not doing' list is shared and acknowledged. Ongoing: prioritization is a system, not a crisis — monthly reviews adjust for context changes. Measured by: WIP count (target 2-3 active workstreams), engineering investment mix (metric 8.4), carryover rate (metric 2.6).",
    "mappingNotes": "Based on LeadDev 'Avoiding the priority zero trap' and 'Building a prioritization framework.'",
    "suggestedMetricIds": [
      "2.2",
      "2.6",
      "8.4"
    ]
  },
  {
    "id": "P-C3-4",
    "slug": "managing-tech-debt-strategically-without-stopping-the-roadmap",
    "observableIds": [
      "C3-O4",
      "C3-O11",
      "C3-O1"
    ],
    "capabilityIds": [
      "C3",
      "C9"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Managing Tech Debt Strategically Without Stopping the Roadmap",
    "context": "Your team has significant tech debt that's slowing delivery, but you can't stop the roadmap for a multi-month cleanup. Product is sympathetic but won't give you a quarter of dedicated refactoring time. Engineers are frustrated by working around old decisions. Previous 'tech debt sprints' produced feel-good cleanup that didn't move the needle on actual velocity.",
    "topicsActivated": [
      "Architecture (Systematic Debt Management)",
      "Strategy (Capacity Allocation)",
      "Metrics (Impact Quantification)"
    ],
    "decisionFramework": "1. Inventory and score (Week 1-2): Create a lightweight tech debt register. For each item, score three things: severity (how much does it slow us down daily?), strategic impact (does it block roadmap items?), and effort to resolve. Use a simple 1-5 scale. The goal isn't precision — it's creating shared language for comparing debts across the team. 2. Find the leverage points: The insight from experience is that debt isn't solved by scope — it's solved by relevance. Look for the 2-3 debt items blocking the most roadmap work. An e-commerce team found three requested features were blocked by just two architectural decisions from four years ago. Fixing those two items unblocked months of roadmap. 3. Establish a capacity allocation (Month 1): Adopt a 70/20/10 model — 70% on roadmap delivery, 20% on medium-term technical health (the debt items blocking roadmap), and 10% on longer-term cleanup or experiments. This gives product predictability and engineers breathing space. 4. Quantify in business terms (Ongoing): Frame debt as risk and velocity, not engineering purity. 'This service has 47 known issues and our deploy frequency has dropped from daily to weekly' is more compelling to leadership than 'the architecture needs modernizing.' 5. Make progress visible (Ongoing): Track the debt register publicly. Show items resolved, velocity improvements, and incidents prevented. Treat tech debt like a product backlog — prioritized, visible, and regularly groomed.",
    "commonMistakes": "Trying to fix all debt at once instead of targeting highest-leverage items. Framing debt as an engineering concern rather than a business risk. Running 'tech debt sprints' that clean up low-impact items for morale but don't improve velocity. Not tracking the impact of debt reduction, so leadership sees it as overhead. Allocating 0% to tech health because 'we'll get to it after the next launch.'",
    "whatGoodLooksLike": "Within 1 month: debt register created with scored items; 70/20/10 allocation adopted in sprint planning. Within 3 months: measurable improvement in deploy frequency or cycle time from targeted debt reduction. Ongoing: living debt register referenced by product during planning; 70/20/10 allocation respected in sprint planning. Measured by: deployment frequency (metric 2.3), cycle time (metric 3.2), debt registry item count trend.",
    "mappingNotes": "Based on LeadDev 'How to create a tech debt strategy that works' (70/20/10 model), 'Practical tech-debt prioritization', and tech debt scoring frameworks.",
    "suggestedMetricIds": [
      "2.3",
      "3.2"
    ]
  },
  {
    "id": "P-C7-4",
    "slug": "making-a-high-stakes-technical-decision-under-time-pressure",
    "observableIds": [
      "C7-O1",
      "C7-O10",
      "C7-O11"
    ],
    "capabilityIds": [
      "C7",
      "C3"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Making a High-Stakes Technical Decision Under Time Pressure",
    "context": "You need to decide between two architectural approaches for a critical project. Both have significant trade-offs. Your team is split. Leadership wants an answer by Friday. The wrong choice will be expensive to reverse. You're feeling pressure to either over-analyze (missing the deadline) or decide too fast (regret later). Previous big decisions have been revisited repeatedly because they weren't well-documented.",
    "topicsActivated": [
      "Decision Making (Structured Under Pressure)",
      "Communication (Decision Documentation)",
      "Architecture (Trade-off Analysis)"
    ],
    "decisionFramework": "1. Classify the decision (Hour 1): Is this a one-way door (irreversible, high cost to change) or a two-way door (reversible, low switching cost)? One-way doors deserve deliberation. Two-way doors should be made fast and revisited if needed. If you're not sure, default to two-way and use feature flags to contain blast radius. 2. Structure the options (Day 1): For each option, document three things: what it optimizes for, what it sacrifices, and what assumptions it depends on. Avoid framing as 'good option vs. bad option' — instead, frame as 'Option A optimizes for X at the cost of Y; Option B optimizes for Y at the cost of X.' 3. Seek disconfirming input (Day 1-2): Ask the strongest advocate of the option you're leaning against to make their best case. Run a 15-minute pre-mortem: 'It's 6 months from now and this decision was a disaster — what happened?' This surfaces risks your confidence might be hiding. 4. Decide and document (Day 2-3): Make the call. Write an Architecture Decision Record (ADR): context, options considered, decision, and rationale. The documentation matters because every decision creates a policy — future teams will encounter similar choices and need to understand why you chose what you chose. 5. Plan a decision retro (Schedule for 30/60/90 days out): After 30 days, run a 15-minute retro: 'What worked, what felt unclear, would we make the same call?' This normalizes learning from decisions and reduces the fear of making them.",
    "commonMistakes": "Analysis paralysis — waiting for perfect information that will never come. Making the decision in your head and announcing it without structured input. Treating the decision as purely technical when it has organizational and people implications. Not documenting the rationale, leading to the same debate resurfacing months later. Deciding quickly to relieve pressure without considering trade-offs.",
    "whatGoodLooksLike": "Within timeline: decision made and ADR published with context, options, decision, and rationale. Within 2 weeks: team executing confidently; dissenters feel heard even if they disagreed. At 30 days: retro confirms the decision or surfaces learnings for the next one. Ongoing: ADR referenced by other teams facing similar choices. Decision-making is about creating structure when things are messy — categorize decisions by reversibility (one-way doors get deliberation, two-way doors get bias for action) and require written proposals with alternatives analysis. Measured by: design doc count (metric 2.2), sprint velocity trend (metric 2.5), engineering investment mix (metric 8.3).",
    "mappingNotes": "Based on LeadDev 'Mastering tough technical decisions', 'How to make the right decisions under pressure', and 'Every decision creates a policy' (ADR/documentation emphasis).",
    "suggestedMetricIds": [
      "2.2",
      "2.5",
      "8.3"
    ]
  },
  {
    "id": "P-C11-5",
    "slug": "your-onboarding-is-broken-and-new-hires-are-struggling",
    "observableIds": [
      "C11-O3",
      "C11-O4"
    ],
    "capabilityIds": [
      "C11",
      "C12"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Your Onboarding Is Broken and New Hires Are Struggling",
    "context": "New engineers take 6+ months to become productive. They report feeling lost in their first weeks, unsure who to ask for help, and overwhelmed by undocumented tribal knowledge. Your most recent hire quietly admitted in their 1:1 that they considered leaving in month two. Meanwhile, your tenured engineers complain they spend too much time answering basic questions.",
    "topicsActivated": [
      "Onboarding (Structured Ramp Program)",
      "Culture (Knowledge Sharing and Belonging)",
      "Metrics (Time-to-Productivity Measurement)"
    ],
    "decisionFramework": "1. Diagnose the gaps (Week 1): Interview your 3 most recent hires about their first 90 days. Ask: 'What did you wish you'd known on day one? When did you first feel productive? What almost made you leave?' Also ask tenured engineers: 'What questions do new hires always ask that shouldn't require asking?' The gap between these two perspectives reveals your onboarding holes. 2. Build progressive structure (Week 2-3): Design a 30/60/90 day plan with specific milestones — not 'learn the codebase' but 'deploy your first change to production by day 10, own a small feature by day 30, lead a design discussion by day 60.' Progressive information introduction works: team, then tech stack, then product domain, then first meaningful project. 3. Assign a buddy (not the manager): Every new hire gets a peer-level buddy for 90 days. The buddy's role is distinct from the manager's: the buddy covers 'where things live, how to ask for help, unwritten team norms.' The manager covers 'expectations, career, feedback.' Early buddy 1:1s dramatically increase comfort in asking questions. 4. Make knowledge discoverable: Audit the most common questions new hires ask and document the answers. This isn't about comprehensive documentation — it's about eliminating the 10 questions that every new hire asks in their first month. 5. Measure and iterate (Quarterly): Track time-to-first-deploy, time-to-first-feature, and run an onboarding exit survey at 90 days. Each cohort should onboard faster than the last. Act on the survey feedback — the fixes are usually small and high-leverage.",
    "commonMistakes": "Throwing new hires at the codebase with 'just read the code.' Assigning the manager as the only point of contact, creating a bottleneck. Having no milestones, so nobody knows if onboarding is going well or not. Blaming the hire for 'not being a good fit' when onboarding set them up to fail. Never collecting or acting on feedback from recent hires.",
    "whatGoodLooksLike": "Within 5 days: first deploy completed. Within 90 days: exit survey satisfaction above 8/10; no new hire considers leaving. Ongoing: tenured engineers report fewer repeated questions; each new cohort ramps faster than the last. Target: full productivity in 3-4 months (vs. 6-7 month average). Key principle: structured onboarding with buddy pairing, progressive task complexity, and 30/60/90 milestones achieves consistent time-to-productivity. Measured by: time-to-first-deploy, time-to-first-feature, 90-day survey satisfaction.",
    "mappingNotes": "Based on LeadDev '5 ways onboarding can accelerate engineering efficiency' and 'How to successfully onboard remote engineering staff in four weeks.'",
    "suggestedMetricIds": [
      "7.5",
      "5.1"
    ]
  },
  {
    "id": "P-C13-5",
    "slug": "responding-to-a-critical-dependency-vulnerability",
    "observableIds": [
      "C13-O2",
      "C13-O6",
      "C13-O1"
    ],
    "capabilityIds": [
      "C13",
      "C8"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Responding to a Critical Dependency Vulnerability",
    "context": "A CVE is published for a widely-used open source dependency in your stack (think Log4Shell, XZ Utils-class events). The vulnerability is rated critical. Your team doesn't know how many services are affected, there's no bill of materials, and leadership is asking for a status update. Security is asking when you'll be patched. You're not sure where to start.",
    "topicsActivated": [
      "Security (Supply Chain Vulnerability Response)",
      "Operations (Rapid Dependency Assessment)",
      "Process (SBOM and Dependency Governance)"
    ],
    "decisionFramework": "1. Assess blast radius (Hour 1-4): Before patching anything, answer: where is this dependency? If you have dependency scanning in CI/CD, query it. If not, run a manual search across all repos. Build a list of affected services ranked by exposure (internet-facing > internal > dev-only). This is your battlefield map. 2. Triage by exposure (Hour 4-8): Not everything needs to be patched simultaneously. Internet-facing services with the vulnerable code path active get patched first. Services behind authentication or internal-only are next. Dev/test environments are last. Communicate this triage to security and leadership — they need to see a plan, not just effort. 3. Patch and verify (Day 1-3): For each affected service: update the dependency, run tests, deploy to staging, verify no regression, deploy to production. If a clean upgrade isn't possible, evaluate workarounds (WAF rules, configuration changes, feature disabling) as temporary mitigations. 4. Communicate continuously: Send status updates at fixed intervals (every 4 hours during active response, then daily). Include: services patched, services remaining, blockers, ETA. Leadership doesn't need technical details — they need 'X of Y services patched, ETA for full remediation is Z.' 5. Prevent recurrence (Week 2+): This is where the real work starts. Implement automated dependency scanning in CI/CD with severity-based gates. Create a software bill of materials (SBOM) so you never have to ask 'where is this dependency?' again. Establish vulnerability SLAs by severity (critical: 24-48hrs, high: 7 days, medium: 30 days).",
    "commonMistakes": "Panicking and trying to patch everything at once without triaging by exposure. Not knowing your dependency tree because there's no scanning or SBOM. Patching production without adequate testing, creating a new incident. Over-communicating technical details to leadership instead of status and ETA. Treating it as a one-time fire drill instead of building systematic prevention.",
    "whatGoodLooksLike": "Within 48 hours: all critical-exposure services patched. Within 7 days: full remediation complete. Within 30 days: SBOM established for all production services. Within 60 days: dependency scanning in CI/CD operational. Outcome: next critical CVE blast radius assessment takes minutes instead of hours because the SBOM exists. Key principle: supply chain attacks are becoming more sophisticated — automated dependency vulnerability detection integrated into CI/CD catches critical CVEs within hours rather than days. Measured by: vulnerability remediation time (metric 9.2), SBOM coverage.",
    "mappingNotes": "Based on supply chain security trends, XZ Utils incident analysis, and SBOM best practices.",
    "suggestedMetricIds": [
      "9.2",
      "3.5"
    ]
  },
  {
    "id": "P-C14-5",
    "slug": "putting-an-engineer-on-a-pip-designed-to-succeed",
    "observableIds": [
      "C14-O6",
      "C14-O4",
      "C14-O7"
    ],
    "capabilityIds": [
      "C14"
    ],
    "relevantLevels": [
      1,
      2,
      3
    ],
    "title": "Putting an Engineer on a PIP Designed to Succeed",
    "context": "An engineer on your team has been underperforming for months despite verbal feedback and informal coaching. You've documented the pattern and HR agrees a formal Performance Improvement Plan is appropriate. You've never run a PIP before and you're unsure how to make it genuinely developmental rather than just a paper trail to termination. The engineer is anxious and the team is watching.",
    "topicsActivated": [
      "Performance (Formal Improvement Process)",
      "People (Developmental Support During PIP)",
      "Process (Documentation and Fairness)"
    ],
    "decisionFramework": "1. Before the PIP — exhaust informal paths (Pre-PIP): Ensure you've delivered clear, specific feedback using SBI (Situation-Behavior-Impact) at least 2-3 times with documented follow-up. If you haven't told someone clearly that their performance is below expectations, you haven't earned the right to put them on a PIP. The PIP should never be the first signal. 2. Define clear, measurable objectives (Day 1): The PIP document must specify exactly what 'meeting expectations' looks like — not 'improve code quality' but 'reduce escaped defects to <2 per sprint and complete code reviews within 24 hours.' Each objective needs a measurable success criterion and a timeline (typically 30-60 days). Ambiguous PIPs fail both the employee and you. 3. Show explicit support (Ongoing): Increase 1:1 frequency to weekly. Assign a peer mentor if appropriate (with their consent and without disclosing the PIP context). Remove obstacles to success — if the engineer needs training, pairing sessions, or reduced scope to focus, provide it. The goal is genuine support, not performative documentation. 4. Check in formally at midpoint: At the halfway mark, provide written feedback on progress against each objective. If the engineer is improving, say so explicitly. If not, be direct about what's still falling short and what needs to change in the remaining time. No surprises at the end. 5. Conclude with integrity: If objectives are met — celebrate genuinely. Remove the PIP, acknowledge the growth, and continue regular performance management. If objectives are not met — the outcome should not surprise the engineer. You've been transparent throughout. Proceed with the agreed consequence (role change, exit) with dignity. Either outcome, do a personal retrospective: could you have intervened earlier or more effectively?",
    "commonMistakes": "Using the PIP as a surprise weapon when feedback has been vague or absent. Writing objectives so ambiguous that success is impossible to prove or disprove. Treating the PIP as a formality to terminate rather than a genuine attempt at improvement. Isolating the engineer — reducing their scope so much they can't demonstrate capability. Not checking in regularly, then delivering a final verdict with no warning. Discussing the PIP with other team members.",
    "whatGoodLooksLike": "Engineer either successfully improves and remains a productive team member, or departs understanding why. No procedural issues raised by HR or the employee. Team observes fair process even if they don't know the details. PIPs should be a last resort, not a first tool — effectiveness depends entirely on genuine commitment to the person's success. Use a structured escalation (informal feedback → documented coaching → formal PIP) where each stage has explicit criteria, timelines, and documentation requirements. Measured by: PIP-to-resolution cycle time, post-PIP retention or clean exit rate (metric 7.1), zero HR procedural issues (metric 5.1).",
    "mappingNotes": "Based on LeadDev 'Driving positive change through performance improvement plans' and 'The relentless rise of the PIP.'",
    "suggestedMetricIds": [
      "7.1",
      "5.1",
      "7.2"
    ]
  },
  {
    "id": "P-C7-5",
    "slug": "your-project-is-going-to-miss-its-deadline",
    "observableIds": [
      "C7-O4",
      "C7-O3",
      "C7-O2"
    ],
    "capabilityIds": [
      "C7",
      "C5"
    ],
    "relevantLevels": [
      1,
      2,
      3
    ],
    "title": "Your Project Is Going to Miss Its Deadline",
    "context": "You're three weeks from a committed deadline and you've realized the team won't make it. The gap isn't small — you're looking at 2-4 weeks of additional work. Your VP has already communicated the date to customers. Product is counting on this for their quarterly goals. You're dreading the conversation but every day you delay makes it worse.",
    "topicsActivated": [
      "Communication (Bad News Delivery)",
      "Stakeholder (Trust Preservation)",
      "Execution (Recovery Planning)"
    ],
    "decisionFramework": "1. Own it before you share it (Hour 1): Before telling anyone, reflect on what led here. Could you have flagged this earlier? What signals did you miss? This isn't self-blame — it's preparation. When you walk into the room, you need to own the situation credibly. Leaders who accept responsibility earn trust even when delivering bad news. 2. Prepare the full picture (Day 1): Before the conversation, have answers to: How late will we be? What's the revised realistic date? What caused the slip? What options exist to reduce scope or parallelize? Don't walk in with just 'we're going to be late' — walk in with 'we're going to be late, here's by how much, here's why, and here are three options.' 3. Deliver early and directly (Day 1): Do not bury the lede. Open with the news: 'We're going to miss the March 15 date. Our realistic delivery is March 28. Here's what happened and what I recommend.' Resist the urge to soften with preamble. Stakeholders will respect directness more than disclosure that feels reluctant. 4. Present options, not just problems: Option A: Ship reduced scope on time. Option B: Ship full scope 2 weeks late. Option C: Add resources to compress by 1 week. Each option has trade-offs — present them clearly and make a recommendation. 5. Manage your emotions and theirs: In tense moments, pause and breathe. Name emotions if needed: 'I understand this is frustrating — this deadline matters and I take the miss seriously.' Don't get defensive. Don't blame the team. After the conversation, communicate the revised plan to your team transparently.",
    "commonMistakes": "Waiting until the deadline to disclose the miss, hoping for a miracle. Blaming the team or external factors instead of owning the situation. Presenting the problem without options or a recovery plan. Over-promising a compressed timeline to reduce tension in the moment, only to miss again. Soft-pedaling the news so stakeholders don't grasp the severity.",
    "whatGoodLooksLike": "Within 24 hours of recognizing the miss: stakeholders informed with revised timeline and options. Within 1 week: revised plan communicated to team transparently; team feels supported, not thrown under the bus. Outcome: stakeholder is disappointed but trusts you more because you were honest and prepared; revised timeline holds. The organizational expectation should be that bad news travels fast and escalation is rewarded — make failures visible to destigmatize bad news and accelerate learning. Measured by: on-time delivery rate (metric 2.1), sprint predictability (metric 2.4), deployment frequency (metric 1.2).",
    "mappingNotes": "Based on LeadDev 'Getting good at delivering bad news' and deadline management articles.",
    "suggestedMetricIds": [
      "2.1",
      "2.4",
      "1.2"
    ]
  },
  {
    "id": "P-C9-5",
    "slug": "introducing-engineering-metrics-to-a-team-that-has-none",
    "observableIds": [
      "C9-O10",
      "C9-O5",
      "C9-O1"
    ],
    "capabilityIds": [
      "C9",
      "C7",
      "C4"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Introducing Engineering Metrics to a Team That Has None",
    "context": "Your engineering organization has no systematic metrics. Leadership makes decisions based on gut feel and anecdotes. When asked 'how is engineering doing?' you struggle to answer beyond 'we shipped X features.' You may not even have the tooling prerequisites — no CI/CD pipeline, manual deployments, no incident tracking — which means jumping straight to DORA or SPACE feels premature. You want to introduce metrics but you're worried about creating a surveillance culture or picking the wrong things to measure.",
    "topicsActivated": [
      "Metrics (Cold Start Strategy)",
      "Culture (Measurement Without Surveillance)",
      "Communication (Data-Driven Storytelling)"
    ],
    "decisionFramework": "1. Assess readiness and start with one metric (Month 1): Before adopting any framework, check your tooling prerequisites — DORA requires automated deployment, version control, and incident tracking as data sources. If those don't exist, DORA metrics will be manually fabricated. If you have to pick one metric, pick cycle time — the elapsed time from first commit to production deploy. It benefits the business, correlates with code quality, and correlates with developer satisfaction. One metric, well-understood, beats a dashboard nobody trusts. 2. Address engineers' problems first (Month 1-2): Introduce metrics that solve problems engineers already feel. If they complain about slow deploys, measure deployment frequency. If they complain about flaky tests, measure test reliability. When metrics address real pain, adoption is natural. Simultaneously, invest in CI/CD and deployment automation as prerequisites for meaningful measurement later. 3. Add process metrics gradually (Month 2-3): Layer in throughput and flow efficiency to identify bottlenecks. These are diagnostic tools, never performance measures for individuals. Be explicit: 'We measure systems, not people.' 4. Graduate to DORA when ready (Month 3-4): Once automated data sources exist, DORA metrics become meaningful. Add deployment frequency, lead time for changes, change failure rate, and MTTR. Connect to business outcomes by mapping engineering metrics to business impact: deployment frequency → feature velocity → time to market; cycle time → iteration speed → customer responsiveness. 5. Evolve beyond DORA (Month 6+): DORA captures throughput and stability but misses developer experience, learning, and innovation. Add developer satisfaction surveys and outcome metrics when DORA is well-established. Resist the temptation to adopt DORA, SPACE, or DX Core 4 on day one — start simple, build baseline data, then evaluate which framework fits your organization's maturity.",
    "commonMistakes": "Launching a comprehensive dashboard on day one that overwhelms everyone. Measuring individual developer output (lines of code, commits, PRs) rather than system flow. Not explaining why metrics are being introduced, triggering fear. Using metrics punitively, destroying the trust needed for honest measurement. Implementing DORA metrics without the tooling to generate accurate data. Manually tracking DORA metrics (defeats the purpose). Telling leadership 'we can't measure anything' instead of proposing a maturity-appropriate starting point.",
    "whatGoodLooksLike": "Within 2 weeks: 3 baseline metrics chosen (cycle time, deployment frequency, escaped defects). Within 1 month: first data available and reviewed in team standup. Within 1 quarter: team makes at least one decision based on metric data. Outcome: team develops measurement habit without resentment. Measured by: decisions driven by data, team engagement with metrics.",
    "mappingNotes": "Based on LeadDev 'Introducing engineering metrics to your organization', 'Engineering metrics at every level', and 'The flawed five engineering productivity metrics.'",
    "suggestedMetricIds": [
      "1.1",
      "2.2",
      "2.1",
      "1.2"
    ]
  },
  {
    "id": "P-C2-4",
    "slug": "killing-a-project-that-should-have-died-months-ago",
    "observableIds": [
      "C2-O3",
      "C2-O4",
      "C2-O2"
    ],
    "capabilityIds": [
      "C2",
      "C7"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Killing a Project That Should Have Died Months Ago",
    "context": "A major initiative has been running for 6+ months with growing scope, shifting requirements, and declining team morale. Nobody believes it will deliver the original value, but there's significant sunk cost — engineering hours, leadership credibility, customer promises. You suspect the right move is to kill it, but everyone is afraid to be the one to say it.",
    "topicsActivated": [
      "Strategy (Sunk Cost Decision Making)",
      "Communication (Delivering Project Cancellation)",
      "People (Team Morale After Cancellation)"
    ],
    "decisionFramework": "1. Separate sunk cost from future value (Day 1): The money and time already spent are gone regardless. The only question is: 'Starting from today, is the remaining investment justified by the expected outcome?' If the answer is no, the project should stop. This is harder than it sounds because teams conflate 'we've invested so much' with 'therefore we should continue.' Name the sunk cost fallacy explicitly. 2. Build the kill case with data (Week 1): Document the project's trajectory — original timeline vs. current, original scope vs. current, team velocity trend, customer feedback signal. Compare the remaining investment to alternative uses of the same capacity. What else could this team ship in the same timeframe? Present this as an investment decision, not a failure acknowledgment. 3. Get alignment before announcing (Week 1-2): Have private conversations with product, your VP, and key stakeholders before any public announcement. The conversation is: 'Given what we know now, continuing costs X and delivers Y. Stopping frees capacity for Z. I recommend stopping.' Give them space to process. Nobody likes killing something they championed. 4. Communicate with dignity (Day of announcement): To the team: acknowledge their effort explicitly. 'The work you did was good. The decision to stop reflects changed circumstances, not your execution.' Name what was learned and how it applies going forward. Nothing is more demoralizing than having months of work dismissed as wasted. 5. Redirect immediately (Week 2+): Don't leave the team in limbo. Have the next mission ready — ideally something with visible, near-term impact to rebuild momentum. Run a brief retrospective: what signals should have triggered the stop decision earlier? Use the answer to build kill criteria into future projects.",
    "commonMistakes": "Continuing because of sunk cost rather than future value. Framing cancellation as failure rather than strategic reallocation. Announcing the kill without preparing stakeholders privately. Not acknowledging the team's effort, leaving them feeling their work was meaningless. Failing to redirect the team to meaningful work immediately after.",
    "whatGoodLooksLike": "Within 2 weeks: project stops with clear communication. Within 1 month: team redirected to high-impact work; morale recovers. Within 1 quarter: freed capacity delivers visible value. Ongoing: future projects include explicit go/no-go checkpoints; team celebrates what was learned even when the project didn't succeed. Measured by: cost of delay (metric 8.3), engineering investment mix (metric 8.4).",
    "mappingNotes": "Based on sunk cost decision frameworks and LeadDev 'Leading your engineering team through an unexpected product pivot.'",
    "suggestedMetricIds": [
      "8.3",
      "8.4"
    ]
  },
  {
    "id": "P-C11-6",
    "slug": "your-top-candidate-just-declined-winning-in-competitive-market",
    "observableIds": [
      "C11-O6",
      "C11-O10",
      "C11-O5"
    ],
    "capabilityIds": [
      "C11"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Your Top Candidate Just Declined — Winning in a Competitive Talent Market",
    "context": "You've lost three of your last five top candidates to competing offers. Your interview process is solid, candidates reach the offer stage enthusiastic, but they take another offer at the last moment. Your employer brand isn't strong enough, your compensation is competitive but not leading, and your closing process is too slow. Leadership is frustrated by open reqs that stay unfilled for months.",
    "topicsActivated": [
      "Hiring (Competitive Offer Strategy)",
      "Employer Brand (Value Proposition)",
      "Process (Closing Velocity)"
    ],
    "decisionFramework": "1. Diagnose why candidates decline (Week 1): Reach out to recent decliners with a brief, non-defensive follow-up: 'We're working to improve our process — would you share what led to your decision?' Common reasons: compensation gap, slower process than competitors, stronger team/mission fit elsewhere, or remote work flexibility. Don't guess — ask. 2. Compress your timeline (Week 2): If your process takes 3+ weeks end-to-end, you're losing to companies that close in 10 days. Map your pipeline and identify bottlenecks: scheduling delays, too many interview rounds, slow hiring committee approvals. Set a target: 10 business days from first screen to offer. Speed is a competitive advantage. 3. Differentiate your value proposition (Ongoing): If you can't win on total compensation, win on something else. What can you offer that FAANG can't? Impact per person (smaller team, more ownership), learning opportunity (harder problems, more autonomy), mission alignment, work-life balance, or career progression speed. Articulate this clearly in every interaction — candidates who choose you for the right reasons retain better. 4. Improve the closing experience (Per candidate): Assign a 'closing partner' (hiring manager or team member) for final-stage candidates. This person maintains warm contact between offer and acceptance: answers questions, arranges team meet-and-greets, shares exciting team updates. The close isn't the offer letter — it's the entire period until day one. 5. Build pipeline depth (Ongoing): The best way to handle candidate declines is to never depend on a single candidate. Maintain a sourcing pipeline through referrals, technical content, open source contributions, and conference presence. When one candidate declines, you should have another strong option within a week, not start sourcing from scratch.",
    "commonMistakes": "Matching counter-offers reactively instead of building a differentiated value proposition. Blaming candidates for making the wrong choice instead of fixing your process. Extending the interview process to 'be more thorough' when speed is the problem. Competing only on compensation when impact, growth, and culture matter more to many engineers. Not asking declined candidates why they declined.",
    "whatGoodLooksLike": "Within 2 weeks: time-to-offer compressed to under 10 business days. Ongoing: offer acceptance rate above 80%; candidate experience feedback consistently positive even from decliners; pipeline depth ensures no single decline stalls hiring by more than a week. Key principle: differentiate on impact per engineer, career velocity, and team quality rather than compensation alone — teams that move fast, articulate unique value, and treat hiring as a product win talent. Measured by: offer acceptance rate, time-to-offer, 1-year retention of hires.",
    "mappingNotes": "Based on LeadDev 'The great engineer hiring paradox', 'Rethinking your engineer hiring strategy 2024', and '6 engineering hiring trends 2025.'",
    "suggestedMetricIds": [
      "5.1",
      "7.4",
      "7.3"
    ]
  },
  {
    "id": "P-C13-6",
    "slug": "your-vp-wants-engineering-to-own-security-but-nobody-has-expertise",
    "observableIds": [
      "C13-O5",
      "C13-O1",
      "C13-O7"
    ],
    "capabilityIds": [
      "C13",
      "C6"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Your VP Wants Engineering to 'Own Security' But Nobody Has Expertise",
    "context": "Your VP has decided that security should shift left into engineering. There's no dedicated security team, no AppSec engineer, and nobody on your team has deep security expertise. A recent incident — a feature shipped with a known SQL injection vulnerability because 'security didn't review it in time' — has made this an urgent priority. You're expected to embed security into the development lifecycle, but your engineers see security as 'not my job' and compliance requirements are growing.",
    "topicsActivated": [
      "Security (Building Capability From Scratch)",
      "People (Security Champion Program)",
      "Process (Automated Security Integration)"
    ],
    "decisionFramework": "1. Start with automation, not training (Month 1): Don't begin by telling engineers to 'think about security more.' Begin by adding automated security scanning to your CI/CD pipeline — dependency scanning, SAST, secret detection. These tools catch the easy wins (known vulnerabilities, hardcoded secrets, common patterns) without requiring engineers to become security experts. The automation creates a baseline without adding cognitive load. 2. Designate security champions (Month 1-2): Identify 1-2 engineers per team who are curious about security. This isn't a full-time role — it's 10-15% of their time. Their job: review security scanner output, triage findings, learn enough to advise the team, and be the bridge to external security resources. Invest in their development — security training, conference attendance, cross-training with any security staff you do have. 3. Build threat modeling into design (Month 2-3): For any feature touching sensitive data, authentication, or external integrations, add a lightweight threat modeling step to the design phase. It doesn't have to be formal STRIDE analysis — start with three questions: 'What could go wrong? What's the worst case? How do we detect it?' This embeds security thinking without requiring deep expertise. 4. Establish severity-based response SLAs (Month 3): Define how quickly different severity vulnerabilities must be addressed: critical in 48 hours, high in 7 days, medium in 30 days. This gives the team clear expectations without asking them to prioritize security against every other demand subjectively. 5. Build the case for dedicated investment (Month 4+): After 3 months of data from automated scanning, you'll have visibility into your security posture that you didn't have before. Use this data to make the case for what you need next — whether that's a dedicated AppSec hire, a security contractor for audit prep, or a deeper investment in training.",
    "commonMistakes": "Starting with mandatory security training before engineers understand why it matters. Expecting engineers to become security experts overnight. Trying to implement everything at once (team will rebel). Making security a gate that blocks deploys without clear ownership of remediation. Not tuning scanning tools (alert fatigue kills adoption). Treating security as punishment rather than practice.",
    "whatGoodLooksLike": "Within 3 months: automated scanning catches vulnerabilities before production; security champions triaging findings; threat modeling happens for high-risk features. Within 6 months: vulnerability SLA compliance >90%; vulnerability backlog trending down; security catches shifting left; team has data to justify next security investment. Team sees security as 'how we work', not 'extra overhead'. Key principle: embed trained security champions within product teams rather than centralizing expertise; assign security requirements proportional to feature risk tier; integrate security into every layer of the pipeline as a structural property, not an add-on. Measured by: vulnerability remediation time (metric 9.2), vulnerability SLA compliance, security review coverage.",
    "mappingNotes": "Based on LeadDev 'Shifting left on security: Five steps to transformation' and 'Born-left security: The new approach taking over shift-left.'",
    "suggestedMetricIds": [
      "9.2",
      "3.5",
      "9.1"
    ]
  },
  {
    "id": "P-C12-7",
    "slug": "driving-culture-change-against-organizational-resistance",
    "observableIds": [
      "C12-O1",
      "C12-O2",
      "C12-O8"
    ],
    "capabilityIds": [
      "C12"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Driving Culture Change Against Organizational Resistance",
    "context": "You've been hired or promoted to improve engineering culture, but the broader organization actively resists change. Legacy norms, hostile leadership, or institutional inertia work against your efforts. Your skip-level doesn't see culture as a priority, peers are indifferent or defensive, and your team is cynical from failed past initiatives. You believe the culture needs to change, but you lack the organizational power to mandate it.",
    "topicsActivated": [
      "Culture (Grassroots Change Strategy)",
      "Stakeholder (Coalition Building)",
      "People (Team as Microculture)"
    ],
    "decisionFramework": "1. Assess the landscape honestly (Week 1-2): Before trying to change anything, understand the system you're in. Map the power dynamics: who benefits from the current culture, who suffers, who has influence. Identify the 2-3 most damaging cultural norms and the 2-3 people who might be natural allies. Be honest about your scope of influence — you can likely change your team's microculture, you may be able to influence your org, and you probably cannot change the company alone. 2. Build your team as a proof-of-concept (Month 1-2): Create the culture you want within your team first. Establish explicit norms (team charter), model the behavior you want to see, and protect your team from the worst of the broader culture. When your team starts outperforming, you have evidence, not just arguments. This is 'seeding' — growing something healthy in a controlled environment. 3. Build coalitions, not crusades (Month 2-4): Find 2-3 peer managers who share your concerns. Start informal alliances — shared rituals, cross-team norms, joint retros. Coalition-building is slower than top-down mandates but more durable. A single manager asking for change is a complaint; five managers asking together is a movement. 4. Choose your battles deliberately (Ongoing): You cannot fix everything. Pick the 1-2 cultural changes that would have the most impact and are most achievable given your influence. Focus your energy there. Let smaller issues go — fighting every battle guarantees you win none. The battles worth fighting are the ones that affect psychological safety and retention. 5. Know your limits and your exit criteria (Ongoing): Not every culture can be saved from inside. Set a personal timeline: if after 6-12 months of genuine effort, the organizational culture hasn't improved or has gotten worse, consider whether staying serves your team or just enables the dysfunction. The hardest leadership decision is sometimes 'I cannot fix this from here.' That's not failure — it's clarity.",
    "commonMistakes": "Going it alone without building coalitions — the 'lone crusader' burns out. Trying to change everything at once instead of focusing on 1-2 high-impact norms. Expecting organizational culture to change on your timeline rather than building incrementally. Confusing protecting your team with isolating them — your team needs to function within the broader org, not as an island. Staying too long in a genuinely toxic environment out of misplaced loyalty or sunk cost.",
    "whatGoodLooksLike": "Within 6 months: your team has a visibly healthier culture than the surrounding org; peer managers start asking how you did it and adopting similar practices; at least one cultural norm you championed spreads beyond your team. Ongoing: you have honest clarity about what you can and cannot change; if you leave, improvements persist because they're embedded in team practices, not dependent on your presence. Key principle: successful culture change documents are concrete, behavioral, and publicly committed to — leaders held to the same standards; cultural transformation takes 3+ years of consistent reinforcement before metrics show sustained change. Measured by: team health survey scores, peer adoption of practices, attrition rate (metric 7.1).",
    "mappingNotes": "Based on LeadDev 'Culture change in a hostile environment' (Jason Wong, ActBlue). Fills a genuine gap — existing C12 playbooks assume cooperative organizational context.",
    "suggestedMetricIds": [
      "5.1",
      "7.8",
      "7.9"
    ]
  },
  {
    "id": "P-C1-5",
    "slug": "restructuring-teams-around-a-platform-strategy",
    "observableIds": [
      "C1-O11",
      "C1-O5",
      "C1-O2"
    ],
    "capabilityIds": [
      "C1",
      "C3"
    ],
    "relevantLevels": [
      4,
      5
    ],
    "title": "Restructuring Teams Around a Platform Strategy",
    "context": "Multiple product teams are independently building similar infrastructure — authentication wrappers, data pipelines, deployment tooling, notification services. Each team's version is slightly different, creating maintenance burden and inconsistent user experience. You've identified the opportunity to extract a platform team, but you need to restructure without disrupting current delivery.",
    "topicsActivated": [
      "Org Design (Platform Team Extraction)",
      "Architecture (Shared Infrastructure)",
      "Stakeholder Mgmt (Internal Customer Relationships)"
    ],
    "decisionFramework": "1. Validate the platform opportunity (Week 1-2): Count how many teams are building the same capability independently. A platform is justified when 3+ teams duplicate the same infrastructure. Map the current state: who owns what, how different are the implementations, what's the maintenance cost of the duplication. 2. Define the platform contract (Week 2-3): Before moving people, define what the platform will provide and what it won't. The biggest platform team failure is scope creep — trying to be everything to everyone. Start with the 2-3 highest-leverage shared capabilities. Define SLAs: the platform team serves product teams as internal customers with explicit response times and reliability commitments. 3. Staff the team carefully (Week 3-4): Seed the platform team with engineers who built the original implementations across different product teams — they understand both the domain and the consumer needs. Don't staff it with engineers nobody else wanted. Platform teams need your strongest infrastructure engineers because they're building multipliers. 4. Establish the operating model (Month 2): The platform team is a product team whose customers are internal engineers. They need a product manager (or PM-minded engineer), a roadmap driven by internal customer feedback, and clear prioritization criteria. Avoid the 'ticket queue' anti-pattern where the platform team just reacts to requests. 5. Migration strategy (Month 2-3): Don't force immediate migration. Let early adopter teams migrate first, iterate on the platform based on their feedback, then provide migration support for remaining teams. Set a deprecation date for the old implementations only after the platform is proven. 6. Measure platform health (Ongoing): Track adoption rate, developer satisfaction with platform APIs, time-to-integrate for new consumers, and incident rate of platform vs. bespoke solutions.",
    "commonMistakes": "Extracting a platform team before the shared need is proven (premature abstraction). Staffing the platform team with the weakest engineers. No product management — platform becomes a reactive ticket queue. Forcing migration before the platform is ready, creating resentment. Platform team builds what they think is elegant rather than what product teams need. No SLAs — product teams can't depend on the platform reliably.",
    "whatGoodLooksLike": "Within 3 months: 2-3 product teams have migrated to the platform with measurable reduction in their infrastructure maintenance burden; platform team has a roadmap driven by internal customer feedback. Within 6 months: remaining teams migrated, duplicated infrastructure decommissioned; developer satisfaction with shared infrastructure improved. New product teams can start building features on day one instead of rebuilding common infrastructure. Measured by: platform adoption rate (>80% target), time-to-integrate for new consumers, developer satisfaction survey, infrastructure cost reduction.",
    "mappingNotes": "Platform team extraction and org restructuring",
    "suggestedMetricIds": [
      "1.2",
      "2.5",
      "3.2"
    ]
  },
  {
    "id": "P-C2-5",
    "slug": "building-and-defending-a-strategic-not-doing-list",
    "observableIds": [
      "C2-O5",
      "C2-O2",
      "C2-O1"
    ],
    "capabilityIds": [
      "C2",
      "C7"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Building and Defending a Strategic 'Not Doing' List",
    "context": "Your team is pulled in too many directions. Every stakeholder has a 'critical' request. Your backlog has 6 months of work but leadership expects everything in the next quarter. You've tried prioritization frameworks but they all result in 'everything is P1.' You need a way to make trade-offs explicit and defensible.",
    "topicsActivated": [
      "Strategy (Explicit Trade-offs)",
      "Communication (Stakeholder Alignment)",
      "Planning (Capacity-Based Prioritization)"
    ],
    "decisionFramework": "1. Establish capacity reality (Day 1-2): Calculate your team's actual delivery capacity based on the last 3 months of throughput — not idealized estimates. Show the gap: 'We have capacity for X weeks of work this quarter. The current ask is 3X.' This forces a prioritization conversation. 2. Build the 'Not Doing' list (Week 1): For every initiative that won't fit in capacity, create an explicit entry: what it is, who requested it, why it's not making the cut this quarter, and what would need to change for it to be prioritized. This is not a rejection — it's a transparent trade-off. 3. Rank by strategic value, not urgency (Week 1): Use a simple 2x2: strategic alignment (does this advance company goals?) vs. engineering leverage (does this make future work faster?). High/High items are obvious. Low/Low items go on the Not Doing list. The hard decisions are High/Low — these are where you earn your salary as a leader. 4. Present the trade-offs, not the decisions (Week 2): Go to leadership with: 'Given our capacity, here's what we can do and what we can't. Here are the trade-offs. Which set of trade-offs do you prefer?' This shifts the conversation from 'why isn't engineering doing more?' to 'which business outcomes matter most?' 5. Defend the list quarterly (Ongoing): The Not Doing list is a living document. Review it every quarter. Some items will be promoted as strategy shifts. Others will be permanently archived. The key discipline is that adding something to the 'Doing' list requires removing something of equal size.",
    "commonMistakes": "Keeping the Not Doing list private — its power comes from transparency. Allowing leadership to add work without removing work ('just fit it in'). Framing as 'engineering can't do this' instead of 'here's the trade-off.' Not updating the list — it becomes stale and loses credibility. Treating it as permanent rejection rather than 'not now, and here's why.'",
    "whatGoodLooksLike": "Within 2 weeks: stakeholders reference the Not Doing list in their own planning. Within 1 month: leadership uses it to make resource allocation decisions; the team focuses on fewer things and delivers them well. Ongoing: quarterly planning takes days, not weeks, because priorities are already clear; new requests evaluated against the list ('What would this replace?'). Measured by: cost of delay (metric 8.3), engineering investment mix (metric 8.4).",
    "mappingNotes": "Explicit prioritization trade-offs and strategic communication",
    "suggestedMetricIds": [
      "8.3",
      "8.4"
    ]
  },
  {
    "id": "P-C3-5",
    "slug": "running-a-build-vs-buy-decision-for-a-critical-system",
    "observableIds": [
      "C3-O3",
      "C3-O9",
      "C3-O1"
    ],
    "capabilityIds": [
      "C3",
      "C10"
    ],
    "relevantLevels": [
      3,
      4
    ],
    "title": "Running a Build-vs-Buy Decision for a Critical System",
    "context": "Your team needs a capability — observability platform, feature flag system, CI/CD pipeline, identity provider, or similar infrastructure. You could build it in-house or buy/adopt an external solution. The build option gives you full control but costs engineering time. The buy option is faster but creates vendor dependency. Both camps have passionate advocates on the team. You need a structured decision process.",
    "topicsActivated": [
      "Architecture (Build vs. Buy Analysis)",
      "Cost Management (TCO Evaluation)",
      "Strategy (Vendor Dependency Trade-offs)"
    ],
    "decisionFramework": "1. Define the decision criteria (Week 1): Before evaluating options, agree on what matters. Common criteria: total cost of ownership over 3 years, time to production readiness, customization requirements, vendor lock-in risk, team expertise required, integration complexity, compliance requirements. Weight these criteria based on your context — a startup values speed-to-production differently than a regulated enterprise. 2. Calculate honest TCO for both options (Week 1-2): Build TCO includes: initial development time, ongoing maintenance (typically 20-30% of build cost annually), opportunity cost of engineers not working on product, hiring/training for specialized skills, infrastructure costs. Buy TCO includes: license fees (watch for per-seat scaling), integration engineering, migration costs if you switch later, vendor management overhead, customization limits that require workarounds. The most common mistake is underestimating build maintenance and underestimating buy integration costs. 3. Evaluate the 'core vs. context' question (Week 1): If this capability is core to your competitive advantage, lean toward building. If it's context (necessary but not differentiating), lean toward buying. Very few companies gain competitive advantage from their CI/CD pipeline or observability platform — these are almost always better bought. 4. Run a time-boxed proof of concept (Week 2-3): For the top 2 options (one build, one buy), run a 1-week spike with real engineers on real requirements. This surfaces integration issues, developer experience, and feasibility that no spreadsheet analysis can capture. 5. Make the decision and document it (Week 3): Write an ADR (Architecture Decision Record) capturing: the decision, alternatives considered, criteria and weights, trade-offs accepted, and conditions that would trigger revisiting. Publish it widely — this prevents relitigating the decision later.",
    "commonMistakes": "Defaulting to build because 'we're engineers, we can build it better.' Underestimating ongoing maintenance cost of custom solutions (the 20-30% annual tax). Not accounting for opportunity cost — engineers building infrastructure aren't building product. Choosing a vendor based on a demo without running a real proof of concept. Making the decision in a meeting without documented analysis. Treating the decision as permanent — not defining conditions for revisiting.",
    "whatGoodLooksLike": "Within 3-4 weeks: decision made with documented rationale. ADR published with TCO analysis covering 3-year horizon including maintenance and opportunity cost. Team supports decision because they participated in evaluation via proof-of-concept. Within expected timeline: system in production. At 1-year review: decision still holds, or was explicitly revisited when conditions changed. Measured by: time-to-production vs. projection, ongoing maintenance cost vs. TCO estimate.",
    "mappingNotes": "Structured build-vs-buy decision process with TCO analysis",
    "suggestedMetricIds": [
      "3.2",
      "2.3"
    ]
  },
  {
    "id": "P-C4-5",
    "slug": "establishing-a-remote-first-operating-rhythm",
    "observableIds": [
      "C4-O11",
      "C4-O1",
      "C4-O7"
    ],
    "capabilityIds": [
      "C4"
    ],
    "relevantLevels": [
      2,
      3,
      4
    ],
    "title": "Establishing a Remote-First Operating Rhythm",
    "context": "Your team is distributed across time zones (or transitioning from co-located to hybrid/remote). Meetings that worked in-person don't translate — some people are in a conference room while others are on small laptop screens. Information flows through hallway conversations that remote team members miss. Async communication is chaotic, and remote engineers feel like second-class citizens. You need to redesign your operating rhythm for distributed work.",
    "topicsActivated": [
      "Operational Rhythm (Distributed Cadences)",
      "Communication (Async-First Practices)",
      "Team Health (Inclusion & Equal Access)"
    ],
    "decisionFramework": "1. Audit the current information flow (Week 1): Track where decisions are actually made for one week. If more than 30% of decisions happen in hallway conversations, ad-hoc office discussions, or meetings without remote-friendly facilitation, your operating rhythm is office-first regardless of your stated policy. 2. Establish async-first defaults (Week 1-2): Default to async for status updates, decisions that don't need real-time debate, and information sharing. Use written RFCs for technical decisions, async standups (Slack/written updates), and recorded demos. Reserve synchronous time for things that genuinely need it: brainstorming, conflict resolution, relationship building. 3. Redesign synchronous meetings (Week 2): If even one person is remote, the meeting is remote — everyone joins from their own device, no conference rooms with a single camera. Keep meetings within overlapping hours. Record everything for team members in non-overlapping time zones. Rotate meeting times if the team spans more than 4 hours of time zones — don't always disadvantage the same people. 4. Create structured social connection (Week 2-3): Remote teams lose the organic social interaction of offices. Intentionally create it: virtual coffee pairings, team social hours, in-person offsites quarterly. The key is making it opt-in and varied — not everyone bonds the same way. 5. Protect focus time explicitly (Ongoing): Remote work's biggest advantage is deep focus time. Protect it with meeting-free days or blocks. Set team norms for response times — not everything needs an immediate reply. Async communication works only if people trust that messages will be read within a reasonable window (e.g., 4 hours during work hours).",
    "commonMistakes": "Treating hybrid as 'remote people dial into office meetings' — this creates two tiers. Requiring real-time presence for everything, negating remote work's flexibility advantage. No explicit async norms — people feel pressure to respond immediately to every message. Forgetting social connection — remote teams that never interact informally lose trust and cohesion. Over-indexing on tools over practices — Slack and Zoom don't fix a broken communication culture.",
    "whatGoodLooksLike": "Within 1 month: remote and office-based team members report equal access to information and decisions. Within 2 months: meeting load decreases as async practices mature. Ongoing: team members in different time zones contribute fully; documentation quality improves as a side benefit. Outcome: new team members onboard from anywhere with equal effectiveness. Measured by: information access parity surveys, meeting count trend, onboarding time by location.",
    "mappingNotes": "Remote-first operating practices for distributed teams",
    "suggestedMetricIds": [
      "1.2",
      "5.1"
    ]
  },
  {
    "id": "P-C9-6",
    "slug": "designing-okrs-that-actually-drive-engineering-behavior",
    "observableIds": [
      "C9-O4",
      "C9-O5",
      "C9-O3"
    ],
    "capabilityIds": [
      "C9",
      "C2"
    ],
    "relevantLevels": [
      3,
      4,
      5
    ],
    "title": "Designing OKRs That Actually Drive Engineering Behavior",
    "context": "Your organization uses OKRs but they're not working. Teams set objectives during quarterly planning, put them in a spreadsheet, and never look at them again until the next planning cycle. Key results are either unmeasurable ('improve developer experience'), sandbagged ('deploy at least once per sprint' when you already deploy daily), or activity-based ('ship features X, Y, Z') rather than outcome-based. Leadership reviews OKRs quarterly but the process feels like theater.",
    "topicsActivated": [
      "Metrics (Outcome-Oriented Goal Setting)",
      "Strategy (Engineering-to-Business Alignment)",
      "Communication (Cross-Functional Planning)"
    ],
    "decisionFramework": "1. Diagnose the current dysfunction (Week 1): Classify your existing OKRs into four categories: unmeasurable (no number), sandbagged (already achieved), activity-based (counts output not outcomes), and genuine (measures outcomes with stretch targets). If more than 50% fall into the first three categories, you have a systemic problem, not a writing problem. 2. Teach the outcome mindset (Week 1-2): The key shift is from 'what will we ship?' to 'what will change because we ship it?' Instead of 'Launch new onboarding flow' (activity), write 'Reduce time-to-first-value for new users from 14 days to 3 days' (outcome). The engineering team should own the outcome, not just the output. Run a workshop where each team rewrites their OKRs using the outcome frame. 3. Set the right difficulty level (Week 2): A common benchmark: 70% achievement on a well-set OKR is good. If teams routinely hit 100%, targets are too easy. If they routinely hit 30%, targets are demoralizing. Set one 'committed' key result (must hit, 100% expected) and one 'aspirational' key result (stretch, 60-70% expected) per objective. 4. Create the review cadence (Week 2-3): OKRs that are only reviewed quarterly are OKRs in name only. Establish a mid-quarter check-in where teams assess trajectory and adjust approach (not targets). The question isn't 'are we on track?' but 'what's blocking progress and what should we change?' 5. Connect to resource allocation (Ongoing): OKRs should drive sprint planning. If a team's sprint work doesn't connect to any OKR, either the OKRs are wrong or the sprint is wrong. Use OKR progress to inform next quarter's resource allocation — teams delivering high-impact outcomes should get more investment.",
    "commonMistakes": "Writing OKRs as a compliance exercise rather than a strategic tool. Setting key results that are binary (shipped/didn't ship) rather than continuous. Not reviewing until end of quarter — too late to course-correct. Disconnecting OKRs from sprint work — they live in a spreadsheet nobody opens. Using OKRs for individual performance evaluation (kills honest target-setting). Setting too many OKRs — more than 3 objectives per team dilutes focus.",
    "whatGoodLooksLike": "Within 2 weeks: draft OKRs with measurable key results. Within 1 month: OKRs finalized and influencing sprint planning. At quarter end: OKRs scored honestly with 0.6-0.8 average. Outcome: OKRs drive daily prioritization; team can explain how their current work connects to an objective. Measured by: OKR score, team ability to state connection between daily work and objectives.",
    "mappingNotes": "Outcome-oriented OKR design and operational integration",
    "suggestedMetricIds": [
      "8.3",
      "8.4",
      "1.1"
    ]
  }
]
