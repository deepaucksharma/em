[
  {
    "id": "C1-O1",
    "capabilityId": "C1",
    "shortText": "Designs team topology aligned to value streams and cognitive load",
    "slug": "designs-team-topology-aligned-to-value-streams-and-cognitive-load",
    "fullExample": "Reorganizes from layer-based to stream-aligned teams, mapping ownership to customer value flows with clear boundaries.",
    "evidenceTypes": [
      "decision_doc",
      "org_chart"
    ],
    "defaultWeight": 0.106,
    "requiredFrequency": "episodic",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Directors design topology; EMs advocate for changes within their team",
    "why": "Teams organized by tech layer create handoff bottlenecks and can't own outcomes end-to-end. Small, stream-aligned teams with clear ownership boundaries ship faster and own outcomes without coordination tax because each team controls its full value-delivery pipeline.",
    "how": "Map value streams using a dependency matrix updated quarterly; assign ownership boundaries documented in a team topology registry; minimize cross-team handoffs to less than 5% of sprint work. Assess cognitive load per team using a structured survey (domains owned, number of services, on-call scope) reviewed semi-annually. Validate each team can deploy independently without waiting on other teams. Validate with metric 8.4 (Engineering Investment Mix), 8.2 (Revenue / Business Impact Attribution), 2.6 (Blocked Work Rate), and 7.6 (Span of Control).",
    "expectedResult": "Teams ship independently with <5% of work requiring cross-team coordination; clear accountability; no 'I thought the other team owned that'; each team's blast radius is contained.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-036",
        "observableId": "C1-O1",
        "capabilityId": "C1",
        "signalText": "The foundational org design pattern. Directors must articulate team topology rationale: 'Reorganized from layer-based to stream-aligned teams, reducing cross-team handoffs by X%'",
        "signalType": "metric",
        "sourceSubTopic": "Stream-Aligned Teams"
      },
      {
        "id": "SIG-038",
        "observableId": "C1-O1",
        "capabilityId": "C1",
        "signalText": "Key signal in re-org proposals: 'Team owns too much — proposed split based on cognitive load analysis'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Team Cognitive Load"
      }
    ]
  },
  {
    "id": "C1-O2",
    "capabilityId": "C1",
    "shortText": "Applies inverse Conway maneuver to align org structure with target architecture",
    "slug": "applies-inverse-conway-maneuver-to-align-org-structure-with-target-architecture",
    "fullExample": "Restructures teams to match desired microservices architecture, enabling independent deployment per team.",
    "evidenceTypes": [
      "decision_doc",
      "design_doc"
    ],
    "defaultWeight": 0.076,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "Directors execute; EMs provide input on team-level implications",
    "why": "Architecture mirrors org structure whether planned or not — fighting Conway's Law always loses. Decomposing into services is an org design decision as much as a technical one — team boundaries drive API boundaries.",
    "how": "Map current architecture to team structure, find misalignments where service boundaries don't match team boundaries, restructure teams to match desired architecture — not the other way around. Start from the target architecture and design the org to produce it. Validate with metric 8.4 (Engineering Investment Mix).",
    "expectedResult": "Architecture and org structure reinforce each other; service boundaries align with team boundaries; deploy frequency increases because teams don't block each other.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-039",
        "observableId": "C1-O2",
        "capabilityId": "C1",
        "signalText": "Director-level insight: 'Applied inverse Conway — re-orged teams to align with target microservices architecture, enabling independent deployment'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Conway's Law (Intentional)"
      }
    ]
  },
  {
    "id": "C1-O3",
    "capabilityId": "C1",
    "shortText": "Executes re-orgs with structured communication and zero regrettable attrition",
    "slug": "executes-re-orgs-with-structured-communication-and-zero-regrettable-attrition",
    "fullExample": "Runs re-org of 30 engineers across 4 teams: managers told first, ICs within 24hrs, phased transition over 3 weeks, full productivity restored by week 4.",
    "evidenceTypes": [
      "comms_plan",
      "attrition_data"
    ],
    "defaultWeight": 0.101,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "Directors initiate and own; EMs execute team-level transitions",
    "why": "Poorly run re-orgs destroy trust and productivity for months — people leave because of how it was handled, not what was decided. Execution quality of the transition separates orgs that recover in weeks from those that bleed talent for quarters.",
    "how": "Communication plan (managers→ICs→broader org within 24hrs), phased transition with explicit ownership transfer docs, named DRI for each workstream migration, and a 30-day check-in cadence to catch integration issues early. Validate with metric 2.6 (Blocked Work Rate).",
    "expectedResult": "Teams productive within 2-4 weeks; zero regrettable attrition directly attributable to re-org; people feel respected; full velocity restored by week 4.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-041",
        "observableId": "C1-O3",
        "capabilityId": "C1",
        "signalText": "'Executed re-org of [X] engineers across [Y] teams with zero regrettable attrition and full productivity restored within 3 weeks'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Executing Re-orgs"
      },
      {
        "id": "SIG-173",
        "observableId": "C1-O3",
        "capabilityId": "C1",
        "signalText": "Observed during re-org: communicated rationale in team all-hands with empathy and clarity, answered hard questions directly, followed up individually with affected engineers within 24 hours.",
        "signalType": "manager_observation",
        "sourceSubTopic": "Re-org Execution Quality"
      }
    ]
  },
  {
    "id": "C1-O4",
    "capabilityId": "C1",
    "shortText": "Recognizes re-org signals and acts before problems become crises",
    "slug": "recognizes-re-org-signals-and-acts-before-problems-become-crises",
    "fullExample": "Identifies growing coordination overhead and unclear ownership across two teams, proposes and executes split before productivity degrades.",
    "evidenceTypes": [
      "meeting_note",
      "metric"
    ],
    "defaultWeight": 0.076,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "Directors initiate; EMs surface signals from their team",
    "why": "Teams accumulate scope creep and unclear ownership invisibly over time. When a team outgrows its cognitive load budget, that's the signal to split — not when delivery grinds to a halt.",
    "how": "Monitor coordination overhead, ownership confusion, team size >10, >3 active contexts per team using a quarterly team health scorecard. Track cross-team handoff frequency and meeting load as leading indicators — when coordination cost exceeds 20% of team capacity, the structure is wrong. Review signals monthly in a dedicated org-design checkpoint; trigger re-org evaluation when two or more thresholds are breached for two consecutive months. Validate with metric 2.6 (Blocked Work Rate) and 7.6 (Span of Control).",
    "expectedResult": "Proactive structural changes; re-orgs feel intentional, not reactive; team splits happen before productivity degrades, not after.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-040",
        "observableId": "C1-O4",
        "capabilityId": "C1",
        "signalText": "Directors initiate re-orgs; EMs execute them. Both need to read the signals early",
        "signalType": "calibration_language",
        "sourceSubTopic": "When to Re-org"
      }
    ]
  },
  {
    "id": "C1-O5",
    "capabilityId": "C1",
    "shortText": "Maintains single-threaded ownership registry with no ambiguity",
    "slug": "maintains-single-threaded-ownership-registry-with-no-ambiguity",
    "fullExample": "Builds and maintains service→team→on-call→contact registry; resolves disputes by escalation, not by sharing ownership.",
    "evidenceTypes": [
      "doc",
      "dashboard"
    ],
    "defaultWeight": 0.051,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs maintain for their team; Directors enforce across org",
    "why": "Shared ownership = no ownership; diffusion of responsibility means nothing gets done well. Every service, every metric, every customer experience needs exactly one owner with the authority and accountability to act.",
    "how": "Build a service-to-team-to-on-call ownership registry in a central tool (e.g., Backstage service catalog or wiki-based CMDB) and audit it monthly for completeness. Resolve disputes by assigning a single DRI (not by splitting ownership); transfer ownership explicitly through a written handoff protocol with acceptance criteria — never let ownership transfer by attrition or team departure. Target 100% of production services having a named owner; flag orphaned services within one sprint of team changes. Validate with metric 7.6 (Span of Control).",
    "expectedResult": "Every service and metric has exactly one accountable owner; zero 'I thought the other team owned that' incidents; on-call pages route to the right team within seconds.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-042",
        "observableId": "C1-O5",
        "capabilityId": "C1",
        "signalText": "Amazon principle applied broadly — calibration rewards clear ownership stories: 'Established single-threaded ownership for [X], resolving [chronic issue]'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Single-Threaded Ownership"
      }
    ]
  },
  {
    "id": "C1-O6",
    "capabilityId": "C1",
    "shortText": "Sets coherent org-level goals aligned to business strategy with visible cross-team dependencies",
    "slug": "sets-coherent-org-level-goals-aligned-to-business-strategy-with-visible-cross-team-dependencies",
    "fullExample": "Creates cascading OKRs from business strategy to org objectives to team goals; maps cross-team dependencies; explicitly connects engineering work to business outcomes and revenue impact.",
    "evidenceTypes": [
      "doc",
      "metric",
      "meeting_note"
    ],
    "defaultWeight": 0.101,
    "requiredFrequency": "quarterly",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Directors own org-level; EMs align team goals upward",
    "why": "Team-level goals don't automatically add up to org outcomes; engineering goals framed in technical terms are invisible to business leadership and fail to earn investment. Cascading objectives from company to team level makes misalignment structurally visible.",
    "how": "Start from business strategy, decompose to org goals, then team OKRs using a cascading structure where each level's key results become the next level's objectives. For each goal, articulate the business impact chain. Run cross-team alignment reviews quarterly. Track org-level outcomes, not just team deliverables. Validate with metric 10.1 (Cloud Cost per Transaction/User).",
    "expectedResult": "All teams rowing same direction; org-level goals met through coordinated execution; >80% of team OKRs explicitly link to org-level objectives; cross-team dependencies visible in planning artifacts.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-097",
        "observableId": "C1-O6",
        "capabilityId": "C1",
        "signalText": "Directors own the 'why' and 'what'; EMs own the 'how'. 'Set org OKRs, coordinated across [X] teams, achieved [Y]% of objectives'",
        "signalType": "metric",
        "sourceSubTopic": "Org-Level Goal Setting"
      },
      {
        "id": "SIG-104",
        "observableId": "C1-O6",
        "capabilityId": "C1",
        "signalText": "The difference between 'shipped features' and 'drove business outcomes'. Directors who can't articulate this connection lose influence",
        "signalType": "calibration_language",
        "sourceSubTopic": "Aligning Team Goals to Business Strategy"
      }
    ]
  },
  {
    "id": "C1-O7",
    "capabilityId": "C1",
    "shortText": "Monitors org health dashboard and intervenes on early signals",
    "slug": "monitors-org-health-dashboard-and-intervenes-on-early-signals",
    "fullExample": "Tracks attrition, engagement scores, skip-level themes, and PTO usage monthly; catches burnout trend in one team before anyone resigns.",
    "evidenceTypes": [
      "dashboard",
      "survey_data"
    ],
    "defaultWeight": 0.076,
    "requiredFrequency": "continuous",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Directors own org-level dashboard; EMs provide team signals",
    "why": "Org problems are invisible until they explode — attrition spike, burnout wave, disengagement crisis. Making team health a first-class metric reviewed alongside delivery metrics catches problems while they're still fixable.",
    "how": "Run squad-style health checks quarterly (autonomy, mission clarity, fun, learning, support). Combine with quantitative dashboard: attrition rate, engagement scores, skip-level themes, 1:1 signals, PTO usage. Review monthly and intervene when two or more indicators trend negative simultaneously. Validate with metric 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "Early intervention on emerging problems before they hit attrition; sustained org health scores above team-defined thresholds; leadership trusts you're on top of people issues.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-098",
        "observableId": "C1-O7",
        "capabilityId": "C1",
        "signalText": "Directors who lose people due to neglected org health lose credibility fast. 'Maintained org health score above [X] across [Y] teams for [Z] quarters'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Organizational Health Monitoring"
      }
    ]
  },
  {
    "id": "C1-O8",
    "capabilityId": "C1",
    "shortText": "Leads org through crisis while maintaining delivery on critical path",
    "slug": "leads-org-through-crisis-while-maintaining-delivery-on-critical-path",
    "fullExample": "During 20% RIF, owns communication, protects remaining team's morale, maintains delivery on top priorities, sees zero additional voluntary attrition for 6 months.",
    "evidenceTypes": [
      "comms_plan",
      "metric"
    ],
    "defaultWeight": 0.101,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "Directors lead org-level crises; EMs manage team-level impact",
    "why": "Layoffs and pivots shatter trust if handled badly; survivor guilt tanks productivity for months. Leaders who are transparent about the 'why', provide genuine support, and maintain delivery focus preserve team trust through crisis.",
    "how": "Own crisis communication directly using a structured comms plan (what happened, what's known, what's next, when the next update is). Be visible with daily or twice-daily standups during acute crises. Acknowledge difficulty honestly; protect morale by clarifying what will not change. Make quick decisions on what to pause — publish a revised priority list within 48 hours. Track voluntary attrition monthly for six months post-crisis; target zero regrettable departures. Validate with metric 7.1 (Regrettable Attrition Rate) and 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Org navigates crisis with minimal unnecessary damage; trust preserved; recovery is faster.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-055",
        "observableId": "C1-O8",
        "capabilityId": "C1",
        "signalText": "Crisis leadership defines Director credibility. 'Led team through [X]% RIF, maintained team delivery, zero additional voluntary attrition in following 6 months'",
        "signalType": "metric",
        "sourceSubTopic": "Leading Through Layoffs/RIF"
      },
      {
        "id": "SIG-099",
        "observableId": "C1-O8",
        "capabilityId": "C1",
        "signalText": "Crisis leadership defines Director reputation. 'Led org through [crisis], maintained delivery on critical path, zero regrettable attrition during transition'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Leading Through Organizational Crisis"
      }
    ]
  },
  {
    "id": "C1-O9",
    "capabilityId": "C1",
    "shortText": "Delegates execution to EMs while operating at Director altitude",
    "slug": "delegates-execution-to-ems-while-operating-at-director-altitude",
    "fullExample": "Defines explicit ownership boundaries: Director owns strategy, cross-team, upward comms; EMs own team execution, 1:1s, sprint management. EMs escalate to Director only when criteria met.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.076,
    "requiredFrequency": "continuous",
    "emRelevance": "Director-Only",
    "directorRelevance": "High",
    "levelNotes": "Director-only capability",
    "why": "Directors who stay in EM-mode become bottlenecks; every decision routes through them and EMs never develop judgment. Director-level leaders must explicitly operate at the 'next altitude' — setting context and coaching, never taking over execution.",
    "how": "Document an explicit delegation charter listing what Director vs EM owns (e.g., Director: cross-team strategy, upward comms, headcount allocation; EM: sprint execution, 1:1s, hiring pipeline). Define intervention criteria — specific thresholds (e.g., missed two consecutive sprint commitments, attrition spike) that trigger Director involvement. Trust EMs with execution; review via weekly outcome dashboards, not activity reports. Revisit delegation boundaries quarterly as EMs grow. Validate with metric 8.1 (OKR Achievement Rate) and 7.6 (Span of Control).",
    "expectedResult": "Director operates at right altitude; EMs develop independence; org scales beyond one person.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-095",
        "observableId": "C1-O9",
        "capabilityId": "C1",
        "signalText": "Most common Director failure: not letting go of EM-level work. 'Delegated [X] to EMs, focused on [org-level initiative], resulting in [Y]'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Delegation at Scale"
      }
    ]
  },
  {
    "id": "C1-O10",
    "capabilityId": "C1",
    "shortText": "Considers second-order effects and system-wide implications of decisions",
    "slug": "considers-second-order-effects-and-system-wide-implications-of-decisions",
    "fullExample": "Before approving a team's velocity metric target, identifies that it would incentivize gaming and instead proposes a balanced scorecard approach.",
    "evidenceTypes": [
      "decision_doc",
      "meeting_note"
    ],
    "defaultWeight": 0.076,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs apply to team; Directors apply across org",
    "why": "Optimizing one team or metric can degrade the whole system — local maxima aren't global maxima. Leaders must evaluate second-order effects across team boundaries before making changes that affect shared resources or APIs.",
    "how": "Map dependencies using a lightweight impact-analysis template (who is affected, what metrics change, what could go wrong) for every decision above a defined threshold (e.g., >1 eng-month or cross-team scope). Identify feedback loops and check for unintended incentives by explicitly listing second-order effects in decision documents. Consult the product triad and downstream EMs before committing; require at least one 'pre-mortem' review for org-level decisions. Validate with metric 8.3 (Cost of Delay) and 2.6 (Blocked Work Rate).",
    "expectedResult": "Decisions improve the system, not just local metrics; fewer 'fixed A but broke B' situations.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-137",
        "observableId": "C1-O10",
        "capabilityId": "C1",
        "signalText": "Director-level expectation: 'Considered org-wide implications, identified [unintended consequence], adjusted approach'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Systems Thinking"
      }
    ]
  },
  {
    "id": "C2-O1",
    "capabilityId": "C2",
    "shortText": "Runs structured planning process aligning eng investment to business priorities",
    "slug": "runs-structured-planning-process-aligning-eng-investment-to-business-priorities",
    "fullExample": "Leads H2 planning: gathers asks from PM/business/tech-debt/reliability, calculates real capacity, stack-ranks, gets stakeholder buy-in on final plan.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.246,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team-level; Directors own org-level planning",
    "why": "Without structured planning, eng investment drifts from business priorities each quarter. Planning is the mechanism that connects engineering capacity to business outcomes — teams that articulate customer value before writing code avoid building the wrong thing fast.",
    "how": "Intake asks from PM/business/tech-debt/reliability, calculate real capacity (minus on-call/interviews/PTO/20% buffer), stack-rank using business impact criteria, get stakeholder buy-in on the final plan and the explicit not-doing list. For major initiatives, write the expected customer outcome first to force clarity on business value. Validate with metric 8.1 (OKR Achievement Rate).",
    "expectedResult": "Clear plan stakeholders understand; team knows what they're building and why; >85% of committed roadmap delivered; capacity assumptions validated within 10% of actuals.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-100",
        "observableId": "C2-O1",
        "capabilityId": "C2",
        "signalText": "EMs own team-level planning; Directors own org-level. 'Led H2 planning across [X] teams, aligned [Y] stakeholders, delivered [Z]% of committed roadmap'",
        "signalType": "metric",
        "sourceSubTopic": "Quarterly/Half Planning"
      },
      {
        "id": "SIG-183",
        "observableId": "C2-O1",
        "capabilityId": "C2",
        "signalText": "Led quarterly planning that resulted in 30% reallocation of engineering investment toward highest-impact business priorities, documented with clear ROI analysis",
        "signalType": "promo_packet",
        "sourceSubTopic": "Quarterly/Half Planning"
      },
      {
        "id": "SIG-184",
        "observableId": "C2-O1",
        "capabilityId": "C2",
        "signalText": "Consistently produces planning artifacts that connect engineering work to business outcomes; stakeholders cite their planning process as a model for other teams",
        "signalType": "manager_observation",
        "sourceSubTopic": "Quarterly/Half Planning"
      }
    ]
  },
  {
    "id": "C2-O2",
    "capabilityId": "C2",
    "shortText": "Declines requests by making trade-offs visible, not by saying no",
    "slug": "declines-requests-by-making-trade-offs-visible-not-by-saying-no",
    "fullExample": "Responds to scope request with: 'Yes, if we deprioritize X — would you like to make that trade-off?' with data on impact of each option.",
    "evidenceTypes": [
      "meeting_note",
      "email"
    ],
    "defaultWeight": 0.245,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels use this; Directors at org-level with VPs",
    "why": "Always saying yes creates debt mountains; always saying no burns relationships. Giving stakeholders enough context to make the trade-off themselves preserves both focus and relationships — you don't decide for them, you make the cost of each option undeniable.",
    "how": "Never say just 'no'; frame as a resource constraint with quantified impact using a one-page trade-off brief (what you'd gain, what you'd drop, estimated cost of delay for each option). Present options with explicit trade-offs: scope, timeline, or quality. Let the requestor own the decision — document the trade-off and name the decision-maker explicitly. Use this framework for every mid-quarter scope request; archive trade-off briefs for quarterly retrospective review. Validate with metric 8.4 (Engineering Investment Mix) and 8.3 (Cost of Delay).",
    "expectedResult": "Focus on highest-impact work; relationships preserved; stakeholders own trade-offs jointly; deprioritized items don't resurface because the decision rationale is documented.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-134",
        "observableId": "C2-O2",
        "capabilityId": "C2",
        "signalText": "The mark of a strong EM/Director: 'What you chose NOT to do, and why' — calibration values this over endless yeses",
        "signalType": "calibration_language",
        "sourceSubTopic": "Saying No Effectively"
      },
      {
        "id": "SIG-185",
        "observableId": "C2-O2",
        "capabilityId": "C2",
        "signalText": "When they say no, it never feels like a rejection — they show you exactly what you'd be trading off and help you make the call yourself",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Saying No Effectively"
      },
      {
        "id": "SIG-186",
        "observableId": "C2-O2",
        "capabilityId": "C2",
        "signalText": "Deflected 5 out-of-scope requests in Q3 by presenting trade-off analyses that stakeholders described as 'the clearest prioritization communication they'd seen'",
        "signalType": "promo_packet",
        "sourceSubTopic": "Saying No Effectively"
      }
    ]
  },
  {
    "id": "C2-O3",
    "capabilityId": "C2",
    "shortText": "Makes progress under ambiguity by matching decision rigor to reversibility",
    "slug": "makes-progress-under-ambiguity-by-matching-decision-rigor-to-reversibility",
    "fullExample": "Classifies a technology choice as reversible, ships in 2 days instead of 2 weeks; classifies a data model change as irreversible, invests 3 weeks in analysis.",
    "evidenceTypes": [
      "decision_doc"
    ],
    "defaultWeight": 0.196,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels; Directors model this for EMs",
    "why": "Ambiguity paralyzes teams that lack frameworks for deciding with incomplete information. Reversible decisions should be made fast by individuals; irreversible decisions warrant full analysis. Matching rigor to reversibility prevents both analysis paralysis and reckless commitment.",
    "how": "Classify each decision: reversible (Type 2) or irreversible (Type 1)? High reversibility → decide with 70% confidence and ship; low reversibility → invest in analysis and get sign-off. Build in explicit reversibility where possible — feature flags, A/B tests, incremental rollouts. Validate with metric 8.3 (Cost of Delay).",
    "expectedResult": "Forward progress even in uncertainty; reversible decisions made in <48 hours; irreversible decisions have documented analysis; team course-corrects without blame when new info arrives.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-135",
        "observableId": "C2-O3",
        "capabilityId": "C2",
        "signalText": "Leadership signal: 'Made [decision] with 70% confidence, built in reversibility, course-corrected at [point] — net outcome positive'",
        "signalType": "metric",
        "sourceSubTopic": "Prioritization Under Ambiguity"
      },
      {
        "id": "SIG-187",
        "observableId": "C2-O3",
        "capabilityId": "C2",
        "signalText": "Distinguishes one-way door decisions from two-way door decisions and applies proportionate rigor; doesn't over-analyze reversible choices or under-analyze irreversible ones",
        "signalType": "manager_observation",
        "sourceSubTopic": "Prioritization Under Ambiguity"
      },
      {
        "id": "SIG-188",
        "observableId": "C2-O3",
        "capabilityId": "C2",
        "signalText": "In ambiguous situations where other teams stall, they find a way to make meaningful progress by scoping down the decision and iterating",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Prioritization Under Ambiguity"
      }
    ]
  },
  {
    "id": "C2-O4",
    "capabilityId": "C2",
    "shortText": "Challenges assumptions using first principles when situation demands fresh thinking",
    "slug": "challenges-assumptions-using-first-principles-when-situation-demands-fresh-thinking",
    "fullExample": "Questions why team is building a custom auth system; first-principles analysis reveals the original requirement no longer exists, saving 3 months of work.",
    "evidenceTypes": [
      "decision_doc",
      "meeting_note"
    ],
    "defaultWeight": 0.147,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "Directors apply more broadly; EMs apply within team scope",
    "why": "Cargo-culting and historical inertia lead to wasted investment on outdated assumptions. Writing the expected customer outcome before designing the solution is a forcing function for first-principles thinking — you can't articulate the customer value without questioning whether the underlying problem statement is still valid.",
    "how": "Identify the assumption, challenge whether it's still true, and decompose the problem from scratch using a structured first-principles template (original assumption, current evidence for/against, reframed problem statement). Write the problem statement before the solution — if you can't articulate why this matters to customers or the business today (not when it was first decided), the assumption is stale. Apply this framework for any initiative exceeding two eng-months before commitment; document the analysis in a decision record. Validate with metric 8.3 (Cost of Delay) and 8.2 (Revenue / Business Impact Attribution).",
    "expectedResult": "Novel solutions; decisions based on current reality, not historical inertia; team develops a habit of asking 'why do we believe this?' before major investments.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-136",
        "observableId": "C2-O4",
        "capabilityId": "C2",
        "signalText": "First principles thinking separates strategic leaders from operational ones. 'Challenged assumption that [X was necessary], saved [Y] by redesigning from first principles'",
        "signalType": "calibration_language",
        "sourceSubTopic": "First Principles Analysis"
      },
      {
        "id": "SIG-189",
        "observableId": "C2-O4",
        "capabilityId": "C2",
        "signalText": "Challenged the team's assumption that a full rewrite was needed by applying first-principles analysis, resulting in a targeted migration that saved 3 months of engineering time",
        "signalType": "promo_packet",
        "sourceSubTopic": "First Principles Analysis"
      },
      {
        "id": "SIG-190",
        "observableId": "C2-O4",
        "capabilityId": "C2",
        "signalText": "Regularly surfaces assumptions that others take as given; asks 'why do we believe this?' in a way that sharpens team thinking rather than undermining confidence",
        "signalType": "manager_observation",
        "sourceSubTopic": "First Principles Analysis"
      }
    ]
  },
  {
    "id": "C3-O1",
    "capabilityId": "C3",
    "shortText": "Authors and maintains multi-quarter technical vision with clear migration path",
    "slug": "authors-and-maintains-multi-quarter-technical-vision-with-clear-migration-path",
    "fullExample": "Co-authors tech strategy doc with Staff engineers: current state → target state → migration path → success metrics. Reviews quarterly against delivery reality.",
    "evidenceTypes": [
      "design_doc",
      "doc"
    ],
    "defaultWeight": 0.168,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs co-author for their area; Directors set vision across org",
    "why": "Teams lack coherent direction without a written technical strategy; engineers make conflicting architectural decisions. A strategy doc with alternatives considered and trade-offs explicit becomes the shared source of truth that aligns engineers across the area.",
    "how": "Collaborate with TL/Staff+ to author a living technical strategy doc; structure as current state → target state → migration path → success metrics. Review quarterly against delivery reality and update when assumptions change. Validate with metric 8.2 (Revenue / Business Impact Attribution).",
    "expectedResult": "Within 2 quarters: team ships coherently toward goals; new engineers read one doc and understand system direction; conflicting architectural decisions drop to near-zero.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-001",
        "observableId": "C3-O1",
        "capabilityId": "C3",
        "signalText": "Promo packet: 'Authored and drove technical vision for [area], adopted by [X] teams, enabling [measurable outcome]'",
        "signalType": "promo_packet",
        "sourceSubTopic": "Setting Technical Vision"
      }
    ]
  },
  {
    "id": "C3-O2",
    "capabilityId": "C3",
    "shortText": "Runs architecture reviews covering service boundaries, modularity, and design quality",
    "slug": "runs-architecture-reviews-covering-service-boundaries-modularity-and-design-quality",
    "fullExample": "Creates RFC template with required sections, sets review cadence, evaluates service boundary decisions against team topology and cognitive load, maintains searchable decision log. Reviews prevent both monolith coupling and over-decomposition.",
    "evidenceTypes": [
      "design_doc",
      "doc"
    ],
    "defaultWeight": 0.137,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs run for team; Directors institutionalize across org",
    "why": "Inconsistent design quality; service boundaries don't match team boundaries (violating Conway's Law); monolith coupling slows all teams while over-decomposition adds coordination overhead. Design reviews catch these misalignments before they become production problems.",
    "how": "Create RFC template with required sections: alternatives, rollback plan, SLO impact. Review evaluates: does this service boundary align with team ownership? Does modularity reduce or increase coordination? Maintain searchable decision log. Use RACI to clarify review roles. Validate with metric 8.1 (OKR Achievement Rate).",
    "expectedResult": "Fewer production surprises from design gaps; new engineers onboard 40% faster with searchable decision log; architecture decisions have documented rationale that survives team turnover.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-002",
        "observableId": "C3-O2",
        "capabilityId": "C3",
        "signalText": "Directors: 'Institutionalized design review process across org — reduced production incidents from design gaps by X%'",
        "signalType": "metric",
        "sourceSubTopic": "Architecture Review Process"
      },
      {
        "id": "SIG-011",
        "observableId": "C3-O2",
        "capabilityId": "C3",
        "signalText": "Conway's Law in practice: 'Realigned service boundaries to team topology, reduced cross-team deploy coordination by X%'",
        "signalType": "metric",
        "sourceSubTopic": "Modularity & Service Boundaries"
      },
      {
        "id": "SIG-171",
        "observableId": "C3-O2",
        "capabilityId": "C3",
        "signalText": "Observed in design reviews: asks probing questions about coupling and blast radius, pushes back on designs that create cross-team dependencies, references team topology constraints in architectural decisions.",
        "signalType": "manager_observation",
        "sourceSubTopic": "Design Review Quality"
      }
    ]
  },
  {
    "id": "C3-O3",
    "capabilityId": "C3",
    "shortText": "Makes build-vs-buy decisions using repeatable framework with TCO analysis",
    "slug": "makes-build-vs-buy-decisions-using-repeatable-framework-with-tco-analysis",
    "fullExample": "Evaluates vendor solution using 3-year TCO including maintenance, lock-in risk, and team capability fit; saves 6 eng-months by buying instead of building.",
    "evidenceTypes": [
      "decision_doc",
      "metric"
    ],
    "defaultWeight": 0.102,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team-level; Directors broker org-level platform decisions",
    "why": "Teams default to building when buying is faster, or buy solutions they outgrow in 6 months. A rigorous TCO analysis including maintenance burden, team opportunity cost, and lock-in risk over a 3-year horizon prevents both errors.",
    "how": "Evaluate using a standardized build-vs-buy scorecard covering: differentiation value (is this your core competency?), 3-year TCO including maintenance and migration cost, team capability fit, vendor lock-in exit cost, and time-to-value. Weight each factor and score numerically to reduce bias. Build only what differentiates; buy commodity infrastructure. Document the decision rationale in an ADR (Architecture Decision Record) for when the question inevitably resurfaces. Revisit build-vs-buy decisions annually or when vendor pricing changes materially. Validate with metric 10.1 (Cloud Cost per Transaction/User) and 8.4 (Engineering Investment Mix).",
    "expectedResult": "Within 1 quarter of decision: custom where it differentiates, commodity where it doesn't; regretted build-vs-buy decisions fewer than 1 per year.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-003",
        "observableId": "C3-O3",
        "capabilityId": "C3",
        "signalText": "Demonstrates strategic thinking: 'Saved [X] eng-months by buying [tool] instead of building, redirected capacity to [high-value project]'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Build vs Buy Decisions"
      }
    ]
  },
  {
    "id": "C3-O4",
    "capabilityId": "C3",
    "shortText": "Manages tech debt systematically with explicit capacity allocation",
    "slug": "manages-tech-debt-systematically-with-explicit-capacity-allocation",
    "fullExample": "Maintains debt inventory classified by severity, allocates 15-20% sprint capacity, tracks impact of paydown on delivery metrics.",
    "evidenceTypes": [
      "doc",
      "metric"
    ],
    "defaultWeight": 0.102,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own team debt; Directors ensure allocation discipline across teams",
    "why": "Unchecked debt slows velocity quarter over quarter; over-investment in debt starves features. Explicit tech debt budgets (typically 15-20% of sprint capacity) prevent both extremes by treating debt as an engineering investment with expected returns.",
    "how": "Maintain a tech debt inventory in the issue tracker (e.g., Jira label or dedicated board) with severity classification (critical/high/medium/low) based on impact to delivery velocity and incident risk. Allocate 15-20% of each sprint's capacity to debt paydown — protect this allocation from feature pressure by making it visible in sprint planning. Measure paydown impact quarterly by tracking change in deployment frequency, incident rate, and developer satisfaction before and after major debt items are resolved. Validate with metric 1.2 (Lead Time for Changes), 1.4 (Change Failure Rate), and 8.4 (Engineering Investment Mix).",
    "expectedResult": "Within 2 quarters: sustained velocity trend without compounding debt; deployment frequency and incident rate improve measurably after major debt items are resolved.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-004",
        "observableId": "C3-O4",
        "capabilityId": "C3",
        "signalText": "Promo packet: 'Reduced incident rate X% and deployment time Y% through strategic tech debt paydown of [specific systems]'",
        "signalType": "promo_packet",
        "sourceSubTopic": "Tech Debt Management"
      }
    ]
  },
  {
    "id": "C3-O5",
    "capabilityId": "C3",
    "shortText": "Defines SLOs with error budgets that drive reliability trade-offs",
    "slug": "defines-slos-with-error-budgets-that-drive-reliability-trade-offs",
    "fullExample": "Partners with SRE to set SLOs per service, defines error budgets, uses budget consumption as signal to slow feature work and invest in reliability.",
    "evidenceTypes": [
      "doc",
      "dashboard",
      "metric"
    ],
    "defaultWeight": 0.137,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own per-service SLOs; Directors own SLO culture across org",
    "why": "Without clear targets, every incident feels like a crisis and reliability investment is reactive. The error budget model transforms reliability from a subjective debate into an objective trade-off: when the error budget is consumed, feature work stops and reliability work starts.",
    "how": "Set SLOs for latency/availability/error rates aligned to user-visible impact. Define error budgets as the inverse of the SLO. Dashboard with burn rate alerts. When budget consumption exceeds the burn rate threshold, shift team focus to reliability. Validate with metric 8.5 (Customer Impact Metrics).",
    "expectedResult": "Objective reliability targets informing trade-offs; feature vs. reliability investment is data-driven, not political; team knows exactly when to invest in reliability based on error budget burn rate.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-005",
        "observableId": "C3-O5",
        "capabilityId": "C3",
        "signalText": "Directors own SLO culture across their org; EMs own per-service. 'Established SLO framework, reduced Sev1s by X% through error budget discipline'",
        "signalType": "metric",
        "sourceSubTopic": "SLA/SLO Definition & Monitoring"
      }
    ]
  },
  {
    "id": "C3-O6",
    "capabilityId": "C3",
    "shortText": "Plans and executes migrations incrementally without business disruption",
    "slug": "plans-and-executes-migrations-incrementally-without-business-disruption",
    "fullExample": "Uses strangler fig pattern to migrate system serving 10K QPS: parallel running with comparison testing, phased rollout with feature flags, zero customer-facing incidents.",
    "evidenceTypes": [
      "design_doc",
      "metric"
    ],
    "defaultWeight": 0.102,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs execute team-level; Directors own cross-org migration strategy",
    "why": "Big-bang migrations fail because risk compounds nonlinearly. The strangler fig pattern — replacing components incrementally while keeping the old system running — lets teams ship incremental value, catch issues early, and maintain business continuity throughout.",
    "how": "Use the strangler fig pattern to incrementally replace legacy components behind feature flags; run old and new systems in parallel with comparison testing (shadow traffic or dual-write with diff checks). Execute phased rollout: 1% canary, 10% early adopters, 50%, then 100%, with automated rollback triggers at each gate (e.g., error rate >0.5% or latency p99 increase >20%). Track migration progress weekly using a completion dashboard (percentage of traffic migrated, endpoints remaining, incidents caused). Target zero customer-facing incidents during migration. Validate with metric 1.4 (Change Failure Rate) and 1.3 (Mean Time to Restore).",
    "expectedResult": "Successful migrations with minimal disruption; team doesn't burn out. Zero production incidents directly caused by migration; customer-facing impact <1 hour total; team satisfaction survey shows no burnout signal during migration period.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-009",
        "observableId": "C3-O6",
        "capabilityId": "C3",
        "signalText": "High-visibility promo evidence: 'Led migration of [system] serving [X] QPS from [old] to [new] with zero customer-facing incidents'",
        "signalType": "promo_packet",
        "sourceSubTopic": "Migration Strategy"
      }
    ]
  },
  {
    "id": "C3-O7",
    "capabilityId": "C3",
    "shortText": "Defines observability standards and dependency maps with fallback strategies per service",
    "slug": "maintains-observability-standards-and-dependency-maps-enabling-fast-incident-diagnosis",
    "fullExample": "Defines structured logging format, required metrics per service, end-to-end trace propagation, and dependency maps with fallback strategies; reduces mean time to diagnose from 4 hours to 20 minutes.",
    "evidenceTypes": [
      "doc",
      "dashboard"
    ],
    "defaultWeight": 0.102,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own team instrumentation; Directors standardize across org",
    "why": "Without observability, debugging is guesswork and incidents take hours to diagnose. Observability is a prerequisite for ownership — if you can't measure it, you can't own it.",
    "how": "Define standards (structured logs, required metrics per service, end-to-end trace propagation). Instrument critical paths. Map upstream/downstream dependencies with health checks and fallback patterns. Set alerting thresholds. Every new service must meet observability requirements before production launch. Validate with metric 1.3 (Mean Time to Restore).",
    "expectedResult": "Fast incident diagnosis (<15min to root cause); data-driven performance optimization.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-007",
        "observableId": "C3-O7",
        "capabilityId": "C3",
        "signalText": "Cross-org impact awareness: 'Mapped [X] critical dependencies, implemented fallbacks, reduced cascading failures by Y%'",
        "signalType": "metric",
        "sourceSubTopic": "Dependency Management"
      },
      {
        "id": "SIG-008",
        "observableId": "C3-O7",
        "capabilityId": "C3",
        "signalText": "Table-stakes expectation — absence is a red flag. 'Instrumented [X] services, reduced mean time to diagnose from Y hours to Z minutes'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Observability Strategy"
      }
    ]
  },
  {
    "id": "C4-O1",
    "capabilityId": "C4",
    "shortText": "Establishes operating cadences with clear purposes and measurable outputs",
    "slug": "establishes-operating-cadences-with-clear-purposes-and-measurable-outputs",
    "fullExample": "Creates quarterly planning, weekly ops review, and async status cadence — each with defined roles, inputs, and outputs. Eliminates 3 meetings with unclear purpose.",
    "evidenceTypes": [
      "doc",
      "calendar"
    ],
    "defaultWeight": 0.119,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team cadence; Directors standardize across org",
    "why": "Without clear cadences, teams drift between meetings that produce nothing and chaos with no alignment. A consistent operating rhythm with defined inputs and outputs creates organizational muscle memory — the cadence itself is the management mechanism.",
    "how": "Define each meeting's purpose, required inputs, expected outputs, and cadence. Eliminate purposeless meetings ruthlessly. Structure: weekly metrics review (what happened), monthly operational review (what's broken), quarterly planning (where we're going). Validate with metric 2.5 (Sprint Commitment Accuracy) and 2.1 (Flow Time).",
    "expectedResult": "Within 1 month: predictable rhythm; every meeting produces decisions or action items; new hires understand the operating system within their first week; zero meetings without a stated purpose.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-060",
        "observableId": "C4-O1",
        "capabilityId": "C4",
        "signalText": "EMs define team's communication operating system. The test: can a new hire figure out how information flows within their first week?",
        "signalType": "calibration_language",
        "sourceSubTopic": "Team Communication Rhythms"
      }
    ]
  },
  {
    "id": "C4-O2",
    "capabilityId": "C4",
    "shortText": "Tracks delivery commitments with measurable KPIs and >85% hit rate",
    "slug": "tracks-delivery-commitments-with-measurable-kpis-and-85-hit-rate",
    "fullExample": "Creates commitment tracking with planned-vs-delivered ratio, velocity trend, and carryover rate; team consistently delivers 85%+ of committed work.",
    "evidenceTypes": [
      "metric",
      "dashboard"
    ],
    "defaultWeight": 0.152,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own team delivery; Directors benchmark across teams",
    "why": "Without measurement, delivery is unpredictable and stakeholder trust erodes. DORA research proved that deployment frequency, lead time, change failure rate, and mean time to recovery predict both engineering performance and business outcomes.",
    "how": "Track commitment accuracy, velocity trend, carryover rate (<10%); augment with DORA metrics (deployment frequency, lead time to change, change failure rate, MTTR). Use velocity for planning calibration, never for individual evaluation. Validate with metric 2.1 (Flow Time) and 2.3 (Cycle Time).",
    "expectedResult": "Within 2 quarters: >85% commitment hit rate sustained for 3+ consecutive cycles; sustainable pace; early warning when delivery drifts; DORA metrics trending in healthy direction.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-023",
        "observableId": "C4-O2",
        "capabilityId": "C4",
        "signalText": "Execution credibility: 'Team delivers 85%+ of committed work consistently' — this earns trust and autonomy from leadership",
        "signalType": "metric",
        "sourceSubTopic": "Sprint/Cycle Health"
      },
      {
        "id": "SIG-046",
        "observableId": "C4-O2",
        "capabilityId": "C4",
        "signalText": "'Team delivers 85%+ of committed work consistently' — execution credibility that earns autonomy",
        "signalType": "metric",
        "sourceSubTopic": "Capacity Planning & Allocation"
      }
    ]
  },
  {
    "id": "C4-O3",
    "capabilityId": "C4",
    "shortText": "Invests in developer experience and tooling with measurable productivity gains",
    "slug": "invests-in-developer-experience-and-tooling-with-measurable-productivity-gains",
    "fullExample": "Reduces CI pipeline from 45min to 8min, reclaiming 200 developer-hours/week; tracks build times, test duration, and local dev setup time.",
    "evidenceTypes": [
      "metric",
      "doc"
    ],
    "defaultWeight": 0.089,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs drive team-level DX; Directors justify org-level DX investment",
    "why": "Slow builds, flaky tests, and broken dev environments silently drain engineering capacity. Teams that ignore developer experience lose their best engineers to teams that don't. Developer wait time is a first-class cost metric.",
    "how": "Instrument CI/CD pipeline stages, set targets for each stage (build <5min, test <10min, deploy <15min), dedicate 10-15% capacity for DX improvements quarterly, measure developer satisfaction and time-to-first-commit for new hires. Validate with metric 1.2 (Lead Time for Changes) and 5.4 (Code Review Turnaround Time).",
    "expectedResult": "Developers spend >80% of time coding, not waiting on builds or fighting tooling; CI pipeline under 10 minutes; new hire productive within first week; measurable productivity gains tracked quarterly.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-017",
        "observableId": "C4-O3",
        "capabilityId": "C4",
        "signalText": "Concrete impact: 'Reduced CI pipeline from 45min to 8min, reclaiming [X] developer-hours/week across team'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Performance (System & Tooling)"
      },
      {
        "id": "SIG-172",
        "observableId": "C4-O3",
        "capabilityId": "C4",
        "signalText": "Observed in sprint reviews: team consistently delivers within 10% of committed scope, EM flags risks early in standup rather than at sprint end, retro action items are tracked to completion.",
        "signalType": "manager_observation",
        "sourceSubTopic": "Delivery Predictability"
      }
    ]
  },
  {
    "id": "C4-O4",
    "capabilityId": "C4",
    "shortText": "Eliminates toil through automation with tracked capacity reclamation",
    "slug": "eliminates-toil-through-automation-with-tracked-capacity-reclamation",
    "fullExample": "Runs toil audit, identifies top 3 manual processes, automates them, reclaims equivalent of 1 FTE in capacity per quarter.",
    "evidenceTypes": [
      "metric",
      "doc"
    ],
    "defaultWeight": 0.089,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs drive team-level; Directors track org-level toil reduction",
    "why": "Toil accumulates invisibly and drains capacity from value-creating work. When toil exceeds 50% of operational time, the team must stop feature work and automate. Unchecked toil compounds: 10 minutes/day becomes a full engineer-month per year.",
    "how": "Run a 1-week toil audit: every team member logs repetitive manual work with time spent, frequency, and people affected. Calculate annualized cost (hours × frequency × team members × loaded cost). Stack-rank by impact and automate the top 3 items. Track hours reclaimed monthly — target: recover at least 10% of team capacity in the first quarter. Validate with metric 5.2 (Developer Toil Hours).",
    "expectedResult": "Engineering time redirected from maintenance to value creation; morale improves.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-019",
        "observableId": "C4-O4",
        "capabilityId": "C4",
        "signalText": "Leverage-thinking: 'Automated [X], saving [Y] eng-hours/quarter, equivalent to [Z] FTE capacity reclaimed'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Toil Reduction & Automation"
      }
    ]
  },
  {
    "id": "C4-O5",
    "capabilityId": "C4",
    "shortText": "Implements risk-appropriate release process with automated rollback",
    "slug": "implements-risk-appropriate-release-process-with-automated-rollback",
    "fullExample": "Deploys with canary (1%→10%→50%→100%), automated rollback triggers on error rate spike, feature flags decoupling deploy from release.",
    "evidenceTypes": [
      "doc",
      "metric"
    ],
    "defaultWeight": 0.089,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own team release process; Directors standardize risk tiers across org",
    "why": "One-size-fits-all release process is either too slow for low-risk changes or too risky for high-risk ones. Risk-tiered deployments — configuration changes in minutes, feature changes through canary, infrastructure changes through staged rollout with automatic rollback — optimize both speed and safety.",
    "how": "Feature flags, canary deployments (1%→10%→50%→100%), staged rollouts, automated rollback triggering on error-rate spike within 5 minutes. Define risk tiers (low/medium/high) per change type and review tier assignments quarterly. Track rollback frequency and escaped defects per release weekly in a shared deployment dashboard. Validate with metric 1.1 (Deployment Frequency), 1.4 (Change Failure Rate), 4.3 (Rollback Rate), and 4.4 (Escaped Defect Rate).",
    "expectedResult": "Ship fast with confidence; rollback within 5 minutes when things go wrong; deploy frequency >1x/day for low-risk changes; zero-downtime deploys as standard.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-020",
        "observableId": "C4-O5",
        "capabilityId": "C4",
        "signalText": "EMs own release culture for their team; Directors standardize risk-appropriate release practices across org",
        "signalType": "calibration_language",
        "sourceSubTopic": "Release Management"
      },
      {
        "id": "SIG-022",
        "observableId": "C4-O5",
        "capabilityId": "C4",
        "signalText": "EMs should articulate their team's testing philosophy: 'We test [this way] because [this reason], resulting in [this outcome]'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Testing Strategy"
      }
    ]
  },
  {
    "id": "C4-O6",
    "capabilityId": "C4",
    "shortText": "Maintains code review culture with <24hr SLA and knowledge distribution",
    "slug": "maintains-code-review-culture-with-24hr-sla-and-knowledge-distribution",
    "fullExample": "Sets review SLA target, implements reviewer rotation for knowledge spread, tracks review bottleneck trends.",
    "evidenceTypes": [
      "metric",
      "doc"
    ],
    "defaultWeight": 0.059,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Low",
    "levelNotes": "EMs own team review culture directly",
    "why": "Slow reviews block flow and create single-reviewer bottlenecks that are a bus factor risk. Teams with >24-hour review turnaround see PR abandonment rates climb and developer satisfaction drop measurably.",
    "how": "Set a code-review SLA of <24 hours median turnaround, tracked weekly via PR analytics. Implement reviewer rotation to spread knowledge — no single reviewer handling >30% of PRs. Surface review bottleneck trends in sprint retrospectives and adjust rotation accordingly. Target: every PR reviewed by at least one non-author within 8 business hours; bus-factor >=2 for every critical service. Validate with metric 5.4 (Code Review Turnaround Time) and 2.3 (Cycle Time).",
    "expectedResult": "Quality gate + knowledge sharing; no single reviewer bottleneck. PR review turnaround <24 hours; at least 3 team members qualified to review any area of the codebase; review load distributed within 20% of equal split.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-021",
        "observableId": "C4-O6",
        "capabilityId": "C4",
        "signalText": "Healthy review culture is a team health indicator. 'Implemented review SLA, reduced review bottleneck from 3 days to <8 hours'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Code Review Culture"
      }
    ]
  },
  {
    "id": "C4-O7",
    "capabilityId": "C4",
    "shortText": "Protects team focus with interrupt budget and scope change process",
    "slug": "protects-team-focus-with-interrupt-budget-and-scope-change-process",
    "fullExample": "Designates rotating interrupt handler per sprint, defines escalation criteria for true urgency, requires explicit trade-off for any mid-sprint scope addition.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.089,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own directly; Directors set org-level expectations",
    "why": "Unmanaged interrupts fragment focus; scope creep erodes trust in commitments. Deep work requires 4+ hour uninterrupted blocks, and the operating model must structurally protect that constraint.",
    "how": "Designate rotating interrupt handler per sprint, define escalation criteria for true urgency (SEV1/SEV2 only), require explicit trade-off documentation for any mid-sprint scope addition. Protect 4+ hours of contiguous maker time daily — schedule all ceremonies and meetings in consolidated blocks. Validate with metric 2.2 (WIP (Work in Progress)).",
    "expectedResult": "Teams report 4+ hours of daily uninterrupted focus time; urgent issues still handled via interrupt rotation; mid-sprint scope changes require documented trade-off approval.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-048",
        "observableId": "C4-O7",
        "capabilityId": "C4",
        "signalText": "Shows team management maturity: 'Protected team focus by [implementing interrupt rotation], delivered [project] on schedule despite [X] interrupt requests'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Managing Interrupts & Scope Creep"
      }
    ]
  },
  {
    "id": "C4-O8",
    "capabilityId": "C4",
    "shortText": "Runs retrospectives with >80% action item completion rate",
    "slug": "runs-retrospectives-with-80-action-item-completion-rate",
    "fullExample": "Bi-weekly retros with strict format, action items with owners, tracked completion rate, specific process improvements implemented each quarter.",
    "evidenceTypes": [
      "meeting_note",
      "metric"
    ],
    "defaultWeight": 0.059,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own team retros; Directors coach on retro effectiveness",
    "why": "Retros without follow-through teach teams that feedback doesn't matter. Teams that close 80%+ of retro action items see 2x fewer repeated failures in subsequent quarters.",
    "how": "Bi-weekly retros, strict format (worked/didn't/actions), track completion, experiment with changes. Validate with metric 3.7 (Post-Incident Action Item Completion Rate).",
    "expectedResult": "Within 1 quarter: >80% retro action item completion rate; specific process improvements implemented each cycle; team reports feeling agency over their working environment.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-049",
        "observableId": "C4-O8",
        "capabilityId": "C4",
        "signalText": "Evidence of learning culture: 'Team retro action item completion rate >80%, resulting in [specific improvements]'",
        "signalType": "metric",
        "sourceSubTopic": "Retrospectives & Process Improvement"
      }
    ]
  },
  {
    "id": "C5-O1",
    "capabilityId": "C5",
    "shortText": "Maintains healthy Eng/PM/Design triad with joint roadmap ownership",
    "slug": "maintains-healthy-engpmdesign-triad-with-joint-roadmap-ownership",
    "fullExample": "Runs weekly triad syncs with shared context: PM shares business data, Design shares user research, Eng shares technical constraints. Disagreements resolved through data, not hierarchy.",
    "evidenceTypes": [
      "meeting_note",
      "doc"
    ],
    "defaultWeight": 0.089,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team-level triad; Directors broker org-level partnerships",
    "why": "Misaligned triad causes rework, delays, scope fights, and blame games. The cross-functional triad (Eng/PM/Design) is the atomic unit of product development — breakdowns here are the root cause of most delivery failures.",
    "how": "Run weekly triad syncs with shared context documents, establish joint roadmap ownership with explicit DACI roles, resolve disagreements through data and structured trade-off analysis where each function has veto power in their domain. Validate with metric 3.5 (SLO Compliance Rate) and 3.6 (Incident Rate by Severity).",
    "expectedResult": "Aligned decisions balancing user needs, business goals, and technical feasibility; cross-functional escalations drop to near-zero; triad partners actively seek each other's input on scope decisions.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-106",
        "observableId": "C5-O1",
        "capabilityId": "C5",
        "signalText": "'Strong partner feedback from PM and Design' — explicit calibration dimension at most Big Tech companies",
        "signalType": "calibration_language",
        "sourceSubTopic": "Triad Dynamics"
      }
    ]
  },
  {
    "id": "C5-O2",
    "capabilityId": "C5",
    "shortText": "Navigates PM disagreements by framing trade-offs in business terms",
    "slug": "navigates-pm-disagreements-by-framing-trade-offs-in-business-terms",
    "fullExample": "Responds to PM's deadline pressure by quantifying tech debt cost: 'This approach costs 200 eng-hours in maintenance over 3 quarters' and proposes three options with trade-offs.",
    "evidenceTypes": [
      "meeting_note",
      "decision_doc"
    ],
    "defaultWeight": 0.089,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels; Directors model for EMs",
    "why": "EMs who just say no are ineffective; EMs who always say yes create debt mountains. Productive disagreement requires framing your position in terms the other side values — then committing fully once decided.",
    "how": "Quantify trade-offs in business terms (eng-hours, maintenance cost, opportunity cost) using a structured trade-off doc template. Propose 2-3 alternatives with explicit consequences — include timeline, risk, and tech-debt impact for each option. Escalate with data if stuck after one week of discussion — never escalate with opinions. Document decisions in an ADR (Architecture Decision Record) for future reference. Review recurring disagreement patterns in quarterly PM-EM retros. Validate with metric 8.3 (Cost of Delay) and 8.6 (Tech Debt Ratio).",
    "expectedResult": "Decisions balance short-term needs with long-term health; PM trusts your judgment.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-107",
        "observableId": "C5-O2",
        "capabilityId": "C5",
        "signalText": "EMs who just say 'no' to PM are ineffective. EMs who always say 'yes' create debt mountains. The skill is the nuance between them",
        "signalType": "calibration_language",
        "sourceSubTopic": "Navigating Disagreements with PM"
      }
    ]
  },
  {
    "id": "C5-O3",
    "capabilityId": "C5",
    "shortText": "Proactively shapes product strategy through technical insight",
    "slug": "proactively-shapes-product-strategy-through-technical-insight",
    "fullExample": "Identifies a new API capability that enables a product surface PM hadn't considered; presents with user impact framing, leading to a roadmap pivot.",
    "evidenceTypes": [
      "meeting_note",
      "doc"
    ],
    "defaultWeight": 0.067,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs contribute for team scope; Directors influence org-level product strategy",
    "why": "When engineering is treated as feature factory, technical possibilities don't inform product direction. The trio model explicitly requires engineering to co-own product discovery — the EM isn't waiting for a spec, they're shaping what gets built.",
    "how": "Surface technical capabilities that unlock new product surfaces, present to PM with user impact framing (not tech framing), participate in product strategy sessions as a peer with standing to propose roadmap items. Validate with metric 3.3 (Error Budget Remaining + Burn Rate) and 3.5 (SLO Compliance Rate).",
    "expectedResult": "Product strategy informed by technical possibilities; engineering seen as creative partner.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-108",
        "observableId": "C5-O3",
        "capabilityId": "C5",
        "signalText": "Director-level: 'Technical insight led to [product direction change], driving [Y] outcome that PM hadn't considered'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Technical Input to Product Strategy"
      },
      {
        "id": "SIG-176",
        "observableId": "C5-O3",
        "capabilityId": "C5",
        "signalText": "Observed in cross-functional syncs: PM and Design actively seek EM's input on scope trade-offs, EM raises technical constraints without blocking, disagreements resolved constructively in the room.",
        "signalType": "manager_observation",
        "sourceSubTopic": "Cross-Functional Collaboration Quality"
      }
    ]
  },
  {
    "id": "C5-O4",
    "capabilityId": "C5",
    "shortText": "Resolves cross-team dependencies through partnership, not escalation",
    "slug": "resolves-cross-team-dependencies-through-partnership-not-escalation",
    "fullExample": "Builds relationship with partner EM before needing anything; frames dependency request as mutual benefit; unblocks project without VP involvement.",
    "evidenceTypes": [
      "meeting_note",
      "email"
    ],
    "defaultWeight": 0.09,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs negotiate team-level; Directors broker org-level dependencies",
    "why": "Dependencies on teams with different priorities can stall your roadmap indefinitely. Service-oriented architecture explicitly minimizes cross-team dependencies — when dependencies are unavoidable, teams negotiate SLAs with explicit timelines and escalation paths.",
    "how": "Build relationships with partner EMs before needing anything — schedule recurring monthly 1:1s with key dependency owners. Understand their priorities and OKRs; frame dependency requests as mutual benefit with clear value proposition for both teams. Contribute back (bug fixes, documentation, design reviews) to build reciprocity. Track cross-team dependencies in a shared tracker (Jira, Linear) and review status weekly. Target: resolve dependencies through partnership within 2 sprints, escalation rate <10% of cross-team requests. Validate with metric 2.6 (Blocked Work Rate) and 2.1 (Flow Time).",
    "expectedResult": "Dependencies resolved through partnership; both teams benefit; relationship preserved.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-109",
        "observableId": "C5-O4",
        "capabilityId": "C5",
        "signalText": "Influence skill: 'Negotiated priority alignment with [partner team], unblocking [project] without escalation'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Dependency Negotiation"
      },
      {
        "id": "SIG-112",
        "observableId": "C5-O4",
        "capabilityId": "C5",
        "signalText": "'Strong TPM partnership enabled [X] cross-team initiative to deliver on time across [Y] teams'",
        "signalType": "calibration_language",
        "sourceSubTopic": "TPM Partnership"
      }
    ]
  },
  {
    "id": "C5-O5",
    "capabilityId": "C5",
    "shortText": "Builds effective partnerships with platform, infra, and support functions",
    "slug": "builds-effective-partnerships-with-platform-infra-and-support-functions",
    "fullExample": "Joins platform team's planning process, provides clear requirements with business justification, contributes bug fixes back; team's needs reflected in platform roadmap.",
    "evidenceTypes": [
      "meeting_note",
      "doc"
    ],
    "defaultWeight": 0.067,
    "requiredFrequency": "continuous",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs partner at team level; Directors broker org-level platform investment",
    "why": "Platform teams serve many customers and your needs get deprioritized without active partnership. Teams that depend on internal platforms should designate a liaison who maintains a direct relationship with the platform team's PM — passive ticket filing produces passive deprioritization.",
    "how": "Attend platform and infra team roadmap reviews quarterly. Provide requirements with business justification and prioritized use cases. Contribute back — submit bug fixes, write documentation, volunteer as design partner for new capabilities. Maintain a shared requirements backlog with platform team and review priority alignment monthly. Track platform adoption metrics and feedback scores to demonstrate partnership ROI. Validate with metric 5.8 (IDP Adoption Rate) and 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Your needs reflected in platform roadmaps; mutual investment creates better platform for everyone.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-110",
        "observableId": "C5-O5",
        "capabilityId": "C5",
        "signalText": "Directors broker org-level platform partnerships: 'Partnered with [platform team] on [capability], adopted by [X] teams'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Platform & Infra Partnerships"
      },
      {
        "id": "SIG-111",
        "observableId": "C5-O5",
        "capabilityId": "C5",
        "signalText": "Increasingly important partnership. 'Built DS collaboration model that increased experiment velocity from X to Y experiments/quarter'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Data Science & Analytics Partnerships"
      },
      {
        "id": "SIG-113",
        "observableId": "C5-O5",
        "capabilityId": "C5",
        "signalText": "'Built pre-approved design patterns for [privacy scenario], eliminating [X] weeks of launch-blocking legal review'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Legal/Privacy/Policy Partnerships"
      }
    ]
  },
  {
    "id": "C5-O6",
    "capabilityId": "C5",
    "shortText": "Manages up by mapping to manager's priorities, pre-wiring before surprises, and providing solutions with options",
    "slug": "manages-up-intentionally-to-build-trust-autonomy-and-sponsorship",
    "fullExample": "Understands manager's priorities; brings solutions not just problems; pre-wires before surprises; manager advocates for team in rooms they're not in.",
    "evidenceTypes": [
      "email",
      "meeting_note"
    ],
    "defaultWeight": 0.067,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels; the #1 relationship for career trajectory",
    "why": "Manager relationship determines impact ceiling, autonomy level, and career trajectory. Earning trust applies upward too — your manager's confidence in you is the single biggest variable in how much scope you get.",
    "how": "Map your manager's top 3 priorities and align your updates to them, bring solutions with options (not open-ended problems), pre-wire before any meeting where they might be surprised, send weekly written updates in their preferred format",
    "expectedResult": "Strong sponsorship; more autonomy; manager advocates in rooms you're not in; trust level high enough that you get benefit of the doubt during setbacks.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-058",
        "observableId": "C5-O6",
        "capabilityId": "C5",
        "signalText": "Navigating leadership transitions is a survival skill. 'Built trust with new [Director/VP] within 30 days, aligned roadmap to new priorities, maintained team stability'",
        "signalType": "calibration_language",
        "sourceSubTopic": "New Leadership Above You"
      },
      {
        "id": "SIG-116",
        "observableId": "C5-O6",
        "capabilityId": "C5",
        "signalText": "Your manager is your primary sponsor in calibration and promo discussions. Invest in this relationship deliberately",
        "signalType": "promo_packet",
        "sourceSubTopic": "Managing Your Manager"
      }
    ]
  },
  {
    "id": "C5-O7",
    "capabilityId": "C5",
    "shortText": "Accumulates trust through tracked commitments, 24-hour peer unblocking, public credit sharing, and consistent follow-through",
    "slug": "builds-political-capital-through-consistent-delivery-and-helping-others-succeed",
    "fullExample": "Accumulates trust by: delivering consistently, sharing credit, unblocking peer teams, following through on every commitment. Trusted advisor for domain decisions.",
    "evidenceTypes": [
      "peer_feedback",
      "meeting_note"
    ],
    "defaultWeight": 0.067,
    "requiredFrequency": "continuous",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "Both levels; compounds over time and pays off enormously",
    "why": "Influence comes from accumulated trust and credibility, not from title. Leaders who consistently deliver and help others succeed accumulate disproportionate influence regardless of org chart position — peer feedback patterns are the most reliable measure of earned trust.",
    "how": "Deliver consistently on every commitment — track commitments in a personal log and follow up within stated timelines (small ones matter most). Unblock peer teams without keeping score; respond to cross-team requests within 24 hours. Share credit publicly in team-wide and org-wide forums. Share information proactively via written updates, not just hallway conversations. Seek 360-degree peer feedback annually to calibrate trust trajectory — treat trust as compound interest that takes years to build and moments to destroy. Validate with metric 8.1 (OKR Achievement Rate).",
    "expectedResult": "Opinions carry weight in cross-org decisions; peers proactively include you in strategy discussions; trusted advisor status across organizational boundaries.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-045",
        "observableId": "C5-O7",
        "capabilityId": "C5",
        "signalText": "Director-level: trust is the currency of cross-org execution. Built through consistent follow-through, not team-building exercises",
        "signalType": "calibration_language",
        "sourceSubTopic": "Trust Building Across Teams"
      },
      {
        "id": "SIG-063",
        "observableId": "C5-O7",
        "capabilityId": "C5",
        "signalText": "The 90-day window determines your trajectory. 'Established credibility by [quick win], built stakeholder trust through [action]'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Earning Credibility as New Leader"
      },
      {
        "id": "SIG-119",
        "observableId": "C5-O7",
        "capabilityId": "C5",
        "signalText": "Compound interest of engineering leadership — builds slowly, pays off enormously. 'Trusted advisor to [peer/senior leaders] for [domain] decisions'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Building Political Capital"
      }
    ]
  },
  {
    "id": "C5-O8",
    "capabilityId": "C5",
    "shortText": "Pre-wires alignment across orgs before formal decision meetings",
    "slug": "pre-wires-alignment-across-orgs-before-formal-decision-meetings",
    "fullExample": "Has 1:1 conversations with each stakeholder before group meeting; creates shared context document; proposes strawman for reaction. Drives convergence without VP forcing it.",
    "evidenceTypes": [
      "meeting_note",
      "doc"
    ],
    "defaultWeight": 0.09,
    "requiredFrequency": "episodic",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Directors use extensively; EMs learn the pattern",
    "why": "Getting 4 teams to agree when you don't manage them is pure influence — cold meetings rarely converge because positions harden publicly. Alignment is built through 1:1 pre-wiring before the group meeting — the meeting itself is for ratification, not negotiation.",
    "how": "Pre-wire alignment through 1:1 conversations with each key stakeholder before any group decision meeting. Create a shared context document (Google Doc, Notion, or RFC) with problem statement, options, and recommendation. Propose a strawman solution and invite stakeholders to edit toward consensus asynchronously before the meeting. For cross-org initiatives, allocate 2-3 days for pre-wiring per major decision. Target: >80% of org-level decisions reached in the first group meeting, escalation rate to VP <10%. Validate with metric 8.1 (OKR Achievement Rate).",
    "expectedResult": "Org-level initiatives move forward without escalation; seen as someone who makes things happen.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-120",
        "observableId": "C5-O8",
        "capabilityId": "C5",
        "signalText": "Director-level core competency: 'Drove alignment across [X] orgs on [Y] initiative without escalation'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Driving Alignment Across Orgs"
      }
    ]
  },
  {
    "id": "C5-O9",
    "capabilityId": "C5",
    "shortText": "Presents at VP reviews quarterly and cultivates sponsor relationships through recurring 1:1s and cross-org initiative delivery",
    "slug": "builds-sponsor-relationships-with-senior-leaders-for-visibility-and-advocacy",
    "fullExample": "Delivers visible results, presents at VP reviews, volunteers for cross-org initiatives; VP sponsors them for scope expansion.",
    "evidenceTypes": [
      "peer_feedback",
      "meeting_note"
    ],
    "defaultWeight": 0.067,
    "requiredFrequency": "continuous",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Directors actively cultivate; EMs begin building",
    "why": "Mentors give advice; sponsors give opportunities. Without sponsors, you're invisible above skip-level. Promotion committees weight sponsor testimony heavily — having a senior leader who has seen your work firsthand and advocates for you in rooms you're not in is the difference between recognition and invisibility.",
    "how": "Deliver visible, high-impact results that senior leaders value. Seek exposure by presenting at VP/Director reviews quarterly and volunteering for cross-org initiatives aligned with their strategic priorities. Build genuine relationships through recurring 1:1s (monthly or quarterly) with potential sponsors — share progress, ask for feedback, and demonstrate coachability. Target: at least one senior leader who actively advocates for your work in rooms you are not in within 12 months. Validate with metric 8.1 (OKR Achievement Rate) and 8.2 (Revenue / Business Impact Attribution).",
    "expectedResult": "Opportunities come to you; work gets visibility; advocates in promotion and scope discussions.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-122",
        "observableId": "C5-O9",
        "capabilityId": "C5",
        "signalText": "Sponsors are essential for promotion above Director level. 'Sponsored by [VP] for [opportunity/promotion]'",
        "signalType": "promo_packet",
        "sourceSubTopic": "Sponsor Relationships"
      }
    ]
  },
  {
    "id": "C5-O10",
    "capabilityId": "C5",
    "shortText": "Navigates scope ambiguity by demonstrating capability, not political maneuvering",
    "slug": "navigates-scope-ambiguity-by-demonstrating-capability-not-political-maneuvering",
    "fullExample": "When scope between two orgs is ambiguous, demonstrates competence by delivering in the space; frames expansion in terms of customer benefit; builds coalition of supporters.",
    "evidenceTypes": [
      "decision_doc",
      "peer_feedback"
    ],
    "defaultWeight": 0.045,
    "requiredFrequency": "episodic",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Director-level skill for scope expansion",
    "why": "Ambiguous ownership + growth = territory disputes that burn relationships if handled wrong. Scope expansion is earned through demonstrated competence — you demonstrate capability in the space by shipping value, not by lobbying leadership.",
    "how": "Demonstrate capability by delivering in the ambiguous space first — ship a concrete win (prototype, migration, improvement) within one quarter using a structured impact summary template (problem, action, outcome, metrics). Frame scope expansion in terms of customer/org benefit using data (usage metrics, customer impact, cost savings), never empire-building language. Build a coalition of 3+ supporters who witnessed your competence and will advocate in scope discussions. Be transparent about intentions with your manager and skip-level via a quarterly scope roadmap shared in 1:1s. Target: scope proposal accepted within two quarters of first delivery. Validate with metric 8.2 (Revenue / Business Impact Attribution) and 8.4 (Engineering Investment Mix).",
    "expectedResult": "Scope expands through earned trust; relationships preserved. Stakeholders proactively include you in planning conversations. Zero surprised stakeholders per quarter; at least one new cross-functional partnership initiated per half.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-123",
        "observableId": "C5-O10",
        "capabilityId": "C5",
        "signalText": "Director skill: 'Expanded org scope to include [X] based on [demonstrated capability], increasing org impact by [Y]'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Navigating Scope & Land Grabs"
      }
    ]
  },
  {
    "id": "C6-O1",
    "capabilityId": "C6",
    "shortText": "Runs structured 1:1s with 60/20/20 agenda split, shared doc, and consistent follow-through on commitments",
    "slug": "runs-structured-11s-that-build-trust-and-drive-career-development",
    "fullExample": "Weekly 1:1s with shared doc; 60% their agenda / 20% yours / 20% career; follows through on every commitment. Skip-level feedback confirms quality.",
    "evidenceTypes": [
      "note",
      "doc"
    ],
    "defaultWeight": 0.105,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own directly; Directors coach EMs on 1:1 effectiveness",
    "why": "Without structured 1:1s, problems fester, careers stagnate, and trust erodes slowly. 1:1s that become status updates are the most common failure mode. Research consistently identifies 'is a good coach' as the #1 behavior of effective managers — and coaching happens in 1:1s, not all-hands.",
    "how": "Maintain consistent weekly cadence with shared running doc (60% their agenda / 20% yours / 20% career), follow through on every action item, separate quarterly career conversations from weekly tactical. Validate with metric 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Trust built over time; early problem detection; career development progress; no performance review surprises; skip-level feedback scores above 4.0/5.0 on 'my manager supports my growth'.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-059",
        "observableId": "C6-O1",
        "capabilityId": "C6",
        "signalText": "Foundation of people management. Skip-level feedback reveals 1:1 quality: 'My manager makes time for me and follows through'",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Effective 1:1s"
      },
      {
        "id": "SIG-080",
        "observableId": "C6-O1",
        "capabilityId": "C6",
        "signalText": "Retention correlates with career development quality. 'All direct reports have active development plans reviewed quarterly'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Career Conversations"
      }
    ]
  },
  {
    "id": "C6-O2",
    "capabilityId": "C6",
    "shortText": "Addresses underperformance early with clear expectations and genuine support",
    "slug": "addresses-underperformance-early-with-clear-expectations-and-genuine-support",
    "fullExample": "Verifies expectations were clear, documents specific gaps with examples, sets improvement plan with milestones, holds weekly check-ins; person either improves or exits with dignity.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.13,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs execute directly; Directors coach EMs through difficult cases",
    "why": "Delayed feedback leaves people confused; the team suffers while the EM avoids the conversation. The keeper test asks 'would I fight to keep this person?' — if the answer is no, that conversation should have started weeks ago.",
    "how": "Verify expectations were clear (the gap might be yours) using a written expectations doc shared within the first week. Document specific behavioral gaps with concrete examples in a shared Google Doc or Notion page within 48 hours of observed behavior. Set a formal Performance Improvement Plan (PIP) with 3-5 measurable milestones, 30/60/90-day checkpoints, and weekly 30-minute check-ins using a structured agenda. Provide genuine coaching — not a paper trail for termination — by pairing the individual with a peer mentor and offering skill-building resources. Target measurable improvement within 60 days; if no progress by day 30, escalate to HR partnership. Validate with metric 5.1 (Developer Satisfaction Score) and 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "Underperformers either improve measurably within 60 days or exit with dignity; high performers feel valued because standards are real; team trust in management increases.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-047",
        "observableId": "C6-O2",
        "capabilityId": "C6",
        "signalText": "THE hardest EM skill. Calibration committees notice avoidance — 'entire team rated Exceeds Expectations' is a credibility killer",
        "signalType": "calibration_language",
        "sourceSubTopic": "Handling Underperformance"
      },
      {
        "id": "SIG-177",
        "observableId": "C6-O2",
        "capabilityId": "C6",
        "signalText": "Observed in skip-levels: direct reports describe clear expectations and regular feedback from EM. EM addresses performance issues directly rather than routing around struggling engineers or reassigning their work silently.",
        "signalType": "manager_observation",
        "sourceSubTopic": "Performance Conversation Quality"
      }
    ]
  },
  {
    "id": "C6-O3",
    "capabilityId": "C6",
    "shortText": "Identifies flight risk through behavioral signals and intervenes proactively",
    "slug": "identifies-flight-risk-through-behavioral-signals-and-intervenes-proactively",
    "fullExample": "Notices reduced engagement and vague answers about future plans from key engineer; conducts stay interview, surfaces growth concern, creates stretch assignment. Engineer stays.",
    "evidenceTypes": [
      "note",
      "meeting_note"
    ],
    "defaultWeight": 0.078,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs catch individual signals; Directors track patterns across teams",
    "why": "By the time someone gives notice, the decision was made weeks ago — early signals are the intervention window. Psychological safety research shows it's the #1 predictor of team effectiveness, and disengagement signals its erosion before attrition confirms it.",
    "how": "Watch for: reduced engagement in design reviews, declining code review thoroughness, withdrawing from optional activities, vague answers about future plans. Track engagement signals weekly via 1:1 sentiment and participation patterns. When risk detected, intervene within one week — surface the concern directly and explore root cause (see C6-O8 for systematic stay interview program). Validate with metric 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Early intervention on retention risks; save key people before they mentally check out.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-050",
        "observableId": "C6-O3",
        "capabilityId": "C6",
        "signalText": "Directors track attrition patterns across teams; EMs catch individual signals. 'Zero regrettable attrition in [X] quarters through proactive retention'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Identifying Flight Risk"
      },
      {
        "id": "SIG-051",
        "observableId": "C6-O3",
        "capabilityId": "C6",
        "signalText": "Counter-offers that don't address root cause just delay departure by 6 months. 'Conducted stay interviews, addressed [specific issue], retained [X] key engineers'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Retention Conversations & Counter-Offers"
      },
      {
        "id": "SIG-054",
        "observableId": "C6-O3",
        "capabilityId": "C6",
        "signalText": "Advanced people management practice: 'Instituted stay interviews, surfaced [X] theme, addressed it, retained [Y] at-risk engineers'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Stay Interviews"
      }
    ]
  },
  {
    "id": "C6-O4",
    "capabilityId": "C6",
    "shortText": "Manages high performers with increasing scope, visibility, and sponsorship",
    "slug": "manages-high-performers-with-increasing-scope-visibility-and-sponsorship",
    "fullExample": "Quarterly career conversations with top talent; provides stretch assignments, visibility opportunities, and comp advocacy. Grows senior engineer to staff through deliberate sponsorship.",
    "evidenceTypes": [
      "doc",
      "peer_feedback"
    ],
    "defaultWeight": 0.105,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs manage directly; Directors sponsor across org",
    "why": "Top talent has different needs; treating them like average performers is how you lose them. High performers should get increasing decision-making authority, not just harder assignments — the reward for great work is more autonomy, not more process.",
    "how": "Quarterly career conversations focused on their ambitions (not your staffing needs) using a structured IDP (Individual Development Plan) template in Google Docs or Lattice, reviewed and updated each quarter. Provide stretch assignments with real stakes and real visibility — assign at least one high-visibility project per half that maps to next-level competencies. Sponsor them in rooms they aren't in by naming their contributions in staff meetings, calibration sessions, and skip-level reviews. Advocate aggressively for comp and promotion with a written promo packet started 6 months before target cycle. Track retention of top-quartile performers quarterly, targeting zero regrettable attrition. Validate with metric 7.1 (Regrettable Attrition Rate) and 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Top talent feels invested in, challenged, and visible; zero regrettable attrition among top-quartile performers; promotion rate for your high performers exceeds org average.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-052",
        "observableId": "C6-O4",
        "capabilityId": "C6",
        "signalText": "Losing top performers is a red flag. 'Grew [X] from senior to staff through [stretch assignment], retained through [specific action]'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Managing High Performers"
      },
      {
        "id": "SIG-082",
        "observableId": "C6-O4",
        "capabilityId": "C6",
        "signalText": "Directors coach Staff+ engineers; EMs learn to work alongside them. 'Supported [Staff engineer]'s [initiative] that impacted [X] teams'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Managing Senior/Staff Engineers"
      }
    ]
  },
  {
    "id": "C6-O5",
    "capabilityId": "C6",
    "shortText": "Designs stretch assignments that build promotion evidence through real work",
    "slug": "designs-stretch-assignments-that-build-promotion-evidence-through-real-work",
    "fullExample": "Identifies business need as stretch opportunity for engineer; provides clear expectations, check-in cadence, and safety net. Engineer demonstrates next-level capability and gets promoted.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.078,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs design for team; Directors create cross-org opportunities",
    "why": "Engineers do great work but can't demonstrate next-level capability without deliberately crafted opportunities. Promotion requires evidence at the next level sustained over time — the EM's job is to create the conditions where that evidence can be generated through real business-critical work.",
    "how": "Identify scope gaps that match growth plans by reviewing the team's IDP backlog and upcoming roadmap quarterly for alignment opportunities. Scaffold with clear expectations documented in a stretch assignment brief (scope, success criteria, timeline, support structure) and bi-weekly 30-minute check-ins using a shared progress tracker. Provide safety net (you own the blast radius if it goes wrong) with explicit rollback plans and escalation paths defined upfront. Ensure the work has visibility with calibration committee members by scheduling a mid-project demo or review with senior stakeholders. Target at least one stretch assignment per engineer per half, with >70% successful completion rate. Validate with metric 5.1 (Developer Satisfaction Score) and 7.5 (New Hire Ramp Time).",
    "expectedResult": "Engineers build promotion evidence through real work; stretch assignments succeed because supported.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-081",
        "observableId": "C6-O5",
        "capabilityId": "C6",
        "signalText": "Strategic matching: 'Identified [business need] as stretch opportunity for [engineer], resulting in [outcome] and successful promotion'",
        "signalType": "promo_packet",
        "sourceSubTopic": "Stretch Assignment Design"
      }
    ]
  },
  {
    "id": "C6-O6",
    "capabilityId": "C6",
    "shortText": "Coaches EMs through management situations using scenario-based development",
    "slug": "coaches-ems-through-management-situations-using-scenario-based-development",
    "fullExample": "Weekly 1:1s focused on management craft, not just status. Walks EM through 'your engineer says they're burned out' scenario; EM develops independent judgment over 6 months.",
    "evidenceTypes": [
      "note",
      "meeting_note"
    ],
    "defaultWeight": 0.105,
    "requiredFrequency": "continuous",
    "emRelevance": "Director-Only",
    "directorRelevance": "High",
    "levelNotes": "Director-only capability",
    "why": "EMs who only learn project management never develop the people-management craft that defines the role. Effective EM coaching pairs new managers with experienced mentors for scenario-based development — real situations like difficult feedback, underperformance, and team conflict.",
    "how": "Conduct weekly 1:1s (45-60 minutes) focused on management craft, not status updates, using a shared coaching agenda in Google Docs or Notion. Use scenario-based coaching: present real situations (e.g., 'your engineer says they're burned out — walk me through your approach') and debrief actual interactions observed in team meetings or skip-levels. Observe EM in action during team standups, 1:1s (with permission), and calibration sessions at least monthly. Track EM growth using a management competency rubric assessed quarterly across dimensions: feedback delivery, conflict resolution, career development, and hiring judgment. Target EM independence within 6 months — measured by reduction in escalations from >3/week to <1/week. Run quarterly skip-level surveys targeting >4.0/5.0 satisfaction with direct manager. Validate with metric 5.1 (Developer Satisfaction Score) and 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "EMs grow into strong independent managers; reduced escalation to Director; management quality improves.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-092",
        "observableId": "C6-O6",
        "capabilityId": "C6",
        "signalText": "Director's primary output is EM quality — measured by EM's team outcomes, EM's team health, EM's retention numbers",
        "signalType": "metric",
        "sourceSubTopic": "EM Coaching & Development"
      },
      {
        "id": "SIG-093",
        "observableId": "C6-O6",
        "capabilityId": "C6",
        "signalText": "Standard Director practice — absence is a red flag. 'Regular skip-level 1:1s identified [X] issue before it became a retention risk'",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Skip-Level 1:1s"
      }
    ]
  },
  {
    "id": "C6-O7",
    "capabilityId": "C6",
    "shortText": "Develops manager pipeline through TL rotations, EM-in-Training shadowing programs, and deliberate readiness development",
    "slug": "builds-manager-bench-through-intentional-em-pipeline-development",
    "fullExample": "Identifies high-potential ICs, creates TL rotation opportunities, runs EM-in-training program with shadowing. Manager transitions happen with zero team disruption.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.078,
    "requiredFrequency": "continuous",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Directors own pipeline; EMs identify candidates from their team",
    "why": "Without bench depth, EM departures create team chaos and succession scrambles. A structured leadership pipeline ensures that when a manager leaves, the team has multiple internal candidates ready within one quarter.",
    "how": "Identify high-potential ICs with management interest through quarterly career conversations and 360 feedback signals. Create a formal TL rotation program (3-6 month rotations) where candidates lead a workstream with coaching support. Run an EM-in-Training program: shadow 1:1s for 4 weeks, attend calibration as observer, co-facilitate retrospectives, and lead a hiring loop with debrief. Maintain a succession matrix in a shared doc updated quarterly listing primary and secondary candidates for each EM/TL role with readiness assessment (ready now / 6 months / 12 months). Target at least 2 ready-now successors per EM role. Measure manager bench strength by tracking time-to-fill for internal EM transitions (target <2 weeks). Validate with metric 7.5 (New Hire Ramp Time) and 7.6 (Span of Control).",
    "expectedResult": "Manager bench depth; smooth transitions; career path visible for aspiring managers.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-096",
        "observableId": "C6-O7",
        "capabilityId": "C6",
        "signalText": "'Developed [X] engineers into management roles' — org building evidence. 'EM transition to new role had zero team disruption due to succession planning'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Building EM Bench Strength"
      }
    ]
  },
  {
    "id": "C7-O1",
    "capabilityId": "C7",
    "shortText": "Frames decisions with options, trade-offs, and clear recommendation",
    "slug": "frames-decisions-with-options-trade-offs-and-clear-recommendation",
    "fullExample": "Sends decision memo with 3 options, quantified trade-offs, recommended path, and anticipated objections. Decision is made in first review meeting.",
    "evidenceTypes": [
      "decision_doc",
      "meeting_note"
    ],
    "defaultWeight": 0.154,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels; Directors at higher-stakes decisions",
    "why": "Vague proposals get sent back; options without trade-offs don't enable real decisions. At Amazon, the 6-pager narrative format forces structured thinking — conclusion first, supporting data, alternatives considered, and risks acknowledged — because PowerPoint lets you hide weak reasoning behind bullet points.",
    "how": "Lead with recommendation and 'why now,' quantify impact of each option, present trade-offs with explicit costs, anticipate the top 3 objections and address them preemptively — structured after Amazon's 6-pager discipline where the document does the thinking, not the meeting. Use a decision memo template (Google Docs or Confluence) with required sections: context, options (minimum 3 including 'do nothing'), quantified trade-offs, recommended path, risks, and reversibility assessment. For every significant decision (>1 engineer-week of effort), produce a written artifact before scheduling a review meeting. Target first-meeting approval rate >80% as a quality signal. Circulate the memo 48 hours before the meeting with a 'silent reading' period. Validate with metric 5.1 (Developer Satisfaction Score) and 8.1 (OKR Achievement Rate).",
    "expectedResult": "Proposals get approved in first review meeting; seen as a strategic partner, not just an implementer; decisions stick because stakeholders understood the trade-offs upfront.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-064",
        "observableId": "C7-O1",
        "capabilityId": "C7",
        "signalText": "Judgment signal: 'Identified [X] issues in first month, addressed critical ones immediately, built 90-day plan for systemic changes'",
        "signalType": "calibration_language",
        "sourceSubTopic": "When to Change vs When to Observe"
      },
      {
        "id": "SIG-115",
        "observableId": "C7-O1",
        "capabilityId": "C7",
        "signalText": "Director-level: every proposal should pass the 'so what' test. 'Secured [resources] by framing [project] as [business impact]'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Framing Asks & Proposals"
      },
      {
        "id": "SIG-133",
        "observableId": "C7-O1",
        "capabilityId": "C7",
        "signalText": "Applied in practice: 'Classified [X] as reversible, shipped in 2 days instead of 2 weeks. Classified [Y] as irreversible, invested 3 weeks in analysis — avoided $Zm mistake'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Reversible vs Irreversible Decisions"
      }
    ]
  },
  {
    "id": "C7-O2",
    "capabilityId": "C7",
    "shortText": "Adapts communication altitude to audience — exec summary to detailed appendix",
    "slug": "adapts-communication-altitude-to-audience-exec-summary-to-detailed-appendix",
    "fullExample": "Creates one-slide exec memo for VP and detailed appendix for engineering; same content, different altitude. Both audiences get what they need.",
    "evidenceTypes": [
      "slide",
      "doc"
    ],
    "defaultWeight": 0.092,
    "requiredFrequency": "continuous",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs learn this; Directors must master it for VP/exec audiences",
    "why": "Engineering detail overwhelms executives; executive brevity leaves engineers without context. At Netflix, context-not-control means leaders must communicate the 'why' at the right altitude for each audience — the same decision needs three different framings to land with VPs, peer EMs, and ICs.",
    "how": "Apply pyramid principle: conclusion first for execs (3 bullets, business impact), technical trade-offs for peers, full implementation context for ICs. Maintain 3 versions of key communications: a 1-slide executive summary (30-second read), a 3-slide peer briefing (5-minute read), and a full technical appendix (15-minute read) — all in a shared Google Slides or Confluence template. Practice the 'so what' test on every slide and paragraph before sending. For recurring updates (weekly status, quarterly reviews), use a consistent template so the audience learns where to find information. Solicit feedback quarterly from each audience tier: 'Is my communication at the right altitude for you?' Target zero instances of 'too much detail' or 'not enough context' feedback per quarter. Validate with metric 5.1 (Developer Satisfaction Score) and 7.8 (eNPS / Engagement Score).",
    "expectedResult": "Executives understand and support direction; engineers have implementation context.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-138",
        "observableId": "C7-O2",
        "capabilityId": "C7",
        "signalText": "Director-level differentiation: same update tailored to 3 audiences — exec summary for VP (3 bullets, business impact), team-level detail for peers (technical trade-offs), and full context for IC contributors (implementation specifics). Skip-level feedback confirms: 'My manager communicates at the right level for each audience.'",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Communication Altitude Adaptation"
      }
    ]
  },
  {
    "id": "C7-O3",
    "capabilityId": "C7",
    "shortText": "Communicates status proactively — wins, risks, and needs — earning autonomy",
    "slug": "communicates-status-proactively-wins-risks-and-needs-earning-autonomy",
    "fullExample": "Weekly written update to manager: top 3 wins, top 3 risks, what I need from you. Manager says: 'always knows what's happening, no surprises.'",
    "evidenceTypes": [
      "email",
      "doc"
    ],
    "defaultWeight": 0.123,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels; the #1 lever for getting more scope and autonomy",
    "why": "Leaders who learn about problems from others lose trust fast. At Amazon, the 'no surprises' principle is a career-defining expectation — your manager should never learn about a problem in your area from someone else's status update.",
    "how": "Send weekly written update structured as: top 3 outcomes (not activities), top 3 risks with mitigation status, specific asks; flag bad news within 24 hours, never in a weekly summary. Validate with metric 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Leadership trusts you; more autonomy and scope granted proactively; problems discussed early when options still exist; manager's manager knows your name for the right reasons.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-114",
        "observableId": "C7-O3",
        "capabilityId": "C7",
        "signalText": "The #1 lever for getting more scope and responsibility. 'Manager feedback: always knows what's happening, no surprises'",
        "signalType": "manager_observation",
        "sourceSubTopic": "Effective Status Communication"
      }
    ]
  },
  {
    "id": "C7-O4",
    "capabilityId": "C7",
    "shortText": "Delivers bad news early with ownership and a mitigation plan",
    "slug": "delivers-bad-news-early-with-ownership-and-a-mitigation-plan",
    "fullExample": "Flags project slip 3 weeks before deadline with root cause, impact assessment, and 2 mitigation options. Leadership thanks them for transparency.",
    "evidenceTypes": [
      "email",
      "meeting_note"
    ],
    "defaultWeight": 0.123,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels; trust is built by how you handle bad news",
    "why": "Hiding bad news to 'try to fix it first' is how surprises destroy credibility. At Google, structured decision-making requires surfacing risks early with data — the penalty for a late surprise far exceeds the penalty for an early honest assessment.",
    "how": "Immediately upon discovery (within 4 hours for Sev1, within 24 hours for Sev2): send a structured bad-news notification using a template in Slack or email with sections: what happened, customer/business impact (quantified in users affected or revenue at risk), preliminary root cause, current mitigation actions, what you need from leadership, and next update ETA. Own it even if the root cause wasn't your team — accountability builds credibility. Follow up with a written post-incident summary within 48 hours using a blameless retrospective format. Target zero instances per quarter where leadership learns about a problem from someone other than you. Track time-from-discovery-to-notification as a personal SLA (target <4 hours for Sev1). Validate with metric 1.3 (Mean Time to Restore) and 1.4 (Change Failure Rate).",
    "expectedResult": "Leadership trusts you to handle problems; they learn issues from you first; credibility survives setbacks.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-117",
        "observableId": "C7-O4",
        "capabilityId": "C7",
        "signalText": "Trust is built by how you handle bad news. 'Flagged [project slip] early with mitigation plan, leadership appreciated transparency'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Delivering Bad News Upward"
      },
      {
        "id": "SIG-178",
        "observableId": "C7-O4",
        "capabilityId": "C7",
        "signalText": "Observed in exec reviews: delivers updates with appropriate altitude (business impact, not implementation detail), surfaces risks proactively with options rather than waiting to be asked, handles tough questions with composure.",
        "signalType": "manager_observation",
        "sourceSubTopic": "Executive Communication Quality"
      }
    ]
  },
  {
    "id": "C7-O5",
    "capabilityId": "C7",
    "shortText": "Delivers crisp executive presentations driving decisions within allotted time",
    "slug": "delivers-crisp-executive-presentations-driving-decisions-within-allotted-time",
    "fullExample": "Pyramid-structured 5-slide business review: conclusion first, supporting metrics, anticipated questions prepared. Earns invitation to future strategy discussions.",
    "evidenceTypes": [
      "slide",
      "meeting_note"
    ],
    "defaultWeight": 0.092,
    "requiredFrequency": "episodic",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Directors must master; EMs develop this skill for growth",
    "why": "Executives have 15-minute attention spans; burying the lead wastes their time and your opportunity. At Amazon, the 6-page narrative memo forces the most important conclusion to appear first — presentations that don't drive a decision within the allotted slot are presentations that failed.",
    "how": "Structure every exec presentation using pyramid principle: conclusion on slide 1, 2-3 key data points on slide 2, appendix for deep-dives. Use a consistent 5-slide template (Situation, Recommendation, Data, Risks, Ask) in Google Slides or PowerPoint. Anticipate the question behind the question by pre-briefing a trusted exec peer 24 hours before the meeting. Practice brevity — rehearse to fit within 50% of allotted time, leaving room for discussion. Time-box preparation: 2 hours max for a 15-minute presentation. Measure success by decisions made in meeting (target: decision reached in first review >80% of the time) and by repeat invitations to strategy discussions. Solicit post-presentation feedback from your manager quarterly. Validate with metric 8.1 (OKR Achievement Rate).",
    "expectedResult": "Executives understand and support initiatives; on their radar for higher-scope opportunities.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-118",
        "observableId": "C7-O5",
        "capabilityId": "C7",
        "signalText": "'Strong executive presence in [review type]' — opens doors to Director+ scope and visibility",
        "signalType": "calibration_language",
        "sourceSubTopic": "Exec Reviews & Business Reviews"
      },
      {
        "id": "SIG-121",
        "observableId": "C7-O5",
        "capabilityId": "C7",
        "signalText": "The unlock for Director → Sr. Director/VP trajectory. 'Consistently delivers crisp executive communication' — noted in promo discussions",
        "signalType": "promo_packet",
        "sourceSubTopic": "Executive Communication"
      }
    ]
  },
  {
    "id": "C7-O6",
    "capabilityId": "C7",
    "shortText": "Implements DACI/RFC frameworks that reduce decision revisitation",
    "slug": "implements-dacirfc-frameworks-that-reduce-decision-revisitation",
    "fullExample": "Establishes DACI roles for every significant decision; RFC template with 5-day comment period and escalation path. Decision revisitation drops measurably.",
    "evidenceTypes": [
      "doc",
      "decision_doc"
    ],
    "defaultWeight": 0.092,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs implement for team; Directors standardize across org",
    "why": "Without clear decision ownership, decisions get revisited endlessly or made by whoever argues loudest. At Amazon, the 6-pager and PR/FAQ process embeds decision-making rigor into the culture — every significant decision has a written artifact with explicit ownership.",
    "how": "Name DACI roles explicitly for every decision above threshold, RFC template with required sections (context, options, recommendation, risks), 5-day comment period with escalation path, maintain searchable decision log. Validate with metric 2.6 (Blocked Work Rate).",
    "expectedResult": "Decisions made efficiently and stick; decision revisitation drops below 10%; new hires can understand why things are the way they are by reading the decision log.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-131",
        "observableId": "C7-O6",
        "capabilityId": "C7",
        "signalText": "Directors use DACI for org-level decisions; EMs use for team-level. 'Implemented DACI framework, reduced decision revisitation by X%'",
        "signalType": "metric",
        "sourceSubTopic": "DACI/RACI for Decisions"
      },
      {
        "id": "SIG-132",
        "observableId": "C7-O6",
        "capabilityId": "C7",
        "signalText": "Engineering maturity signal: 'Established RFC culture, [X] design docs/quarter, referenced in [Y] onboarding sessions'",
        "signalType": "calibration_language",
        "sourceSubTopic": "RFC/Design Doc Culture"
      }
    ]
  },
  {
    "id": "C7-O7",
    "capabilityId": "C7",
    "shortText": "Communicates roadmap trade-offs with explicit 'what we're NOT doing' section",
    "slug": "communicates-roadmap-trade-offs-with-explicit-what-were-not-doing-section",
    "fullExample": "Every planning doc includes de-prioritized items with rationale; new requests require explicit deprioritization trade-off visible to all stakeholders.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.092,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels; transparency builds trust",
    "why": "Hidden trade-offs create surprise scope cuts and erode stakeholder trust. At Spotify, squads publish explicit 'not doing' lists alongside their roadmaps — stakeholders who discover trade-offs through omission lose confidence in the team's transparency and escalate to leadership.",
    "how": "Every quarterly planning doc (in Confluence, Notion, or Google Docs) must include a mandatory 'What we are NOT doing' section listing de-prioritized items with rationale. Maintain a visible priority stack rank in Jira or Linear, ordered by business impact, and shared with all stakeholders. New requests require an explicit trade-off: 'To add X, we must deprioritize Y — do you approve?' documented in the request ticket. Review the stack rank in a monthly stakeholder sync (30 minutes) to ensure alignment. Track trade-off acceptance rate — target >90% of de-prioritization decisions supported by stakeholders at quarter-end. Publish the roadmap with 'not doing' section to all stakeholders within 1 week of planning completion. Validate with metric 8.1 (OKR Achievement Rate).",
    "expectedResult": "Informed decisions; no surprise cuts; stakeholders own trade-offs jointly. Zero roadmap items cut without prior stakeholder notification; trade-off document exists for every planning cycle; stakeholder satisfaction with communication >4/5.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-101",
        "observableId": "C7-O7",
        "capabilityId": "C7",
        "signalText": "Communication quality differentiates in calibration. 'Stakeholder feedback: team roadmap is the clearest in the org'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Roadmap Communication"
      },
      {
        "id": "SIG-102",
        "observableId": "C7-O7",
        "capabilityId": "C7",
        "signalText": "Maturity signal: 'Recommended de-prioritizing [X] to invest in [Y], presented trade-off to [VP], resulting in [Z] better outcome'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Trade-off Communication"
      }
    ]
  },
  {
    "id": "C8-O1",
    "capabilityId": "C8",
    "shortText": "Runs incident command with clear roles, reducing Sev1 duration measurably",
    "slug": "runs-incident-command-with-clear-roles-reducing-sev1-duration-measurably",
    "fullExample": "Implements ICS model: IC, Comms Lead, Ops Lead. Quarterly game days to practice. Sev1 duration reduced 40% after adoption.",
    "evidenceTypes": [
      "incident_report",
      "doc"
    ],
    "defaultWeight": 0.226,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team readiness; Directors own org-level incident process",
    "why": "Without clear roles, incidents devolve into chaos with everyone talking and nobody deciding. Defined ICS roles (IC, Comms, Ops) practiced through regular game days separate a 30-minute resolution from a 4-hour fire drill.",
    "how": "Define ICS roles (Incident Commander, Comms Lead, Ops Lead), practice with quarterly game days using realistic scenarios, debrief every incident for process improvements. Assign IC on-call rotation separate from engineering on-call. Validate with metric 1.3 (Mean Time to Restore) and 3.6 (Incident Rate by Severity).",
    "expectedResult": "Within 2 quarters: Sev1 MTTR reduced by 40%+; clear stakeholder communication during every incident; team confident in handling production emergencies.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-030",
        "observableId": "C8-O1",
        "capabilityId": "C8",
        "signalText": "EMs own team's incident readiness; Directors own org-level incident process. 'Implemented ICS model, reduced Sev1 duration by X%'",
        "signalType": "metric",
        "sourceSubTopic": "Incident Command Process"
      },
      {
        "id": "SIG-179",
        "observableId": "C8-O1",
        "capabilityId": "C8",
        "signalText": "Observed during Sev1 incident: EM took command role calmly, established clear communication cadence, delegated investigation while managing stakeholder updates, ran structured post-mortem within 48 hours.",
        "signalType": "manager_observation",
        "sourceSubTopic": "Incident Response Behavior"
      }
    ]
  },
  {
    "id": "C8-O2",
    "capabilityId": "C8",
    "shortText": "Drives blameless post-mortems with >90% action item completion",
    "slug": "drives-blameless-post-mortems-with-90-action-item-completion",
    "fullExample": "Structured template with timeline, 5-whys, systemic action items with owners and deadlines. Tracks completion rate; repeat incident rate drops measurably.",
    "evidenceTypes": [
      "postmortem",
      "metric"
    ],
    "defaultWeight": 0.226,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team post-mortems; Directors ensure pattern identification across org",
    "why": "Blame-oriented post-mortems drive incidents underground; incomplete follow-through means repeat failures. Teams that blame people for incidents learn to hide them; teams that complete >90% of action items see repeat incident rates drop to near-zero.",
    "how": "Run blameless methodology with structured template (timeline, 5-whys, contributing factors, systemic action items). Assign owners and deadlines to every action item. Track completion rate weekly. Conduct quarterly meta-review of post-mortem themes. Validate with metric 3.7 (Post-Incident Action Item Completion Rate) and 1.4 (Change Failure Rate).",
    "expectedResult": "Root causes addressed systemically; action item completion rate >90%; repeat incident rate drops measurably quarter over quarter; team reports near-misses voluntarily because the culture is genuinely blameless.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-031",
        "observableId": "C8-O2",
        "capabilityId": "C8",
        "signalText": "Strong cultural signal. 'Implemented blameless post-mortem culture, action item completion rate >90%, repeat incidents reduced X%'",
        "signalType": "metric",
        "sourceSubTopic": "Blameless Post-Mortems"
      }
    ]
  },
  {
    "id": "C8-O3",
    "capabilityId": "C8",
    "shortText": "Maintains healthy on-call with <2 off-hours pages/night and low false-positive rate",
    "slug": "maintains-healthy-on-call-with-2-off-hours-pagesnight-and-low-false-positive-rate",
    "fullExample": "Tracks pages per rotation, off-hours pages, and false positive rate. Runs noise reduction sprints when thresholds exceeded. Engineers willing to be on-call.",
    "evidenceTypes": [
      "metric",
      "dashboard"
    ],
    "defaultWeight": 0.18,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own team on-call health; Directors set org-level standards",
    "why": "High page volume drives burnout and attrition; on-call shouldn't be a source of dread. When on-call load exceeds thresholds, the team earns the right to stop feature work and fix reliability — making on-call health a first-class engineering concern.",
    "how": "Track pages per rotation, off-hours pages, and false positive rate on a dashboard. Run noise reduction sprints when thresholds exceeded. Implement compensatory time off. Use error budget policy to justify reliability investment when on-call degrades. Validate with metric 5.6 (On-Call Burden).",
    "expectedResult": "Sustainable on-call with <2 off-hours pages per night; false positive rate below 15%; engineers view on-call as a learning opportunity, not punishment; on-call health metrics stable quarter over quarter.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-032",
        "observableId": "C8-O3",
        "capabilityId": "C8",
        "signalText": "On-call health is a retention signal. High page volume + attrition = EM failure. 'Reduced off-hours pages by X% through [noise reduction / architecture improvement]'",
        "signalType": "metric",
        "sourceSubTopic": "On-Call Health"
      }
    ]
  },
  {
    "id": "C8-O4",
    "capabilityId": "C8",
    "shortText": "Conducts proactive failure mode analysis and launch readiness reviews",
    "slug": "conducts-proactive-failure-mode-analysis-and-launch-readiness-reviews",
    "fullExample": "Pre-mortem before major launch identifies 5 failure modes; mitigation in place for each. Launch checklist gates deployment. Zero Sev1s on launches for 4 consecutive quarters.",
    "evidenceTypes": [
      "doc",
      "checklist"
    ],
    "defaultWeight": 0.18,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team launch readiness; Directors standardize across org",
    "why": "Reactive risk management means you only learn from failures, not prevent them. Pre-mortem exercises before major launches — imagining failure before it happens and building mitigations — are cheaper by orders of magnitude than learning from production incidents.",
    "how": "Run pre-mortem exercises before every major launch (>100 users affected or new infrastructure dependency) using structured FMEA template. Identify failure modes, assign severity × probability scores, build mitigations for top risks. Include launch readiness checklist: observability, rollback plan, on-call coverage, communication plan. Validate with metric 3.6 (Incident Rate by Severity).",
    "expectedResult": "Known failure modes mitigated before launch; zero Sev1 incidents on launches for 4+ consecutive quarters; team builds resilience confidence through preparation, not luck.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-033",
        "observableId": "C8-O4",
        "capabilityId": "C8",
        "signalText": "Standard practice at Big Tech — EMs who skip this create visible risk. 'Zero Sev1 incidents on launches in [X] quarters through readiness discipline'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Launch Readiness Reviews"
      },
      {
        "id": "SIG-034",
        "observableId": "C8-O4",
        "capabilityId": "C8",
        "signalText": "'Identified and mitigated [X] failure modes proactively — prevented [estimated Y] incidents'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Failure Mode Analysis"
      },
      {
        "id": "SIG-035",
        "observableId": "C8-O4",
        "capabilityId": "C8",
        "signalText": "Cross-org risk awareness: 'Mapped [X] critical dependencies, implemented fallbacks — survived [upstream team]'s outage with zero customer impact'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Dependency Risk Assessment"
      }
    ]
  },
  {
    "id": "C9-O1",
    "capabilityId": "C9",
    "shortText": "Tracks DORA metrics with trend analysis and targeted bottleneck removal",
    "slug": "tracks-dora-metrics-with-trend-analysis-and-targeted-bottleneck-removal",
    "fullExample": "Instruments CI/CD pipeline for all 4 DORA metrics; identifies review bottleneck as longest stage; reduces lead time from 5 days to 8 hours by implementing review SLA.",
    "evidenceTypes": [
      "metric",
      "dashboard"
    ],
    "defaultWeight": 0.192,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team metrics; Directors benchmark across teams",
    "why": "Without measurement, delivery problems are invisible and improvement is guesswork. DORA research proved that deployment frequency, lead time, change failure rate, and MTTR predict both engineering effectiveness and business outcomes — these are diagnostic instruments, not vanity metrics.",
    "how": "Instrument every pipeline stage for all 4 DORA metrics, build team/service dashboards with trend lines, decompose lead time into stages to pinpoint bottlenecks, set improvement targets per quarter — modeled after Google's DORA framework and validated by Stripe's developer productivity research. Validate with metric 1.2 (Lead Time for Changes) and 1.4 (Change Failure Rate).",
    "expectedResult": "Within 1 quarter: visibility into delivery cadence with weekly trend data; targeted friction removal based on stage-level decomposition; measurable improvement in at least one DORA metric.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-012",
        "observableId": "C9-O1",
        "capabilityId": "C9",
        "signalText": "DORA elite: multiple deploys/day. EMs expected to know their team's numbers and trend direction",
        "signalType": "calibration_language",
        "sourceSubTopic": "Deployment Frequency"
      },
      {
        "id": "SIG-013",
        "observableId": "C9-O1",
        "capabilityId": "C9",
        "signalText": "Calibration evidence: 'Reduced lead time from 5 days to 8 hours by [specific action: eliminated manual QA gate / parallelized tests / etc.]'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Lead Time for Changes"
      },
      {
        "id": "SIG-014",
        "observableId": "C9-O1",
        "capabilityId": "C9",
        "signalText": "High CFR despite fast deploys = shipping garbage fast. Low CFR + low frequency = over-cautious. EMs own this balance",
        "signalType": "calibration_language",
        "sourceSubTopic": "Change Failure Rate"
      },
      {
        "id": "SIG-015",
        "observableId": "C9-O1",
        "capabilityId": "C9",
        "signalText": "Promo packet: 'Reduced MTTR from 4hrs to 20min through [automated rollback / improved observability / incident response training]'",
        "signalType": "promo_packet",
        "sourceSubTopic": "Mean Time to Recovery (MTTR)"
      }
    ]
  },
  {
    "id": "C9-O2",
    "capabilityId": "C9",
    "shortText": "Measures developer satisfaction and acts on friction signals before they become attrition",
    "slug": "measures-developer-satisfaction-and-acts-on-friction-signals-before-they-become-attrition",
    "fullExample": "Quarterly 5-question developer experience survey; identifies tooling frustration as top theme; invests in fix; next quarter satisfaction improves and attrition risk decreases.",
    "evidenceTypes": [
      "survey_data",
      "metric"
    ],
    "defaultWeight": 0.114,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs act on team signals; Directors track trends across org",
    "why": "Satisfaction problems that go unmeasured become attrition problems that are expensive to fix. Developer satisfaction is a leading indicator of productivity — not a lagging feel-good metric. The SPACE framework established that satisfaction predicts both retention and output quality.",
    "how": "Run quarterly DX survey (5 questions max, SPACE-aligned), cross-reference with pulse checks in 1:1s, track trends quarter over quarter, invest in the top 2 friction points each quarter with measurable improvement targets. Validate with metric 5.2 (Developer Toil Hours) and 5.5 (Cognitive Load Index).",
    "expectedResult": "Within 2 quarters: early warning on retention risk with 1-2 quarter lead time; targeted DX investment with measurable satisfaction improvement; developer satisfaction scores trending above 4.0/5.0.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-016",
        "observableId": "C9-O2",
        "capabilityId": "C9",
        "signalText": "Directors track satisfaction trends across teams; EMs act on individual signals. Attrition spike after ignoring survey results = leadership failure",
        "signalType": "calibration_language",
        "sourceSubTopic": "Satisfaction & Well-being"
      },
      {
        "id": "SIG-180",
        "observableId": "C9-O2",
        "capabilityId": "C9",
        "signalText": "Observed in team meetings: EM references DORA metrics and developer satisfaction data naturally in planning discussions, uses data to justify engineering investments rather than relying on anecdotes or gut feel.",
        "signalType": "manager_observation",
        "sourceSubTopic": "Data Literacy in Practice"
      }
    ]
  },
  {
    "id": "C9-O3",
    "capabilityId": "C9",
    "shortText": "Uses activity metrics as diagnostic signals, never as performance measures",
    "slug": "uses-activity-metrics-as-diagnostic-signals-never-as-performance-measures",
    "fullExample": "Dashboards show team-level patterns (PR throughput, review turnaround) to spot blocked teams; explicitly bans individual-level activity metrics from perf evaluation.",
    "evidenceTypes": [
      "metric",
      "doc"
    ],
    "defaultWeight": 0.075,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels; Directors set the policy, EMs follow it",
    "why": "Activity metrics used as performance measures incentivize gaming and destroy trust. The purpose of measurement is to identify systemic blockers, not to rank individual engineers by commit count or lines of code.",
    "how": "Display team-level patterns only (PR throughput, review turnaround, build times) on a shared team dashboard (Datadog, Grafana, or LinearB) updated daily. Pair every metric with context: show trendlines, not snapshots; annotate with known events (holidays, incidents, reorgs). Use weekly to spot blocked teams (e.g., PR review turnaround >48 hours signals capacity issue) and uneven distribution (one engineer handling >40% of reviews). Explicitly ban individual-level activity metrics (commit counts, lines of code, PR volume) from performance evaluations — document this policy in the team handbook and reinforce in calibration prep. When a pattern anomaly is detected, investigate as a diagnostic signal (is the team blocked? under-resourced? over-committed?) — never as a performance judgment. Review dashboard effectiveness quarterly: ask 'what decision did this metric inform this quarter?' and retire metrics that do not drive action. Validate with metric 5.4 (Code Review Turnaround Time) and 1.2 (Lead Time for Changes).",
    "expectedResult": "Within 1 quarter: blocked teams identified early; no surveillance culture; developer survey confirms metrics are seen as diagnostic tools, not punishment; zero complaints about metrics being used punitively.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-018",
        "observableId": "C9-O3",
        "capabilityId": "C9",
        "signalText": "NEVER use as perf eval input directly. EMs who cite commit counts in reviews lose credibility instantly",
        "signalType": "calibration_language",
        "sourceSubTopic": "Activity Metrics (Used Carefully)"
      }
    ]
  },
  {
    "id": "C9-O4",
    "capabilityId": "C9",
    "shortText": "Defines outcome-oriented OKRs with measurable key results scored quarterly",
    "slug": "defines-outcome-oriented-okrs-with-measurable-key-results-scored-quarterly",
    "fullExample": "Max 3 objectives per team per quarter; all key results measurable; includes 1 stretch; bi-weekly check-in; quarterly scoring drives real priority decisions.",
    "evidenceTypes": [
      "doc",
      "metric"
    ],
    "defaultWeight": 0.152,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team OKRs; Directors ensure coherence across org",
    "why": "Vague OKRs don't drive behavior; too many OKRs mean none get focus. OKRs scored quarterly with brutal honesty — a team hitting 100% wasn't stretching enough; a team that can't articulate their top 3 objectives has a prioritization problem.",
    "how": "Cap at 3 objectives per team per quarter, require every key result to have a measurable threshold and data source defined at OKR creation time, include 1 stretch goal, run bi-weekly check-ins, score quarterly with real consequences for priority decisions. Validate with metric 8.6 (Tech Debt Ratio).",
    "expectedResult": "Within 1 quarter: team focused on outcomes not output; progress measurable weekly; OKRs influence daily prioritization decisions; scoring average between 0.6-0.8 indicating appropriate stretch.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-103",
        "observableId": "C9-O4",
        "capabilityId": "C9",
        "signalText": "Calibration: 'Team achieved [X]% of OKRs, including stretch goal [Y], driving [Z] business outcome'",
        "signalType": "metric",
        "sourceSubTopic": "Effective OKRs"
      }
    ]
  },
  {
    "id": "C9-O5",
    "capabilityId": "C9",
    "shortText": "Translates engineering outcomes into business metrics leadership values",
    "slug": "translates-engineering-outcomes-into-business-metrics-leadership-values",
    "fullExample": "Maps latency reduction to conversion rate lift, framing infrastructure investment as '$2M ARR impact' rather than 'p99 improvement.'",
    "evidenceTypes": [
      "doc",
      "metric"
    ],
    "defaultWeight": 0.152,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "Directors frame for VP audiences; EMs develop this skill",
    "why": "Engineering work described only in technical terms is invisible to business leadership. Framing a latency improvement as a conversion lift is the difference between 'nice to have' and 'funded priority.'",
    "how": "Map every technical improvement to a business metric before presenting upward: latency to conversion, reliability to retention, velocity to time-to-market; build a standing translation table for your domain. Validate with metric 10.1 (Cloud Cost per Transaction/User) and 10.2 (Infrastructure Utilization Rate).",
    "expectedResult": "Within 2 quarters: leadership approves >80% of engineering investment proposals because they include clear ROI; engineering is invited to business planning conversations.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-105",
        "observableId": "C9-O5",
        "capabilityId": "C9",
        "signalText": "The highest-leverage framing skill for EM/Director: 'Reduced p99 latency 40%, driving estimated 2% conversion lift worth $Xm ARR'",
        "signalType": "metric",
        "sourceSubTopic": "Measuring Engineering Impact"
      }
    ]
  },
  {
    "id": "C9-O6",
    "capabilityId": "C9",
    "shortText": "Classifies attrition and identifies systemic retention patterns",
    "slug": "classifies-attrition-and-identifies-systemic-retention-patterns",
    "fullExample": "Categorizes each departure as regrettable vs non-regrettable; identifies that 3 of 4 regrettable departures cited lack of growth; implements fix.",
    "evidenceTypes": [
      "metric",
      "doc"
    ],
    "defaultWeight": 0.114,
    "requiredFrequency": "quarterly",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "Directors own org-level analysis; EMs provide team context",
    "why": "Without classification, all attrition looks the same and systemic issues stay hidden. Distinguishing regrettable from non-regrettable attrition and tracking root causes separates random departures from patterns requiring intervention.",
    "how": "Classify every departure within 1 week of notice using a structured framework: regrettable (would rehire, tried to retain) vs. non-regrettable (performance-managed, mutual fit issue), documented in an attrition tracker spreadsheet or HR system. Track regrettable attrition rate by team and quarter on a rolling 12-month basis — target below industry benchmark (typically <10% annually for engineering). Conduct structured exit interviews using a consistent 10-question template within the final week, with data aggregated quarterly. Perform quarterly pattern analysis (30-minute review with HR business partner): identify top 3 departure reasons, flag emerging trends (e.g., 'growth cited by 3 of 4 regrettable departures'), and create specific action items. Present attrition analysis in quarterly business reviews with retention interventions and their measured impact. Validate with metric 7.1 (Regrettable Attrition Rate) and 7.2 (Non-Regrettable Attrition Rate).",
    "expectedResult": "Within 2 quarters: systemic retention issues identified with data; regrettable attrition below industry benchmark; exit interview themes addressed proactively.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-053",
        "observableId": "C9-O6",
        "capabilityId": "C9",
        "signalText": "Directors own org-level attrition analysis. 'Identified [pattern] driving regrettable attrition, addressed through [action], reduced rate from X% to Y%'",
        "signalType": "metric",
        "sourceSubTopic": "Attrition Analysis"
      }
    ]
  },
  {
    "id": "C10-O1",
    "capabilityId": "C10",
    "shortText": "Builds headcount cases with capacity model, impact projection, and ROI",
    "slug": "builds-headcount-cases-with-capacity-model-impact-projection-and-roi",
    "fullExample": "Secures 4 headcount by demonstrating: current capacity gap, business impact if unfilled, 3x ROI projection within 12 months, prioritized role ranking.",
    "evidenceTypes": [
      "doc",
      "metric"
    ],
    "defaultWeight": 0.211,
    "requiredFrequency": "annual",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs justify team-level; Directors own org headcount story",
    "why": "Headcount requests without data get cut first in budget reviews. At Amazon, every headcount ask is a narrative document with quantified business impact — 'trust me, I need people' doesn't survive an S-team review.",
    "how": "Build capacity model in a spreadsheet or workforce planning tool (e.g., Runn, Forecast) showing current utilization gap vs. planned work, then project impact-if-unfilled in revenue terms using Cost of Delay analysis. Construct ROI timeline with explicit payback period (target <12 months), priority-rank each role against strategic pillars with a weighted scoring matrix, and present a phased hiring plan with quarterly go/no-go checkpoints tied to business milestones. Review and update the model annually during planning and quarterly during budget reviews. Validate with metric 10.3 (Cost per Engineer) and 8.4 (Engineering Investment Mix).",
    "expectedResult": "Headcount secured with >80% approval rate; investment aligned with strategy; finance sees engineering as disciplined stewards; zero headcount clawed back mid-cycle due to weak justification.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-124",
        "observableId": "C10-O1",
        "capabilityId": "C10",
        "signalText": "Directors own org-level headcount story; EMs own team-level justification. 'Secured [X] headcount by demonstrating [Y] business impact with [Z] ROI timeline'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Annual Headcount Planning"
      },
      {
        "id": "SIG-125",
        "observableId": "C10-O1",
        "capabilityId": "C10",
        "signalText": "Shows strategic thinking: 'Chose not to backfill [role], redistributed scope, redirected headcount to [higher-priority area]'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Backfill vs New Headcount"
      },
      {
        "id": "SIG-129",
        "observableId": "C10-O1",
        "capabilityId": "C10",
        "signalText": "The framing that earns trust: 'Demonstrated [X]x ROI on [engineering investment], securing continued funding'",
        "signalType": "calibration_language",
        "sourceSubTopic": "ROI Communication for Eng Investment"
      }
    ]
  },
  {
    "id": "C10-O2",
    "capabilityId": "C10",
    "shortText": "Reprioritizes aggressively during cuts, communicating what stops — not just what continues",
    "slug": "reprioritizes-aggressively-during-cuts-communicating-what-stops-not-just-what-continues",
    "fullExample": "During 15% budget cut: stack-ranks all work, pauses bottom 20%, communicates cuts to stakeholders explicitly, maintains delivery on top-3 priorities.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.17,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels; Directors manage across teams",
    "why": "Trying to do everything with fewer resources guarantees failure across the board. At Netflix, 'highly aligned, loosely coupled' teams survive cuts because leaders can ruthlessly re-scope without waiting for top-down direction.",
    "how": "Stack-rank all active work within 48 hours of cut announcement using a weighted impact/effort matrix in a shared spreadsheet or project tracking tool (Jira, Asana). Identify the bottom 20% to pause, draft named stakeholder communication for each paused initiative explaining the trade-off, and publish an explicit 'stop doing' list visible to all partners. Ring-fence capacity for top-3 priorities with named owners and weekly progress check-ins. Run a 30-day retrospective to validate that the right items were cut. Validate with metric 8.3 (Cost of Delay) and 8.4 (Engineering Investment Mix).",
    "expectedResult": "Team focuses on highest-impact work; stakeholders understand trade-offs; no surprise misses; top-3 priorities delivered on time despite reduced capacity.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-056",
        "observableId": "C10-O2",
        "capabilityId": "C10",
        "signalText": "'During [X]% budget cut, re-prioritized roadmap, maintained delivery on top-3 priorities, transparently paused [Y]'",
        "signalType": "metric",
        "sourceSubTopic": "Budget Cuts & Doing More With Less"
      }
    ]
  },
  {
    "id": "C10-O3",
    "capabilityId": "C10",
    "shortText": "Manages cloud/infra costs with attribution, targets, and optimization results",
    "slug": "manages-cloudinfra-costs-with-attribution-targets-and-optimization-results",
    "fullExample": "Implements cost attribution per service, monthly cost review, optimization targets; reduces cloud costs 25% ($500K annual savings) through right-sizing and reserved instances.",
    "evidenceTypes": [
      "metric",
      "dashboard"
    ],
    "defaultWeight": 0.127,
    "requiredFrequency": "continuous",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs own team costs; Directors own org-level FinOps",
    "why": "Untracked cloud costs spiral; surprise budget overruns damage credibility. At Amazon, FinOps culture means every team owns their cost curve — cloud spend is reviewed with the same rigor as feature delivery.",
    "how": "Tag every resource to a team and service, run monthly cost review with anomaly detection, set per-team optimization targets with quarterly accountability, adopt FinOps practices — modeled after Amazon's cost attribution model where teams own unit economics for their services. Validate with metric 10.5 (AI Tooling ROI).",
    "expectedResult": "Costs tracked with per-service attribution; cloud spend reduced 15-25% through right-sizing and reserved instances; no surprise overruns; engineering seen as cost-conscious partners to finance.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-127",
        "observableId": "C10-O3",
        "capabilityId": "C10",
        "signalText": "Increasingly important at Big Tech. 'Reduced cloud costs X% ($Ym annual savings) through [specific optimizations]'",
        "signalType": "metric",
        "sourceSubTopic": "Cloud/Infra Cost Ownership"
      },
      {
        "id": "SIG-128",
        "observableId": "C10-O3",
        "capabilityId": "C10",
        "signalText": "Quantitative rigor: 'TCO analysis demonstrated buying [X] saves $Ym over 3 years vs. building, including [hidden costs]'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Build vs Buy Cost Analysis"
      }
    ]
  },
  {
    "id": "C10-O4",
    "capabilityId": "C10",
    "shortText": "Optimizes team composition with principled FTE/contractor/vendor mix",
    "slug": "optimizes-team-composition-with-principled-ftecontractorvendor-mix",
    "fullExample": "Evaluates work types: core differentiating work→FTE, commodity/surge work→contractor, specialized work→vendor. Saves 20% while maintaining velocity.",
    "evidenceTypes": [
      "doc",
      "metric"
    ],
    "defaultWeight": 0.127,
    "requiredFrequency": "annual",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "Directors manage blended team models; EMs provide input on work classification",
    "why": "Ad hoc staffing decisions lead to either over-spending on FTEs for commodity work or IP risk from over-contracting. Google's workforce planning model uses a principled framework: FTEs for core IP and long-term investment, contractors for well-scoped time-bound work, and vendors for commodity services.",
    "how": "Classify all active workstreams into categories (core differentiating, commodity/surge, specialized) using a decision matrix. Match each category to the optimal staffing model: FTE for core IP-sensitive work, contractors for surge/commodity work, vendors for specialized niche skills. Maintain a staffing composition dashboard reviewed quarterly with finance, tracking cost-per-output by worker type. Ensure IP and compliance guardrails are in place (code access policies, NDA enforcement, vendor security reviews). Run quarterly quality audits comparing output metrics (defect rates, velocity, code review feedback) across worker types. Validate with metric 10.3 (Cost per Engineer) and 8.4 (Engineering Investment Mix).",
    "expectedResult": "Cost-effective staffing maintaining quality; no IP or compliance surprises. Contractor-to-FTE ratio reviewed quarterly; all contractors have documented scope and IP agreements; cost per deliverable within 10% of FTE equivalent.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-126",
        "observableId": "C10-O4",
        "capabilityId": "C10",
        "signalText": "Directors manage blended team models. 'Optimized team composition: [X]% FTE / [Y]% contractor, saving [Z] while maintaining delivery velocity'",
        "signalType": "metric",
        "sourceSubTopic": "Contractor & Vendor Strategy"
      }
    ]
  },
  {
    "id": "C10-O5",
    "capabilityId": "C10",
    "shortText": "Defends critical investments during cuts by articulating business consequences",
    "slug": "defends-critical-investments-during-cuts-by-articulating-business-consequences",
    "fullExample": "Protects platform team during 20% budget cut by demonstrating: cutting team means 5 product teams lose deploy capability, impacting $10M revenue. Proposes alternative savings instead.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.17,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "Directors defend at VP level; EMs prepare the impact analysis",
    "why": "Leaders who can't articulate cut consequences in business terms lose their most important investments. At Netflix, leaders who defend investments by quantifying the blast radius of cuts — not by pleading — earn continued funding even in lean cycles.",
    "how": "Prepare a consequence analysis document for each possible cut, quantifying downstream impact in revenue terms (e.g., '$X revenue at risk if platform team is cut because Y product teams lose deploy capability'). Rank cuts by blast radius using a dependency map showing how many teams, services, or customers are affected. Propose alternative savings with equivalent dollar value and lower blast radius. Present the analysis in a 1-page decision brief using business terms executives understand: revenue risk, customer impact, competitive position, and time-to-recover if the cut is reversed. Deliver within 72 hours of cut announcement. Validate with metric 8.3 (Cost of Delay) and 8.2 (Revenue / Business Impact Attribution).",
    "expectedResult": "Critical investments preserved; cuts fall on lower-priority items; leadership sees you as a responsible steward who protects the business, not just your headcount.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-130",
        "observableId": "C10-O5",
        "capabilityId": "C10",
        "signalText": "'Defended [critical investment] during [X]% budget cut by demonstrating [business impact], while proposing [Y] alternative savings'",
        "signalType": "metric",
        "sourceSubTopic": "Budget Defense During Cuts"
      }
    ]
  },
  {
    "id": "C11-O1",
    "capabilityId": "C11",
    "shortText": "Runs calibrated hiring loops with trained interviewers and consistent bar",
    "slug": "runs-calibrated-hiring-loops-with-trained-interviewers-and-consistent-bar",
    "fullExample": "Defines competency matrix per role/level, trains interviewers through shadow cycle, enforces independent write-ups before group debrief, maintains bar raiser veto.",
    "evidenceTypes": [
      "hiring_guide",
      "interview_scorecard"
    ],
    "defaultWeight": 0.19,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team hiring quality; Directors standardize across org",
    "why": "Uncalibrated interviewers produce noisy signal; inconsistent bar creates quality variance. At Google, hiring decisions go through an independent hiring committee — the interviewing team doesn't make the call alone, which forces signal quality to be high enough to survive external review.",
    "how": "Define competency matrix per role/level in a shared rubric document (Google Doc or Notion), updated annually per role family. Run interviewer training pipeline: shadow 2 loops, reverse-shadow 1 loop, solo with feedback review. Enforce independent write-ups submitted before group debrief to prevent anchoring bias. Assign a bar raiser (trained outsider) with veto power to every loop — modeled after Amazon's bar raiser program. Track interviewer signal quality quarterly: measure agreement rate (target >80%), false-positive rate, and candidate NPS. Recalibrate or remove interviewers whose signal consistently diverges. Validate with metric 7.3 (Time to Hire) and 7.4 (Offer Acceptance Rate).",
    "expectedResult": "Consistent bar across all interviewers; interviewer agreement rate above 80%; reduced bias in evaluation; candidates report positive experience even when rejected.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-065",
        "observableId": "C11-O1",
        "capabilityId": "C11",
        "signalText": "Big Tech hiring is committee-based — EM must ensure team's interviews generate strong signal. 'Trained [X] calibrated interviewers, interview-to-offer conversion improved Y%'",
        "signalType": "metric",
        "sourceSubTopic": "Calibrated Hiring Loops"
      },
      {
        "id": "SIG-068",
        "observableId": "C11-O1",
        "capabilityId": "C11",
        "signalText": "Director-level skill: 'Secured [X] headcount by demonstrating [Y] revenue impact / [Z] risk if unfilled'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Headcount Justification"
      }
    ]
  },
  {
    "id": "C11-O2",
    "capabilityId": "C11",
    "shortText": "Maintains hiring bar under pressure — no warm body hires",
    "slug": "maintains-hiring-bar-under-pressure-no-warm-body-hires",
    "fullExample": "Despite 4 open reqs for 6 months, passes on borderline candidates; eventual hires all rated 'strong hire' with zero regrettable offers.",
    "evidenceTypes": [
      "interview_scorecard",
      "metric"
    ],
    "defaultWeight": 0.152,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels; Directors hold EMs accountable to bar",
    "why": "Filling reqs with warm bodies creates years of management overhead and drags team performance. At Amazon, the bar raiser program exists specifically to prevent hiring managers from lowering standards under pressure — an independent evaluator can veto any hire.",
    "how": "Assign bar raiser veto power to a trained interviewer outside the hiring team for every loop. Define explicit no-hire criteria in the role scorecard before the loop starts, and share with all interviewers in a pre-loop calibration session (15-min sync on level expectations and must-have signals). Refuse to extend offers on split decisions without escalation to hiring committee or skip-level. Track quality-of-hire signals at 90 days (manager satisfaction, ramp velocity, first-quarter output) and correlate back to interview scores quarterly to validate hiring bar accuracy. Target: zero regrettable offers per year, 95%+ 90-day retention. Validate with metric 7.3 (Time to Hire) and 7.4 (Offer Acceptance Rate).",
    "expectedResult": "Every hire raises team capability; zero regrettable offers per year; team trusts the process; 90-day retention above 95%.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-066",
        "observableId": "C11-O2",
        "capabilityId": "C11",
        "signalText": "'Maintained hiring bar despite [X] open reqs for [Y] months, resulting in [Z] strong hires with zero regrettable offers'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Bar Raiser Discipline"
      }
    ]
  },
  {
    "id": "C11-O3",
    "capabilityId": "C11",
    "shortText": "Designs 30/60/90 onboarding with measurable ramp milestones",
    "slug": "designs-306090-onboarding-with-measurable-ramp-milestones",
    "fullExample": "Week 1: first commit. Month 1: buddy-guided starter project. Month 2: independent feature delivery. Month 3: full autonomy. Ramp time reduced from 90 to 45 days.",
    "evidenceTypes": [
      "doc",
      "metric"
    ],
    "defaultWeight": 0.152,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own team onboarding; Directors standardize org-level program",
    "why": "Unstructured onboarding means new hires float for months, creating frustration and slow ramp. At Meta, Bootcamp immerses every new engineer in a structured multi-week program before team placement — the principle is that intentional onboarding compounds into years of higher productivity.",
    "how": "Build a 30/60/90 plan template in Notion or Confluence with measurable milestones: first commit by day 5, buddy-guided starter project shipped by day 30, independent feature delivered by day 60, full autonomous contribution by day 90. Assign a buddy from day one (not the manager) for daily questions and pairing. Pair with a mentor for career context and team culture orientation. Run structured feedback checkpoints at 30/60/90 days using a standard assessment form with explicit 'on track / needs support / at risk' ratings, shared with the new hire. Track time-to-first-PR as a leading indicator and update the onboarding plan based on cohort feedback each quarter. Validate with metric 7.5 (New Hire Ramp Time).",
    "expectedResult": "New hires productive within 60 days; time-to-first-PR under 5 business days; early identification of ramp issues; new hires report feeling supported in onboarding survey (>4.0/5.0).",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-069",
        "observableId": "C11-O3",
        "capabilityId": "C11",
        "signalText": "Time-to-productivity is a team effectiveness metric. 'Structured onboarding reduced ramp time from 90 to 45 days average'",
        "signalType": "metric",
        "sourceSubTopic": "Structured Onboarding"
      },
      {
        "id": "SIG-070",
        "observableId": "C11-O3",
        "capabilityId": "C11",
        "signalText": "Investment in people that calibration notices: 'Buddy program reduced new hire Time-to-first-PR from 2 weeks to 3 days'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Mentorship & Buddy Programs"
      },
      {
        "id": "SIG-181",
        "observableId": "C11-O3",
        "capabilityId": "C11",
        "signalText": "Observed in interview debriefs: EM calibrates interviewers on evidence quality, pushes back on vague 'culture fit' signals, ensures structured scoring rubric is applied consistently across candidates.",
        "signalType": "manager_observation",
        "sourceSubTopic": "Interview Process Rigor"
      }
    ]
  },
  {
    "id": "C11-O4",
    "capabilityId": "C11",
    "shortText": "Collects and acts on onboarding feedback, improving program each cohort",
    "slug": "collects-and-acts-on-onboarding-feedback-improving-program-each-cohort",
    "fullExample": "30/60/90 surveys with 5 questions; identifies that documentation gaps slow ramp; fixes docs; next cohort ramps 2 weeks faster.",
    "evidenceTypes": [
      "survey_data",
      "doc"
    ],
    "defaultWeight": 0.076,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs collect and act; Directors track org-level trends",
    "why": "Static onboarding programs calcify; each cohort hits the same friction that could have been fixed. At Meta, Bootcamp iterates continuously based on cohort feedback — every friction point discovered gets fixed before the next cohort arrives.",
    "how": "Run 30/60/90 surveys with 5 targeted questions (e.g., 'What slowed you down most?', 'What was missing from documentation?', 'Rate your buddy experience 1-5') using Google Forms or Lattice. Hold a 30-minute onboarding retrospective with each cohort after their 90-day mark. Track time-to-first-PR trend as a leading metric on a team dashboard, targeting improvement each quarter. Fix the top friction point identified before the next hire starts — maintain a running onboarding improvement backlog. Iterate the program each cohort and share improvements with the team to demonstrate investment in the process. Validate with metric 7.5 (New Hire Ramp Time) and 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Measurable improvement each cohort: ramp time decreasing quarter-over-quarter; onboarding satisfaction above 4.0/5.0; documentation gaps identified and closed within one cohort cycle.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-071",
        "observableId": "C11-O4",
        "capabilityId": "C11",
        "signalText": "'Reduced ramp time from X to Y weeks through systematic onboarding iteration based on new hire feedback'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Onboarding Feedback Loops"
      }
    ]
  },
  {
    "id": "C11-O5",
    "capabilityId": "C11",
    "shortText": "Builds continuous sourcing pipeline through referrals and technical brand",
    "slug": "builds-continuous-sourcing-pipeline-through-referrals-and-technical-brand",
    "fullExample": "Creates team referral culture, publishes engineering blog posts showcasing team's work, supports conference talks; fills roles through referrals in avg 30 days.",
    "evidenceTypes": [
      "metric",
      "doc"
    ],
    "defaultWeight": 0.113,
    "requiredFrequency": "continuous",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs build team-level brand; Directors own org-level talent brand",
    "why": "Reactive hiring (post req, wait for recruiter) is slow and produces worse candidates. At Google, top engineering managers maintain a warm candidate pipeline year-round — when a req opens, they already have 3-5 strong candidates in conversation.",
    "how": "Set quarterly referral goals (e.g., 2 qualified referrals per engineer per quarter) and track in team meetings. Publish engineering blog posts showcasing team work at least monthly using the company engineering blog or Medium. Support 1-2 conference talks per quarter from team members, sponsoring abstract submissions and rehearsal time. Maintain open source contributions that showcase technical depth. Track pipeline source mix (referral vs. inbound vs. recruiter) monthly and aim for 40%+ referral-sourced candidates. Build a diverse pipeline by partnering with organizations focused on underrepresented groups in tech. Validate with metric 7.3 (Time to Hire) and 7.7 (Diversity Metrics).",
    "expectedResult": "Positions filled faster with higher quality; diverse pipeline; less recruiter dependence.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-067",
        "observableId": "C11-O5",
        "capabilityId": "C11",
        "signalText": "Directors own org-level talent brand; EMs own team-level referral networks. 'Built pipeline that filled [X] roles through referrals in [Y] average days'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Sourcing & Pipeline Building"
      },
      {
        "id": "SIG-072",
        "observableId": "C11-O5",
        "capabilityId": "C11",
        "signalText": "Eng brand creates recruiting leverage. 'Team blog posts generated [X] inbound candidate inquiries and [Y] hires'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Engineering Blog Strategy"
      },
      {
        "id": "SIG-073",
        "observableId": "C11-O5",
        "capabilityId": "C11",
        "signalText": "Director-level: 'Built eng brand through [X] conference talks and [Y] open source projects, contributing to [Z] senior hires'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Conference & Open Source Presence"
      }
    ]
  },
  {
    "id": "C11-O6",
    "capabilityId": "C11",
    "shortText": "Articulates clear team value proposition driving higher offer acceptance",
    "slug": "articulates-clear-team-value-proposition-driving-higher-offer-acceptance",
    "fullExample": "Defines and maintains: what problems we solve, what scale we operate at, what you'll learn, career path, culture. Offer acceptance rate improves from 60% to 80%.",
    "evidenceTypes": [
      "doc",
      "metric"
    ],
    "defaultWeight": 0.076,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own team-level EVP; Directors own org-level narrative",
    "why": "Without a clear value proposition, you lose candidates to teams that tell a better story. At Stripe, every hiring manager crafts a team-specific pitch that addresses what candidates will learn, ship, and own — generic 'great culture' pitches don't differentiate.",
    "how": "Define a team Employee Value Proposition (EVP) document covering: what problems we solve, what scale we operate at, what you will learn in 12 months, career growth path, team culture, and tech stack. Store in Notion or a shared doc and update quarterly or when team scope changes. Use the EVP in recruiter briefings, sourcing outreach, and candidate closing conversations. Train all interviewers to articulate the EVP consistently. Track offer acceptance rate monthly (target >75%) and run brief post-decline surveys to identify EVP gaps. Validate with metric 7.4 (Offer Acceptance Rate) and 7.3 (Time to Hire).",
    "expectedResult": "Stronger pipeline; higher acceptance; lower attrition because expectations set honestly.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-074",
        "observableId": "C11-O6",
        "capabilityId": "C11",
        "signalText": "'Articulated team value proposition, increased offer acceptance rate from X% to Y%'",
        "signalType": "metric",
        "sourceSubTopic": "Why Engineers Want YOUR Team"
      }
    ]
  },
  {
    "id": "C12-O1",
    "capabilityId": "C12",
    "shortText": "Models psychological safety through personal vulnerability and constructive response to bad news",
    "slug": "models-psychological-safety-through-personal-vulnerability-and-constructive-response-to-bad-news",
    "fullExample": "Shares own mistakes publicly; responds to production incident disclosure with 'tell me more' not 'how did this happen'; team health survey shows safety scores above org average.",
    "evidenceTypes": [
      "survey_data",
      "meeting_note"
    ],
    "defaultWeight": 0.251,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs build for team; Directors ensure culture scales across org",
    "why": "Psychological safety is the #1 predictor of team effectiveness (Google Project Aristotle). Without it, problems stay hidden until they become crises, knowledge hoarding replaces collaboration, and the team's best ideas never surface because speaking up feels unsafe.",
    "how": "Model vulnerability by sharing your own mistakes publicly, respond to bad news with 'tell me more' not 'how did this happen', celebrate learning from failure in team retros, measure safety quarterly in surveys — modeled after Google's Project Aristotle findings where psychological safety was the single strongest differentiator of high-performing teams. Validate with metric 7.1 (Regrettable Attrition Rate) and 7.9 (Burnout Risk Index).",
    "expectedResult": "Issues surface within hours not weeks; team psychological safety survey scores above org 75th percentile; smart risks taken without fear; candid feedback flows bidirectionally.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-043",
        "observableId": "C12-O1",
        "capabilityId": "C12",
        "signalText": "Google's Project Aristotle: #1 predictor of team effectiveness. Measured through skip-level feedback and team health surveys",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Building Psychological Safety"
      },
      {
        "id": "SIG-044",
        "observableId": "C12-O1",
        "capabilityId": "C12",
        "signalText": "EMs who avoid conflict are a red flag in calibration. 'Facilitated resolution of [technical/process disagreement], resulting in [better outcome]'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Constructive Conflict"
      },
      {
        "id": "SIG-091",
        "observableId": "C12-O1",
        "capabilityId": "C12",
        "signalText": "Measurable through team health surveys and skip-level feedback. Gap between 'we have psychological safety' and 'people actually feel safe' is visible",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Psychological Safety in Practice"
      }
    ]
  },
  {
    "id": "C12-O2",
    "capabilityId": "C12",
    "shortText": "Establishes team charter and engineering principles that guide daily decisions",
    "slug": "establishes-team-charter-and-engineering-principles-that-guide-daily-decisions",
    "fullExample": "Collaboratively drafts team charter covering values, communication norms, and decision process. Creates 3-5 engineering principles referenced in design reviews.",
    "evidenceTypes": [
      "doc"
    ],
    "defaultWeight": 0.151,
    "requiredFrequency": "annual",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs establish for team; Directors align principles across org",
    "why": "Without explicit norms, culture is whatever happens; principles without documentation aren't principles. At Netflix, the Freedom and Responsibility culture doc became the operating system for every team decision — written norms scale culture in ways hallway conversations never can. At Stripe, writing culture means decisions and principles are documented by default.",
    "how": "Draft the team charter collaboratively in a dedicated workshop (90-minute session, not EM-dictated), covering: team mission, communication norms (response time expectations, async-first vs. sync preferences), decision-making process (RACI or DRI model), and disagreement escalation paths. Document in Notion or Confluence and link from team README. Define 3-5 engineering principles (e.g., 'Ship small, learn fast', 'Leave code better than you found it') and reference them explicitly in design reviews, retros, and code review comments. Review and update the charter annually or when team composition changes by >30%. Modeled after Stripe's writing culture where documented principles are living references, not shelf-ware. Validate with metric 7.8 (eNPS / Engagement Score) and 5.1 (Developer Satisfaction Score).",
    "expectedResult": "New hires understand team operating model within first week; charter referenced in at least 2 decision discussions per sprint; existing members have shared expectations to invoke during disagreements.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-086",
        "observableId": "C12-O2",
        "capabilityId": "C12",
        "signalText": "Intentional culture-building signal: 'Established team charter adopted by [X] engineers, referenced in onboarding and retros'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Team Charter & Working Agreement"
      },
      {
        "id": "SIG-087",
        "observableId": "C12-O2",
        "capabilityId": "C12",
        "signalText": "Org-level influence: 'Established engineering principles adopted by [X] teams across org'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Engineering Principles"
      }
    ]
  },
  {
    "id": "C12-O3",
    "capabilityId": "C12",
    "shortText": "Institutionalizes knowledge sharing that reduces bus factor and accelerates onboarding",
    "slug": "institutionalizes-knowledge-sharing-that-reduces-bus-factor-and-accelerates-onboarding",
    "fullExample": "Implements bi-weekly tech talks, documentation standards (every system has README + runbook + design doc), design doc culture. Documentation coverage goes from 40% to 90%.",
    "evidenceTypes": [
      "doc",
      "metric"
    ],
    "defaultWeight": 0.151,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs drive team practices; Directors standardize across org",
    "why": "Knowledge locked in individuals creates single-point-of-failure risk and slow onboarding. At Stripe, writing culture means every significant decision, architecture choice, and operational procedure is documented — knowledge sharing isn't a nice-to-have, it's how the company operates.",
    "how": "Run bi-weekly tech talks, enforce documentation standards (every system has README + runbook + design doc), build design doc culture where writing precedes building, track documentation coverage as a team metric. Validate with metric 7.5 (New Hire Ramp Time).",
    "expectedResult": "Bus factor above 2 for every critical system; onboarding time reduced 30%+; documentation coverage above 85%; team member vacation doesn't create on-call emergencies.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-088",
        "observableId": "C12-O3",
        "capabilityId": "C12",
        "signalText": "Culture investment that compounds: 'Documentation coverage from X% to Y%, onboarding time reduced by Z%'",
        "signalType": "metric",
        "sourceSubTopic": "Knowledge Sharing Practices"
      },
      {
        "id": "SIG-182",
        "observableId": "C12-O3",
        "capabilityId": "C12",
        "signalText": "Observed in team meetings: diverse voices participate actively, EM facilitates equitably (draws out quieter members, time-boxes dominant speakers), async decision-making options provided for different working styles.",
        "signalType": "manager_observation",
        "sourceSubTopic": "Inclusive Team Dynamics"
      }
    ]
  },
  {
    "id": "C12-O4",
    "capabilityId": "C12",
    "shortText": "Implements recognition rituals that reinforce desired behaviors",
    "slug": "implements-recognition-rituals-that-reinforce-desired-behaviors",
    "fullExample": "Weekly shout-outs in team sync tied to engineering principles; peer recognition channel. Recognition is specific ('Sarah's design doc saved 2 weeks of rework') not generic ('good job').",
    "evidenceTypes": [
      "meeting_note",
      "doc"
    ],
    "defaultWeight": 0.101,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs run for team; Directors notice recognition culture quality",
    "why": "Generic recognition is noise; specific recognition aligned to values is culture reinforcement. At Google, peer bonuses require explicit connection to a team value — this trains the whole team to notice and name the behaviors that matter.",
    "how": "Establish a weekly recognition ritual in team sync (2-3 minutes): specific shout-outs tied to team engineering principles (e.g., 'Sarah's design doc saved 2 weeks of rework — this is what Simplify First looks like'). Create a dedicated #team-kudos Slack channel for peer-to-peer recognition, visible to leadership. Ensure recognition is specific (what happened, why it mattered, which principle it exemplifies) not generic ('good job'). Track recognition frequency monthly — aim for every team member receiving at least one specific recognition per sprint. Rotate who gives shout-outs to encourage peer recognition culture, not just top-down praise. Validate with metric 5.1 (Developer Satisfaction Score) and 7.8 (eNPS / Engagement Score).",
    "expectedResult": "Positive behaviors reinforced; culture becomes self-reinforcing; team members feel valued.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-089",
        "observableId": "C12-O4",
        "capabilityId": "C12",
        "signalText": "Low-cost, high-impact culture investment. Directors notice teams with strong recognition culture vs. 'good job everyone' teams",
        "signalType": "calibration_language",
        "sourceSubTopic": "Recognition Rituals"
      }
    ]
  },
  {
    "id": "C12-O5",
    "capabilityId": "C12",
    "shortText": "Drives inclusive practices ensuring equitable opportunity distribution",
    "slug": "drives-inclusive-practices-ensuring-equitable-opportunity-distribution",
    "fullExample": "Uses round-robin facilitation, async participation options, and equitable stretch assignment distribution. Promotion rates equitable across demographics.",
    "evidenceTypes": [
      "doc",
      "metric"
    ],
    "defaultWeight": 0.151,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs practice for team; Directors set org-level inclusion standards",
    "why": "Inclusion must be proactive — absence of exclusion is not the same as belonging. Google's Project Aristotle found that psychological safety (the inclusion proxy) is the single strongest predictor of team performance. Teams that don't actively distribute opportunities see attrition skew toward underrepresented groups.",
    "how": "Implement structured meeting facilitation practices: round-robin for input, written-first proposals before discussion (Slack thread or doc comments), and async participation options for different time zones and working styles. Track stretch assignment and high-visibility opportunity distribution quarterly using a simple spreadsheet — flag if any demographic group receives <20% of stretch opportunities relative to their representation. Rotate on-call, demo presentations, and cross-team liaison roles equitably. Measure inclusion through quarterly pulse surveys with specific questions on belonging and voice (target >4.0/5.0 across all demographic groups). Validate with metric 7.7 (Diversity Metrics) and 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Diverse perspectives contribute; retention across demographics improves; innovation increases.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-090",
        "observableId": "C12-O5",
        "capabilityId": "C12",
        "signalText": "D&I is a team health and innovation driver. 'Equitable distribution of stretch assignments and visibility opportunities across team'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Inclusive Environment"
      }
    ]
  },
  {
    "id": "C13-O1",
    "capabilityId": "C13",
    "shortText": "Embeds security checkpoints in development workflow as automatic gates",
    "slug": "embeds-security-checkpoints-in-development-workflow-as-automatic-gates",
    "fullExample": "Threat modeling during design phase using STRIDE; automated SAST/DAST in CI; security champion rotation; pre-launch security checklist. Zero last-minute security blockers.",
    "evidenceTypes": [
      "doc",
      "pipeline_config"
    ],
    "defaultWeight": 0.223,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs ensure team follows process; Directors drive security culture across org",
    "why": "Security found late is expensive to fix; teams skip reviews under deadline pressure creating org-level risk. At Amazon, mandatory security reviews are gates in the deployment pipeline — you literally cannot ship without passing threat model review, making security non-negotiable rather than aspirational.",
    "how": "Embed threat modeling in design phase using STRIDE, wire automated SAST/DAST scanning into CI as blocking gates, rotate security champion role quarterly, enforce pre-launch security checklist — modeled after Amazon's threat model gates where security review is a pipeline stage, not a meeting request. Validate with metric 9.1 (Vulnerability Remediation Time).",
    "expectedResult": "Security issues caught at design time (10x cheaper than production); zero last-minute launch blockers; security review turnaround under 24 hours; 80%+ of vulnerabilities caught before code review.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-024",
        "observableId": "C13-O1",
        "capabilityId": "C13",
        "signalText": "EMs ensure team follows security processes — this is a compliance signal. Gaps here create VP-level visibility in the wrong way",
        "signalType": "calibration_language",
        "sourceSubTopic": "Security Review Integration"
      },
      {
        "id": "SIG-191",
        "observableId": "C13-O1",
        "capabilityId": "C13",
        "signalText": "Reduced security review bottleneck from 5 days to <1 day by embedding automated SAST/DAST scanning into CI pipeline, catching 85% of issues before human review",
        "signalType": "metric",
        "sourceSubTopic": "Security Review Integration"
      },
      {
        "id": "SIG-192",
        "observableId": "C13-O1",
        "capabilityId": "C13",
        "signalText": "Designed and implemented security-by-default development workflow adopted by 3 teams, reducing post-deploy security findings by 60%",
        "signalType": "promo_packet",
        "sourceSubTopic": "Security Review Integration"
      }
    ]
  },
  {
    "id": "C13-O2",
    "capabilityId": "C13",
    "shortText": "Manages vulnerabilities within severity-based SLAs with tracked patch velocity",
    "slug": "manages-vulnerabilities-within-severity-based-slas-with-tracked-patch-velocity",
    "fullExample": "Automated dependency scanning in CI; Critical: 48hr, High: 1 week, Medium: 1 month. Zero critical CVEs beyond SLA for 6 consecutive quarters.",
    "evidenceTypes": [
      "metric",
      "dashboard"
    ],
    "defaultWeight": 0.224,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team vulnerability hygiene; Directors own org-level posture",
    "why": "Known vulnerabilities sitting unpatched are ticking time bombs; zero-days require rapid systematic response. At Amazon, vulnerability SLAs are tracked as rigorously as operational SLAs — breaching a patching SLA triggers the same escalation path as breaching a customer-facing SLA.",
    "how": "Wire automated dependency scanning into CI, define severity-based SLAs (Critical: 48hr, High: 1 week, Medium: 1 month), track patch velocity on a dashboard, escalate blocked patches within 24 hours, schedule regular dependency upgrades as sprint-level work items. Validate with metric 9.2 (Dependency Currency).",
    "expectedResult": "Zero critical CVEs beyond SLA for 4+ consecutive quarters; clean vulnerability posture at any point-in-time audit; patch velocity dashboard shows P0 under 48 hours consistently.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-025",
        "observableId": "C13-O2",
        "capabilityId": "C13",
        "signalText": "Auditors and security teams track this. EM ownership is non-negotiable — 'Zero critical CVEs beyond SLA for [X] consecutive quarters'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Vulnerability Management"
      },
      {
        "id": "SIG-193",
        "observableId": "C13-O2",
        "capabilityId": "C13",
        "signalText": "Maintained P0 vulnerability patch velocity under 48 hours and P1 under 7 days for 4 consecutive quarters; zero SLA breaches on critical vulnerabilities",
        "signalType": "metric",
        "sourceSubTopic": "Vulnerability Management"
      },
      {
        "id": "SIG-194",
        "observableId": "C13-O2",
        "capabilityId": "C13",
        "signalText": "Treats vulnerability management as an ongoing practice, not a periodic exercise; team consistently patches within SLA without needing escalation",
        "signalType": "manager_observation",
        "sourceSubTopic": "Vulnerability Management"
      }
    ]
  },
  {
    "id": "C13-O3",
    "capabilityId": "C13",
    "shortText": "Enforces least-privilege access with quarterly reviews and auto-expiring elevation",
    "slug": "enforces-least-privilege-access-with-quarterly-reviews-and-auto-expiring-elevation",
    "fullExample": "Quarterly access reviews, role-based access aligned to on-call rotation, temporary elevation for debugging that auto-expires, full audit trail.",
    "evidenceTypes": [
      "doc",
      "audit_log"
    ],
    "defaultWeight": 0.179,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs maintain for team; Directors ensure compliance across org",
    "why": "Over-provisioned access multiplies blast radius of any breach or insider mistake. At Google, BeyondCorp zero-trust architecture assumes the network is hostile — every access request is authenticated and authorized regardless of location, eliminating the 'inside the firewall means trusted' assumption.",
    "how": "Run quarterly access reviews with automated stale-account flagging, enforce role-based access aligned to on-call rotation, implement temporary elevation with auto-expiry for debugging — modeled after Google's BeyondCorp principle where access is granted per-request, not per-network. Validate with metric 9.3 (Compliance Audit Readiness).",
    "expectedResult": "Over-privileged accounts reduced by 70%+; clean audit trail with zero unexplained elevations; compliance-ready access posture at any point-in-time; no access-related security incidents.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-026",
        "observableId": "C13-O3",
        "capabilityId": "C13",
        "signalText": "Compliance requirement at Big Tech — EMs who ignore this create org-level risk and VP-level escalations",
        "signalType": "calibration_language",
        "sourceSubTopic": "Access Control & Least Privilege"
      },
      {
        "id": "SIG-195",
        "observableId": "C13-O3",
        "capabilityId": "C13",
        "signalText": "Their access review process is thorough but not burdensome — they automated the quarterly review and elevated permissions expire automatically, so we rarely have stale access",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Access Control & Least Privilege"
      },
      {
        "id": "SIG-196",
        "observableId": "C13-O3",
        "capabilityId": "C13",
        "signalText": "Implemented automated access governance reducing over-privileged accounts by 70% and eliminating the quarterly manual access review burden for 200+ engineers",
        "signalType": "promo_packet",
        "sourceSubTopic": "Access Control & Least Privilege"
      }
    ]
  },
  {
    "id": "C13-O4",
    "capabilityId": "C13",
    "shortText": "Maintains continuous compliance posture with automated evidence collection",
    "slug": "maintains-continuous-compliance-posture-with-automated-evidence-collection",
    "fullExample": "Automated compliance checks in CI/CD; evidence collection pipelines; quarterly self-audit against controls; compliance dashboard. Clean audit for 4 consecutive cycles with zero findings.",
    "evidenceTypes": [
      "dashboard",
      "doc"
    ],
    "defaultWeight": 0.179,
    "requiredFrequency": "continuous",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs maintain team compliance; Directors own org-level audit readiness",
    "why": "Audits are painful fire drills when compliance is manual; last-minute scrambles damage credibility and pull engineers off roadmap work. Amazon automates compliance evidence collection so that SOC2/ISO audits are a data export, not a 6-week project. Continuous compliance posture means you're audit-ready every day.",
    "how": "Automated checks in CI/CD, evidence collection pipelines, quarterly self-audit, compliance dashboard. Validate with metric 9.4 (AI-Generated Code Security Scan Rate).",
    "expectedResult": "Painless audits; no surprises; credibility with security and compliance teams. Audit prep time <1 week; zero critical findings from external audits; continuous compliance dashboard shows green on >95% of controls.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-029",
        "observableId": "C13-O4",
        "capabilityId": "C13",
        "signalText": "Table-stakes at Big Tech — SOC2, GDPR, internal controls. 'Clean audit for [X] consecutive cycles with zero findings'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Compliance & Audit Readiness"
      },
      {
        "id": "SIG-197",
        "observableId": "C13-O4",
        "capabilityId": "C13",
        "signalText": "Achieved continuous SOC2 compliance with automated evidence collection; audit prep time reduced from 6 weeks to 3 days",
        "signalType": "metric",
        "sourceSubTopic": "Compliance & Audit Readiness"
      },
      {
        "id": "SIG-198",
        "observableId": "C13-O4",
        "capabilityId": "C13",
        "signalText": "Compliance used to be a dreaded annual exercise — now it runs continuously in the background and we barely notice it. That's entirely due to their leadership on automation",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Compliance & Audit Readiness"
      }
    ]
  },
  {
    "id": "C14-O1",
    "capabilityId": "C14",
    "shortText": "Writes evidence-based performance reviews with level-anchored assessment",
    "slug": "writes-evidence-based-performance-reviews-with-level-anchored-assessment",
    "fullExample": "Collects feedback continuously using impact log; uses level rubric language explicitly; reviews are specific ('drove X resulting in Y, demonstrating next-level Z') not vague ('did great work').",
    "evidenceTypes": [
      "doc",
      "note"
    ],
    "defaultWeight": 0.129,
    "requiredFrequency": "biannual",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs write directly; Directors coach on quality and review EM drafts",
    "why": "Vague reviews don't help anyone grow; biased reviews create inequity; weak reviews fail in calibration. Calibration committees scrutinize review quality — managers who write 'did great work' instead of 'drove X resulting in Y, demonstrating next-level Z' get their reviews sent back.",
    "how": "Anchor every assessment to the company's level rubric language with specific examples and quantified impact (e.g., 'Led migration reducing latency 40%, demonstrating Staff-level system design'). Draw from continuous performance notes (see C14-O7) rather than reconstructing from memory. Collect 360 feedback from 3-5 peers and cross-functional partners 2 weeks before review deadline. Write reviews that would survive a calibration committee's scrutiny — where evidence quality determines credibility. Target: 100% of reviews include at least 3 specific examples with quantified impact, zero surprises for any report. Validate with metric 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "Reviews that are fair, actionable, and defensible in calibration; zero surprises for reports; 100% of reviews include specific examples with quantified impact.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-075",
        "observableId": "C14-O1",
        "capabilityId": "C14",
        "signalText": "Core craft skill. Calibration committees can tell lazy reviews immediately — vague language ('did great work') vs. specific ('drove X resulting in Y, demonstrating next-level Z')",
        "signalType": "calibration_language",
        "sourceSubTopic": "Writing Performance Reviews"
      }
    ]
  },
  {
    "id": "C14-O2",
    "capabilityId": "C14",
    "shortText": "Prepares calibration cases with evidence portfolio and anticipated pushback",
    "slug": "prepares-calibration-cases-with-evidence-portfolio-and-anticipated-pushback",
    "fullExample": "Per person: 1-page impact portfolio with top 3 achievements, peer feedback synthesis, cross-team comparison context, counter-arguments prepared. Expected ratings achieved consistently.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.16,
    "requiredFrequency": "biannual",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs prepare individual cases; Directors run cross-EM calibration",
    "why": "Your IC's rating lives or dies in calibration — you're their advocate in a room of competing advocates. In the calibration committee model, your case competes against every other manager's cases — the quality of your preparation directly determines your people's outcomes.",
    "how": "Build a 1-page impact portfolio per person using a standard template: top 3 achievements with quantified outcomes, peer feedback synthesized into 2-3 themes, cross-team comparison context (how does this person's impact compare to same-level peers on other teams?), and anticipated pushback with prepared counter-arguments. Practice delivery with a trusted peer manager 1 week before calibration. Pre-align with calibration peers on borderline cases informally before the session. Prepare 2x the evidence for contested ratings vs. uncontested. Target: 90%+ of your people receive expected ratings. Validate with metric 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "90%+ of your people receive expected ratings; you're credible in calibration room; zero post-calibration surprises for reports; calibration peers seek your input on borderline cases.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-076",
        "observableId": "C14-O2",
        "capabilityId": "C14",
        "signalText": "The highest-leverage activity an EM does twice a year. 'Prepared calibration cases for [X] engineers, [Y]% received expected rating'",
        "signalType": "metric",
        "sourceSubTopic": "Calibration Preparation"
      }
    ]
  },
  {
    "id": "C14-O3",
    "capabilityId": "C14",
    "shortText": "Navigates ratings distribution with honest differentiation and zero escalations",
    "slug": "navigates-ratings-distribution-with-honest-differentiation-and-zero-escalations",
    "fullExample": "Understands company's specific distribution curve; differentiates honestly without inflating; invests preparation proportional to contested ratings.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.097,
    "requiredFrequency": "biannual",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs navigate for their team; Directors calibrate across EMs for consistency",
    "why": "Every manager wants all their people rated highly — forced distribution means hard choices. The Keeper Test ('would I fight to keep this person?') forces honest differentiation that most managers avoid until it's too late.",
    "how": "Study your company's specific distribution curve (typical: 15% top, 70% meets, 15% below) and map where each of your reports lands before calibration. Differentiate honestly using evidence not politics — rank your team on key dimensions (impact, scope, technical depth, leadership) and prepare a stack-rank you can defend. Prepare contested cases with 3x the evidence of uncontested ones, including specific counter-arguments for likely pushback. Pre-align with calibration peers on borderline cases 1-2 weeks before the session through informal conversations. For Directors: run a pre-calibration across your EMs to ensure consistency before the wider calibration room. Invest preparation time proportional to rating risk: 2 hours for contested cases, 30 minutes for clear cases. Validate with metric 7.1 (Regrettable Attrition Rate) and 7.2 (Non-Regrettable Attrition Rate).",
    "expectedResult": "Fair differentiation with zero escalations; no surprises for reports; credibility with leadership and peers; team trusts the process even when outcomes are hard.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-077",
        "observableId": "C14-O3",
        "capabilityId": "C14",
        "signalText": "Directors manage distribution across their EMs — the meta-calibration. 'Managed distribution across [X] EMs, maintained consistency and zero escalations'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Ratings Distribution Management"
      },
      {
        "id": "SIG-094",
        "observableId": "C14-O3",
        "capabilityId": "C14",
        "signalText": "The meta-skill of Director-level performance management: 'Calibrated [X] EMs, resulting in consistent bar across [Y] teams'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Calibrating Calibrators"
      }
    ]
  },
  {
    "id": "C14-O4",
    "capabilityId": "C14",
    "shortText": "Delivers performance feedback within 48hrs using SBI framework",
    "slug": "delivers-performance-feedback-within-48hrs-using-sbi-framework",
    "fullExample": "Uses Situation→Behavior→Impact framework for all feedback. Never saves feedback for perf review. Direct reports consistently confirm in skip-levels: 'I know where I stand.'",
    "evidenceTypes": [
      "note",
      "meeting_note"
    ],
    "defaultWeight": 0.129,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs deliver directly; Directors coach EMs on feedback skill",
    "why": "Sugar-coated feedback confuses; harsh feedback destroys trust; delayed feedback misses the learning moment. The expectation is feedback within 48 hours using a structured format — this converts feedback from a dreaded annual event into a continuous improvement mechanism.",
    "how": "Use the SBI framework (Situation-Behavior-Impact) for all performance feedback: describe the specific situation, the observed behavior, and its impact on the team or project. Deliver feedback within 48 hours of the observation in a private 1:1 setting, never in public or via Slack. For constructive feedback, pair the observation with a specific ask: 'Next time, I'd like to see you [alternative behavior].' Confirm understanding by asking the report to summarize what they heard. Never save feedback for performance reviews — direct reports should be able to say 'I know where I stand' at any point. Document feedback themes in 1:1 notes and revisit progress monthly. Validate with metric 7.1 (Regrettable Attrition Rate) and 5.1 (Developer Satisfaction Score).",
    "expectedResult": "People know where they stand; behavior changes in real-time; trust built through honesty.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-078",
        "observableId": "C14-O4",
        "capabilityId": "C14",
        "signalText": "'No surprises' policy — confirmed through skip-level feedback. Direct reports consistently say they know where they stand",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Delivering Performance Feedback"
      }
    ]
  },
  {
    "id": "C14-O5",
    "capabilityId": "C14",
    "shortText": "Builds promotion evidence over multiple cycles with deliberate gap-filling",
    "slug": "builds-promotion-evidence-over-multiple-cycles-with-deliberate-gap-filling",
    "fullExample": "Starts building promo case 2-3 cycles before target; maps current work to next-level rubric; identifies evidence gaps; coordinates with the engineer's coach on gap-filling work; gets pre-reads from calibration peers.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.129,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs build individual cases; Directors coach on strategy and provide visibility opportunities",
    "why": "Engineers do great work but can't articulate it at next level; evidence isn't collected over time. The promotion narrative process requires a written document demonstrating sustained next-level impact — managers who don't build evidence over multiple cycles find themselves unable to write a compelling case when it matters.",
    "how": "Start building the promotion case 2-3 review cycles before the target promotion date. Create a shared promotion tracking doc (Google Doc or Notion) mapping the person's current work to each next-level rubric criterion, highlighting evidence gaps explicitly. Collect evidence continuously in a running doc with dated entries — impact metrics, scope artifacts, 360 feedback excerpts. Coordinate with the engineer on gap-filling work (stretch assignment design is C6-O5's territory; evidence collection and portfolio construction is yours). Get pre-reads from calibration peers and skip-level 1 cycle before submission to calibrate expectations and identify weaknesses. The written case must stand on its own without the manager presenting it. Target: promotion cases succeed on first submission. Validate with metric 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "Promotion cases succeed on first submission; promotion rate reflects actual talent; zero 'they do great work but we can't prove it' situations; engineers trust you as an active career sponsor.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-079",
        "observableId": "C14-O5",
        "capabilityId": "C14",
        "signalText": "'Promoted [X] engineers in [Y] cycles' — high-performing EM signal. Promo packet quality reflects EM craft",
        "signalType": "promo_packet",
        "sourceSubTopic": "Promo Packet Construction"
      }
    ]
  },
  {
    "id": "C14-O6",
    "capabilityId": "C14",
    "shortText": "Runs fair PIPs with genuine support and clean process — zero procedural issues",
    "slug": "runs-fair-pips-with-genuine-support-and-clean-process-zero-procedural-issues",
    "fullExample": "Specific gaps with examples, measurable criteria, 30-60 day timeline, weekly check-ins with documented progress, genuine coaching and resources. HR sees them as reliable partner.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.161,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs execute; Directors oversee and ensure consistency across org",
    "why": "PIPs that are too late, too vague, or set up to fail damage trust and create legal risk. Performance improvement plans must include measurable goals, weekly coaching checkpoints, and a genuine path to success — PIPs designed as a paper trail for termination backfire culturally.",
    "how": "Document specific performance gaps with concrete examples (not personality traits) in a PIP template reviewed by HR before delivery. Set 3-5 measurable success criteria the employee can objectively achieve (e.g., 'Ship 2 features independently with <1 critical bug' not 'improve attitude'). Define a clear timeline (30-60 days depending on severity) with weekly check-in cadence. Provide genuine support: pair with a mentor, adjust workload to focus on PIP criteria, offer training resources. Partner with HR on process and legal review at each stage. Document every check-in with written summary shared with the employee. At each checkpoint, assess 'on track / needs adjustment / not improving' and communicate transparently. Target: PIPs result in genuine improvement 40%+ of the time; exits are clean with zero procedural issues. Validate with metric 7.2 (Non-Regrettable Attrition Rate).",
    "expectedResult": "People who can improve do; people who can't exit with dignity; process is defensible.",
    "status": "CARRIED",
    "calibrationSignals": [
      {
        "id": "SIG-083",
        "observableId": "C14-O6",
        "capabilityId": "C14",
        "signalText": "HR scrutinizes PIP quality. Well-run PIPs protect the team, the individual, and the company. 'PIP resulted in [improvement/exit] with clean process and zero escalations'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Performance Improvement Plans (PIP)"
      },
      {
        "id": "SIG-084",
        "observableId": "C14-O6",
        "capabilityId": "C14",
        "signalText": "The hardest thing an EM does. Avoidance is the most common failure mode — and it's visible in team health and calibration",
        "signalType": "calibration_language",
        "sourceSubTopic": "Managing Out"
      },
      {
        "id": "SIG-085",
        "observableId": "C14-O6",
        "capabilityId": "C14",
        "signalText": "Company-specific knowledge that EMs must master — no room for error. 'Executed [X] performance processes with zero procedural issues'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Focus/Pivot/Dev Plan Mechanics"
      }
    ]
  },
  {
    "id": "C1-O11",
    "capabilityId": "C1",
    "shortText": "Identifies platform opportunities and builds teams that reduce org-wide cognitive load",
    "slug": "identifies-platform-opportunities-and-builds-teams-that-reduce-org-wide-cognitive-load",
    "fullExample": "Justified and built internal platform team providing self-service deployment, observability, and auth. 8 stream-aligned teams adopted, saving estimated 12 eng-months/quarter of duplicated effort.",
    "evidenceTypes": [
      "doc",
      "metric",
      "org_chart"
    ],
    "defaultWeight": 0.025,
    "requiredFrequency": "episodic",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Director-level: platform teams are org-level leverage. EMs consume platforms and advocate for team needs.",
    "why": "Stream-aligned teams waste time rebuilding shared infrastructure (auth, observability, deployment) independently. Dedicated platform teams reduce cognitive load and eliminate duplicate effort across the org by identifying capabilities that should be shared.",
    "how": "Identify repeated work across teams by surveying EMs quarterly for duplicated infrastructure efforts (auth, observability, deployment). Justify platform investment with an aggregate cost analysis showing eng-months saved per quarter versus platform team cost. Build platform team with self-service APIs, golden-path templates, and developer documentation; target >80% voluntary adoption within two quarters. Measure adoption rate and time-to-production for consuming teams monthly. Validate with metric 8.4 (Engineering Investment Mix) and 10.1 (Cloud Cost per Transaction/User).",
    "expectedResult": "Reduced duplication; stream-aligned teams focus on differentiated work; platform team measured by adoption.",
    "status": "REVIEWED",
    "calibrationSignals": [
      {
        "id": "SIG-037",
        "observableId": "C1-O11",
        "capabilityId": "C1",
        "signalText": "Director-level leverage: 'Justified and built platform team, [X] teams adopted, saving [Y] eng-months/quarter org-wide'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Platform Teams"
      }
    ]
  },
  {
    "id": "C1-O12",
    "capabilityId": "C1",
    "shortText": "Navigates externally-imposed changes (re-orgs, new leadership) while maintaining team stability",
    "slug": "navigates-externally-imposed-changes-re-orgs-new-leadership-while-maintaining-team-stability",
    "fullExample": "VP announces re-org splitting team; guides team through transition with honest communication about what's known/unknown, preserves key relationships, maintains velocity, zero attrition. Builds trust with new leadership within 30 days.",
    "evidenceTypes": [
      "meeting_note",
      "email",
      "attrition_data"
    ],
    "defaultWeight": 0.012,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels: how you handle adversity you didn't create reveals leadership character.",
    "why": "Imposed re-orgs, new leadership above you, and structural changes you didn't decide create team anxiety and distrust. Once the decision is made above you, your job is to execute it well — disagree and commit, but protect your team through the transition.",
    "how": "Be transparent about what you know and don't using a structured FAQ document shared within 48 hours of the announcement. Shield team from speculation while being honest about uncertainty; hold a dedicated team Q&A session within the first week. Maintain delivery focus on what you can control by publishing a 'what stays the same' list. With new leadership: learn their priorities in week one via a 1:1 with a prepared team state briefing (team composition, key metrics, current bets, top risks), proactively provide context, and demonstrate competence through two to three quick wins in the first 30 days. Track velocity dip and attrition through the transition — target velocity recovery within two sprints. Validate with metric 7.1 (Regrettable Attrition Rate) and 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Team stability maintained through imposed changes; trust preserved; zero attrition cascade; velocity dip limited to <2 sprints; new leadership relationship established within 30 days.",
    "status": "REVIEWED",
    "calibrationSignals": [
      {
        "id": "SIG-057",
        "observableId": "C1-O12",
        "capabilityId": "C1",
        "signalText": "How you handle adversity you didn't create reveals character. 'Guided team through [re-org], maintained velocity and zero attrition'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Re-orgs You Didn't Initiate"
      }
    ]
  },
  {
    "id": "C1-O13",
    "capabilityId": "C1",
    "shortText": "Conducts structured assessment of inherited team within first 30 days",
    "slug": "conducts-structured-assessment-of-inherited-team-within-first-30-days",
    "fullExample": "In first 30 days as new EM: mapped people (strengths, aspirations, concerns), processes (what works, what's broken), and systems (tech debt, reliability gaps). Identified 4 key issues, built trust through listening before initiating changes in month 2.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.012,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels: the 90-day window determines leadership trajectory. EMs assess team; Directors assess org.",
    "why": "Making changes too fast destroys trust; waiting too long misses the window for establishing direction. The first 30 days as a new leader are disproportionately high-leverage — patterns established in this period (listening vs. dictating, data-driven vs. opinion-driven) set the tone for the entire tenure.",
    "how": "First 30 days: listen and assess using a structured template covering people (strengths, aspirations, concerns per person via 1:1s), process (ceremonies, workflows, pain points), and systems (tech debt inventory, reliability gaps, deployment pipeline state). Produce a written assessment document by day 30. Days 30-60: earn credibility through two to three quick wins on pain points identified in the assessment, chosen for high visibility and low risk. Days 60-90: initiate structural changes with team buy-in, referencing assessment findings as rationale. Validate with metric 5.1 (Developer Satisfaction Score) and 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "Informed picture of team state; credibility established; changes made with buy-in rather than resistance.",
    "status": "REVIEWED",
    "calibrationSignals": [
      {
        "id": "SIG-062",
        "observableId": "C1-O13",
        "capabilityId": "C1",
        "signalText": "'Assessed team in first 30 days, identified [X] key issues, built trust before initiating changes'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Inheriting a Team: Assessment"
      }
    ]
  },
  {
    "id": "C3-O8",
    "capabilityId": "C3",
    "shortText": "Conducts proactive capacity planning and identifies platform consolidation opportunities",
    "slug": "conducts-proactive-capacity-planning-and-identifies-platform-consolidation-opportunities",
    "fullExample": "Anticipated 3x load for product launch: pre-scaled infrastructure with zero downtime. Identified that 4 teams built separate caching layers; proposed consolidated platform approach saving $800K/year.",
    "evidenceTypes": [
      "doc",
      "metric",
      "dashboard"
    ],
    "defaultWeight": 0.034,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team-level capacity planning; Directors identify org-level platform opportunities.",
    "why": "Reactive scaling costs 10x what proactive planning costs; duplicate infrastructure investments waste eng capacity across the org.",
    "how": "Model load growth quarterly (seasonal patterns, planned launches, organic growth) using historical data and product roadmap inputs. Pre-scale infrastructure at least two weeks ahead of projected demand spikes using load-test results as the sizing input. Run load tests monthly or before every major launch. Identify duplicate infrastructure across teams through a semi-annual infrastructure audit; flag consolidation candidates when three or more teams maintain similar services. Track cost-per-transaction trends monthly in a shared dashboard. Validate with metric 10.1 (Cloud Cost per Transaction/User) and 8.4 (Engineering Investment Mix).",
    "expectedResult": "No capacity surprises; infrastructure costs optimized; platform thinking reduces org-wide duplication.",
    "status": "REVIEWED",
    "calibrationSignals": [
      {
        "id": "SIG-006",
        "observableId": "C3-O8",
        "capabilityId": "C3",
        "signalText": "Operational maturity signal: 'Anticipated 3x load for [event], pre-scaled with zero downtime'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Capacity Planning"
      },
      {
        "id": "SIG-010",
        "observableId": "C3-O8",
        "capabilityId": "C3",
        "signalText": "Director-level: 'Identified [platform opportunity], built team around it, [X] teams adopted, saving [Y] eng-months/quarter across org'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Platform Thinking"
      }
    ]
  },
  {
    "id": "C4-O9",
    "capabilityId": "C4",
    "shortText": "Establishes runbook culture ensuring every alert has documented remediation steps",
    "slug": "establishes-runbook-culture-ensuring-every-alert-has-documented-remediation-steps",
    "fullExample": "Every page in PagerDuty links to a runbook. New on-call engineers handle incidents independently within first rotation. Runbook coverage: 95% of alerts, reviewed quarterly.",
    "evidenceTypes": [
      "doc",
      "oncall_data",
      "metric"
    ],
    "defaultWeight": 0.029,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own team runbook quality; Directors audit runbook coverage across teams as operational maturity signal.",
    "why": "Tribal knowledge in individuals' heads means fragile operations; new on-call engineers are helpless at 3 AM without documented procedures.",
    "how": "Every alert in PagerDuty/Opsgenie must have a linked runbook. Runbooks include: symptoms, diagnosis steps, remediation commands, escalation criteria, and rollback procedures. Review and update runbooks after every incident that required improvisation. Audit runbook coverage quarterly — target >=95% of alerts with linked, reviewed runbooks. Track coverage percentage and time-since-last-review on a shared operational dashboard. Validate with metric 1.3 (Mean Time to Restore) and 3.7 (Post-Incident Action Item Completion Rate).",
    "expectedResult": "New on-call engineers are effective immediately; operations scale beyond individual heroics; reduced MTTR.",
    "status": "REVIEWED",
    "calibrationSignals": [
      {
        "id": "SIG-027",
        "observableId": "C4-O9",
        "capabilityId": "C4",
        "signalText": "Operational maturity that scales beyond individual heroics. 'Every page has a runbook; new on-call engineers effective within first rotation'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Runbook Culture"
      }
    ]
  },
  {
    "id": "C4-O10",
    "capabilityId": "C4",
    "shortText": "Implements tiered change management proportional to risk level, preventing change-induced incidents",
    "slug": "implements-tiered-change-management-proportional-to-risk-level-preventing-change-induced-incidents",
    "fullExample": "Tiered process: automated checks for config changes, peer review + staged rollout for schema migrations, full approval + maintenance window for cross-service changes. All production changes tracked and auditable. Change-related incidents reduced 65%.",
    "evidenceTypes": [
      "doc",
      "metric",
      "pr"
    ],
    "defaultWeight": 0.059,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "Directors drive change management culture across org; EMs implement for their team. Covers both operational rhythm (C4) and reliability/risk prevention (formerly C8-O5) dimensions.",
    "why": "Uncoordinated changes (config pushes, schema migrations, flag flips) are the #1 cause of production incidents — cascading failures that are entirely preventable through proportional change controls.",
    "how": "Classify changes by blast radius x reversibility into three tiers: low-risk (automated gates, self-serve deploy), medium-risk (peer review + canary/staged rollout), high-risk (approval + maintenance window + documented rollback plan). Review tier definitions quarterly and adjust based on incident post-mortems. Track all production changes in a change log for auditability. Target: zero untracked production changes; change-induced incidents reduced quarter-over-quarter. Validate with metric 1.4 (Change Failure Rate) and 4.3 (Rollback Rate).",
    "expectedResult": "Reduced change-induced incidents; process proportional to risk; teams can move fast on low-risk changes.",
    "status": "UPDATED",
    "calibrationSignals": [
      {
        "id": "SIG-028",
        "observableId": "C4-O10",
        "capabilityId": "C4",
        "signalText": "Directors drive change management culture across org. 'Implemented tiered change management — change-induced incidents reduced X%'",
        "signalType": "metric",
        "sourceSubTopic": "Change Management Discipline"
      }
    ]
  },
  {
    "id": "C4-O11",
    "capabilityId": "C4",
    "shortText": "Adapts team practices for remote/hybrid work ensuring equal engagement regardless of location",
    "slug": "adapts-team-practices-for-remotehybrid-work-ensuring-equal-engagement-regardless-of-location",
    "fullExample": "Manages distributed team across 3 timezones: async-first communication, recorded decisions in shared docs, defined overlap hours for real-time collaboration. No proximity bias in stretch assignments or promotions.",
    "evidenceTypes": [
      "doc",
      "survey_data",
      "meeting_note"
    ],
    "defaultWeight": 0.029,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels: post-COVID essential competency for any team with distributed members.",
    "why": "Distributed teams face proximity bias, isolation, communication gaps, and cultural disconnection between co-located and remote members.",
    "how": "Default to async-first communication (written decisions in shared docs, recorded discussions). Create intentional overlap hours (minimum 3-hour daily window across time zones). Audit for proximity bias quarterly — review stretch assignment distribution, promotion rates, and meeting participation by location. Ensure equal access to information regardless of timezone via shared channels and recorded standups. Measure engagement parity via quarterly pulse surveys targeting >4/5 inclusion scores across all locations. Validate with metric 5.1 (Developer Satisfaction Score) and 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "Equal engagement regardless of location; no proximity bias in opportunities; effective async-first culture.",
    "status": "REVIEWED",
    "calibrationSignals": [
      {
        "id": "SIG-061",
        "observableId": "C4-O11",
        "capabilityId": "C4",
        "signalText": "Post-COVID essential competency. 'Managed distributed team across [X] timezones with equal engagement and promotion rates across locations'",
        "signalType": "promo_packet",
        "sourceSubTopic": "Managing Remote/Hybrid Teams"
      }
    ]
  },
  {
    "id": "C5-O11",
    "capabilityId": "C5",
    "shortText": "Navigates leadership transitions by rebuilding trust proactively within 30 days",
    "slug": "navigates-leadership-transitions-by-rebuilding-trust-proactively-within-30-days",
    "fullExample": "New VP arrives with different priorities. Within 2 weeks: scheduled intro, presented team portfolio with business impact framing, identified 2 alignment opportunities. VP trust established before first planning cycle.",
    "evidenceTypes": [
      "meeting_note",
      "email"
    ],
    "defaultWeight": 0.051,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Directors must proactively manage VP transitions; EMs manage Director transitions. Both critical survival skills.",
    "why": "Leadership transitions are the #1 cause of strategy whiplash and org instability. Passive waiting = 6 months of lost credibility.",
    "how": "Schedule intro meeting within week 1 of the leadership transition. Prepare a concise brief (1-2 pages in Google Docs or Notion) covering: team mission, top 3 recent wins with business impact, current bets and rationale, biggest risk. Ask the new leader: 'What does success look like for you in the first 90 days?' Identify one early alignment opportunity you can deliver within 2 weeks. Follow up with a written summary of agreed priorities within 48 hours. Conduct a trust checkpoint at the 30-day mark to confirm alignment is on track. Target: zero roadmap items overridden without mutual discussion within first quarter. Validate with metric 8.1 (OKR Achievement Rate).",
    "expectedResult": "New leader trusts team within 30 days. Roadmap survives transition intact or adapts by mutual agreement, not mandate.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-139",
        "observableId": "C5-O11",
        "capabilityId": "C5",
        "signalText": "Navigating leadership transitions defines career durability. \"Built trust with new [VP/Director] within 30 days, preserved roadmap through transition, identified alignment opportunities proactively.\"",
        "signalType": "manager_observation",
        "sourceSubTopic": "Leadership Transitions"
      },
      {
        "id": "SIG-335",
        "observableId": "C5-O11",
        "capabilityId": "C5",
        "signalText": "Prepared a 2-page leadership transition brief for incoming VP that was cited as 'the most useful onboarding artifact I received' — covered team mission, top wins, current bets, and biggest risk.",
        "signalType": "promo_packet",
        "sourceSubTopic": "Leadership Transitions"
      },
      {
        "id": "SIG-336",
        "observableId": "C5-O11",
        "capabilityId": "C5",
        "signalText": "When our director changed, they had a 1:1 with the new leader within 3 days and a written alignment doc within a week. The rest of us were still figuring out what the change meant while their team had already confirmed priorities.",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Leadership Transitions"
      }
    ]
  },
  {
    "id": "C5-O12",
    "capabilityId": "C5",
    "shortText": "Partners with Legal/Privacy/Policy teams to build reusable patterns that eliminate launch-blocking reviews",
    "slug": "partners-with-legalprivacypolicy-teams-to-build-reusable-patterns-that-eliminate-launch-blocking-reviews",
    "fullExample": "Built pre-approved design patterns for PII handling, consent flows, and data residency. Reduced launch-blocking legal reviews from 3 weeks to 2 days for 80% of launches.",
    "evidenceTypes": [
      "doc",
      "checklist"
    ],
    "defaultWeight": 0.051,
    "requiredFrequency": "quarterly",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "Directors build org-level patterns; EMs apply them and identify new pattern needs.",
    "why": "Legal/privacy reviews are the most common invisible launch blocker. Proactive partnership converts blockers to enablers.",
    "how": "Catalog recurring legal/privacy review types and frequency (e.g., PII handling, consent flows, data residency). Co-develop pre-approved design patterns with Legal/Privacy teams — document in a shared wiki or Confluence space. Embed as self-serve checklists in the team's launch process (e.g., launch readiness template). Establish a clear escalation path for genuinely novel cases only. Review and update patterns quarterly as regulations evolve. Target: >80% of launches self-serve through pre-approved patterns; launch-blocking legal reviews reduced from weeks to days. Validate with metric 1.2 (Lead Time for Changes) and 2.1 (Flow Time).",
    "expectedResult": "Standard launches self-serve through pre-approved patterns. Legal team focuses on genuinely novel risks. Launch velocity increases.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-140",
        "observableId": "C5-O12",
        "capabilityId": "C5",
        "signalText": "Directors who build reusable compliance patterns create org-wide leverage. \"Built pre-approved design patterns for [privacy scenario], eliminating [X] weeks of launch-blocking reviews for [Y] teams.\"",
        "signalType": "metric",
        "sourceSubTopic": "Legal/Privacy Partnership Patterns"
      },
      {
        "id": "SIG-337",
        "observableId": "C5-O12",
        "capabilityId": "C5",
        "signalText": "Co-developed 4 pre-approved privacy design patterns with Legal, reducing launch-blocking reviews from 3 weeks to 2 days for 80% of feature launches across the org.",
        "signalType": "promo_packet",
        "sourceSubTopic": "Legal/Privacy Partnership Patterns"
      },
      {
        "id": "SIG-338",
        "observableId": "C5-O12",
        "capabilityId": "C5",
        "signalText": "Their team ships features through compliance faster than anyone else because they invested upfront in reusable patterns. Other teams now use their self-serve checklists as a template.",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Legal/Privacy Partnership Patterns"
      }
    ]
  },
  {
    "id": "C5-O13",
    "capabilityId": "C5",
    "shortText": "Earns early credibility as new leader through quick wins and listening before changing",
    "slug": "earns-early-credibility-as-new-leader-through-quick-wins-and-listening-before-changing",
    "fullExample": "First 90 days: spent weeks 1-3 in listening mode (skip-levels, stakeholder intros, codebase review). Identified quick win (flaky test fix) in week 4. Delivered by week 6. Earned mandate for larger changes.",
    "evidenceTypes": [
      "meeting_note",
      "doc"
    ],
    "defaultWeight": 0.044,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "Both levels face this on every role change. Directors have shorter credibility windows.",
    "why": "New leaders who change things before earning trust face resistance that compounds. Quick wins + listening = earned mandate.",
    "how": "Follow a structured 90-day plan. Weeks 1-3: listen mode — conduct 1:1s with every direct report, skip-levels with key ICs, stakeholder intro meetings, and read existing docs (ADRs, postmortems, OKRs). Document observations in a private journal. Week 4: identify 1 visible quick win with high team impact and low risk (e.g., fix a persistent CI flake, remove a toil burden). Weeks 5-8: deliver the quick win and communicate the result broadly. Week 9+: propose larger changes with earned credibility, referencing your listening-phase observations. Target: team confidence score >4/5 by day 60 in pulse check. Validate with metric 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Team says \"they get us\" by week 6. Stakeholders trust judgment. Larger changes face minimal resistance.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-329",
        "observableId": "C5-O13",
        "capabilityId": "C5",
        "signalText": "The 90-day window determines trajectory. \"Established credibility by [quick win] in first 6 weeks, built stakeholder trust through listening before changing, earned mandate for [larger initiative].\"",
        "signalType": "peer_feedback",
        "sourceSubTopic": "New Leader Credibility"
      },
      {
        "id": "SIG-339",
        "observableId": "C5-O13",
        "capabilityId": "C5",
        "signalText": "Completed structured 90-day onboarding: 3 weeks listening (1:1s with all directs and key ICs), delivered a visible quick win (eliminated a 45-minute weekly status meeting) by week 5, then proposed larger changes with earned credibility.",
        "signalType": "manager_observation",
        "sourceSubTopic": "New Leader Credibility"
      },
      {
        "id": "SIG-340",
        "observableId": "C5-O13",
        "capabilityId": "C5",
        "signalText": "Achieved team confidence score of 4.3/5 in 60-day pulse check after joining as new EM, up from 3.1 under previous leadership, by prioritizing listening and quick wins over immediate process changes.",
        "signalType": "metric",
        "sourceSubTopic": "New Leader Credibility"
      }
    ]
  },
  {
    "id": "C6-O8",
    "capabilityId": "C6",
    "shortText": "Conducts systematic stay interviews surfacing retention drivers before they become exit interviews",
    "slug": "conducts-systematic-stay-interviews-surfacing-retention-drivers-before-they-become-exit-interviews",
    "fullExample": "Instituted quarterly stay interviews with all direct reports. Surfaced comp band concern across 3 engineers. Addressed with market adjustment before any resignations.",
    "evidenceTypes": [
      "meeting_note",
      "survey_data"
    ],
    "defaultWeight": 0.068,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs conduct directly; Directors ensure EMs run them and track themes across teams.",
    "why": "Exit interviews are post-mortems. Stay interviews are preventive medicine. Proactive retention is 10x cheaper than backfill.",
    "how": "Quarterly cadence, separate from performance conversations. Ask: What keeps you here? What might cause you to leave? What would make this the best job you've ever had? Track themes, act on systemic issues. Validate with metric 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "Zero surprise departures. Retention themes identified and addressed quarterly. Regrettable attrition below org average.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-142",
        "observableId": "C6-O8",
        "capabilityId": "C6",
        "signalText": "Advanced retention practice. \"Instituted stay interviews, surfaced [X] theme, addressed it, zero regrettable attrition for [Y] consecutive quarters.\"",
        "signalType": "metric",
        "sourceSubTopic": "Systematic Stay Interviews"
      }
    ]
  },
  {
    "id": "C6-O9",
    "capabilityId": "C6",
    "shortText": "Manages Staff+ engineers with calibrated autonomy, influence opportunities, and organizational sponsorship",
    "slug": "manages-staff-engineers-with-calibrated-autonomy-influence-opportunities-and-organizational-sponsorship",
    "fullExample": "Partnered with Staff engineer: co-owned technical vision, provided organizational air cover, created VP-visibility opportunities. Staff engineer led cross-org migration, cited partnership in promo packet.",
    "evidenceTypes": [
      "meeting_note",
      "peer_feedback"
    ],
    "defaultWeight": 0.068,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs learn to work alongside Staff engineers; Directors sponsor Staff+ engineers across the org.",
    "why": "Staff+ engineers need organizational leverage, not task management. EMs who micromanage them lose their best people; EMs who ignore them waste their highest-leverage resource.",
    "how": "Co-create technical strategy (not dictate) through quarterly strategy sessions using a shared technical vision doc with explicit ownership boundaries. Provide organizational context they lack via weekly 1:1s (30 minutes) focused on org dynamics, political landscape, and stakeholder priorities — not task management. Create cross-team visibility opportunities by nominating Staff+ engineers for architecture review boards, VP-level design reviews, and cross-org working groups at least once per quarter. Remove organizational blockers within 48 hours of identification by leveraging Director-level relationships and escalation paths. Give feedback on influence, not code — assess communication clarity, stakeholder management, and technical leadership using a Staff+ competency rubric reviewed bi-annually. Target zero Staff+ attrition from feeling undervalued; track via quarterly pulse surveys with Staff+-specific questions. Validate with metric 5.1 (Developer Satisfaction Score) and 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "Staff+ engineers cite EM/Director partnership as force multiplier. Technical vision has organizational backing. Zero Staff+ attrition from feeling undervalued.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-143",
        "observableId": "C6-O9",
        "capabilityId": "C6",
        "signalText": "Highest-leverage people management. \"Partnered with [Staff engineer] on [technical vision], provided organizational air cover for [initiative], Staff engineer cited partnership as force multiplier.\"",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Staff+ Engineer Partnership"
      }
    ]
  },
  {
    "id": "C6-O10",
    "capabilityId": "C6",
    "shortText": "Runs monthly career conversations (separate from performance 1:1s) focused on 18-month aspirations with written development plans",
    "slug": "drives-career-development-through-structured-career-conversations-distinct-from-performance-feedback",
    "fullExample": "Separated career conversations from performance check-ins. Monthly career 1:1 focused on 18-month aspirations, skill gaps, and growth experiments. 100% of reports have written development plans.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.068,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs conduct directly; Directors ensure EM quality through skip-level feedback.",
    "why": "Career development mixed into performance conversations gets crowded out by tactical urgency. Dedicated space signals genuine investment in the person.",
    "how": "Monthly cadence, separate calendar slot from perf/tactical 1:1. Focus: Where do you want to be in 18 months? What skills are gaps? What experiences would build evidence? Document plans, review quarterly. Validate with metric 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Engineers articulate their own growth path. Stretch assignments aligned to development goals. Promotion readiness is predictable, not surprising.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-144",
        "observableId": "C6-O10",
        "capabilityId": "C6",
        "signalText": "Retention and growth signal. \"100% of direct reports have active development plans. [X] engineers progressed to next level through deliberate skill-building aligned to career conversations.\"",
        "signalType": "metric",
        "sourceSubTopic": "Structured Career Development"
      }
    ]
  },
  {
    "id": "C1-O14",
    "capabilityId": "C1",
    "shortText": "Leads engineering teams through M&A integration with codebase consolidation and talent retention",
    "slug": "leads-engineering-teams-through-ma-integration-with-codebase-consolidation-and-talent-retention",
    "fullExample": "Led integration of 25-person acquired team: mapped overlapping services within 2 weeks, established unified coding standards by month 2, consolidated 3 duplicated services by month 4. Retained 92% of acquired engineers through intentional culture bridging and clear role mapping.",
    "evidenceTypes": [
      "decision_doc",
      "comms_plan",
      "attrition_data"
    ],
    "defaultWeight": 0.025,
    "requiredFrequency": "episodic",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Directors own integration strategy and culture alignment; EMs manage day-to-day team merging and individual onboarding of acquired engineers.",
    "why": "Failed M&A integrations destroy the value the acquisition was supposed to create. Culture clashes and unclear ownership cause acquired talent to leave within 12 months.",
    "how": "Map overlapping systems and ownership within the first two weeks using a service-inventory comparison matrix. Establish unified coding standards, CI/CD tooling, and on-call processes by month two. Pair each acquired engineer with an existing team buddy for the first 60 days. Create explicit role clarity via updated job descriptions and RACI charts. Preserve valuable acquired practices by cataloguing them in a 'best of both' document reviewed jointly. Track retention of acquired engineers monthly for 12 months — target >85% retention at the 12-month mark. Validate with metric 7.1 (Regrettable Attrition Rate) and 10.3 (Cost per Engineer, Fully Loaded).",
    "expectedResult": "Acquired team productive within 60 days; duplicated systems consolidated within 6 months; >85% retention of acquired engineering talent at 12-month mark.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-145",
        "observableId": "C1-O14",
        "capabilityId": "C1",
        "signalText": "M&A integration is Director-defining work. 'Led integration of [X]-person acquired team, consolidated [Y] overlapping services, retained [Z]% of acquired engineers through structured culture bridging.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "M&A Engineering Integration"
      },
      {
        "id": "SIG-146",
        "observableId": "C1-O14",
        "capabilityId": "C1",
        "signalText": "Integration velocity metric: 'Acquired team reached full productivity within [X] days, codebase consolidation completed [Y] months ahead of schedule, saving $[Z] in duplicated infrastructure.'",
        "signalType": "metric",
        "sourceSubTopic": "Acquisition Talent Retention"
      }
    ]
  },
  {
    "id": "C1-O15",
    "capabilityId": "C1",
    "shortText": "Manages divestiture and team wind-down with dignity, knowledge transfer, and minimal organizational disruption",
    "slug": "manages-divestiture-and-team-wind-down-with-dignity-knowledge-transfer-and-minimal-organizational-disruption",
    "fullExample": "Led wind-down of 15-person product team after strategic pivot: created comprehensive knowledge transfer plan within 1 week, placed 12 of 15 engineers into internal roles within 30 days, archived documentation for all critical systems, zero production incidents during transition.",
    "evidenceTypes": [
      "comms_plan",
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.025,
    "requiredFrequency": "episodic",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Directors own wind-down strategy and people placement; EMs execute knowledge transfer and support affected individuals.",
    "why": "Poorly managed wind-downs destroy institutional knowledge, damage employer brand, and create production orphans that haunt the org for years.",
    "how": "Communicate transparently about timeline and decisions using a written wind-down plan shared within one week of the decision (includes timeline, people placement plan, system transfer schedule). Prioritize internal placement for affected people — target >80% internal placement within 30 days by actively matching skills to open roles across the org. Document all critical systems and runbooks using a knowledge-transfer checklist before disbanding; require receiving teams to sign off on each system transfer. Transfer ownership explicitly with receiving teams via paired on-call rotations for at least two weeks. Provide references and career support for those who must leave. Track zero orphaned services as the exit criterion. Validate with metric 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "Zero orphaned systems; knowledge preserved; affected people treated with dignity; team alumni speak well of the experience.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-147",
        "observableId": "C1-O15",
        "capabilityId": "C1",
        "signalText": "How you wind down reveals leadership character. 'Managed wind-down of [X]-person team, placed [Y]% internally, completed knowledge transfer for [Z] critical systems with zero orphaned services.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Team Wind-Down Leadership"
      },
      {
        "id": "SIG-148",
        "observableId": "C1-O15",
        "capabilityId": "C1",
        "signalText": "'Led divestiture affecting [X] engineers: all critical systems had documented ownership transfer, zero production incidents during transition, [Y]% of displaced engineers placed in new roles within 30 days.'",
        "signalType": "metric",
        "sourceSubTopic": "Divestiture Knowledge Preservation"
      }
    ]
  },
  {
    "id": "C3-O9",
    "capabilityId": "C3",
    "shortText": "Designs systems with cost consciousness and drives FinOps practices across the team",
    "slug": "designs-systems-with-cost-consciousness-and-drives-finops-practices-across-the-team",
    "fullExample": "Implemented cost-aware architecture reviews: every design doc includes estimated cloud cost. Established FinOps practice with weekly cost dashboards per service. Identified $400K/year in overprovisioned compute, right-sized without performance impact. Engineering team owns cost as a first-class metric.",
    "evidenceTypes": [
      "dashboard",
      "metric",
      "design_doc"
    ],
    "defaultWeight": 0.034,
    "requiredFrequency": "continuous",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs own team-level cost awareness and optimization; Directors drive FinOps culture and cost architecture principles across the org.",
    "why": "Cloud costs grow invisibly until finance escalates. Teams that don't own cost as a metric build architectures that become unsustainably expensive at scale.",
    "how": "Make cost visible per service/team via dashboards. Include cost estimates in design reviews. Set cost budgets per team with alert thresholds. Run quarterly cost optimization sprints. Track cost-per-transaction as a first-class metric alongside latency and error rate. Validate with metric 10.1 (Cloud Cost per Transaction/User).",
    "expectedResult": "Cost per transaction trends downward; teams proactively optimize before finance escalates; architecture decisions consider cost as a first-class constraint.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-149",
        "observableId": "C3-O9",
        "capabilityId": "C3",
        "signalText": "FinOps maturity is a Director-level differentiator. 'Established cost-per-service dashboards, identified $[X] in optimization opportunities, reduced cloud spend [Y]% while maintaining performance SLAs.'",
        "signalType": "metric",
        "sourceSubTopic": "FinOps and Cost Architecture"
      },
      {
        "id": "SIG-147",
        "observableId": "C3-O9",
        "capabilityId": "C3",
        "signalText": "Cost consciousness embedded in design culture: 'Every design doc includes cost projection. Team reduced cost-per-transaction [X]% through [right-sizing / reserved instances / architecture change].'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Cost-Aware Design Reviews"
      }
    ]
  },
  {
    "id": "C3-O10",
    "capabilityId": "C3",
    "shortText": "Incorporates energy efficiency into architecture decisions and infrastructure planning",
    "slug": "incorporates-sustainability-and-energy-efficiency-into-architecture-decisions-and-infrastructure-planning",
    "fullExample": "Implemented carbon-aware workload scheduling, shifting batch processing to low-carbon-intensity hours. Adopted ARM-based instances for stateless services, reducing energy consumption 30%. Reported sustainability metrics in quarterly business reviews.",
    "evidenceTypes": [
      "design_doc",
      "metric",
      "dashboard"
    ],
    "defaultWeight": 0.025,
    "requiredFrequency": "quarterly",
    "emRelevance": "Low",
    "directorRelevance": "Medium",
    "levelNotes": "EMs implement green computing practices at the team level; Directors set sustainability targets and incorporate them into architectural standards.",
    "why": "Engineering organizations are increasingly accountable for their carbon footprint. Sustainability metrics affect procurement decisions, regulatory compliance, and corporate ESG commitments.",
    "how": "Measure compute carbon footprint quarterly using cloud provider sustainability dashboards (AWS Customer Carbon Footprint Tool, GCP Carbon Footprint, Azure Emissions Impact Dashboard). Schedule deferrable batch workloads during low-carbon-intensity periods using carbon-aware job schedulers. Evaluate energy-efficient instance types (e.g., ARM-based Graviton) for all new services and during annual instance-type reviews. Set idle resource cleanup policies with automated enforcement (e.g., tag-based auto-shutdown for non-production resources after 72 hours idle). Report sustainability metrics (carbon intensity per transaction, energy consumption trends) alongside cost and performance in quarterly business reviews. Validate with metric 10.1 (Cloud Cost per Transaction/User).",
    "expectedResult": "Within 2 quarters: measurable reduction in compute carbon intensity per transaction; sustainability metrics reported alongside cost and performance in quarterly reviews.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-148",
        "observableId": "C3-O10",
        "capabilityId": "C3",
        "signalText": "Emerging differentiator for senior leaders. 'Implemented carbon-aware scheduling for batch workloads, reduced compute carbon intensity [X]% while maintaining SLA commitments.'",
        "signalType": "metric",
        "sourceSubTopic": "Carbon-Aware Computing"
      },
      {
        "id": "SIG-149",
        "observableId": "C3-O10",
        "capabilityId": "C3",
        "signalText": "'Adopted energy-efficient infrastructure (ARM instances, right-sized clusters), reduced energy consumption [X]% for stateless services. Sustainability metrics reported in quarterly business reviews.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Green Architecture Decisions"
      }
    ]
  },
  {
    "id": "C5-O14",
    "capabilityId": "C5",
    "shortText": "Navigates multiple reporting lines and stakeholder alignment in matrix management structures",
    "slug": "navigates-multiple-reporting-lines-and-stakeholder-alignment-in-matrix-management-structures",
    "fullExample": "Reports to VP Engineering with dotted line to VP Product and CTO. Maintains weekly syncs with all three, aligns conflicting priorities through transparent trade-off communication. When priorities conflict, escalates with recommendation rather than forcing stakeholders to discover misalignment.",
    "evidenceTypes": [
      "meeting_note",
      "email",
      "doc"
    ],
    "defaultWeight": 0.034,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs often manage up to EM + PM + TL; Directors navigate VP-level matrix structures with higher-stakes conflicts.",
    "why": "Matrix organizations create conflicting priorities by design. Leaders who can't navigate multiple stakeholders become bottlenecks or lose credibility with one side.",
    "how": "Map each stakeholder's priorities and success metrics in a written stakeholder map (update quarterly). Maintain regular cadence: weekly 1:1 with primary reporting line, biweekly with secondary stakeholders. Surface conflicts early with a structured options-and-recommendation format (problem, 2-3 options, your recommendation, trade-offs). Never let stakeholders discover misalignment in a group meeting — pre-wire individually first. Track priority conflicts and resolution outcomes in a log to build pattern recognition. Target: zero 'I didn't know about this' moments; conflicts surfaced within 48 hours of detection. Validate with metric 8.1 (OKR Achievement Rate).",
    "expectedResult": "Stakeholders trust you as the integration point; conflicting priorities resolved proactively; no surprise escalations or 'I didn't know about this' moments.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-153",
        "observableId": "C5-O14",
        "capabilityId": "C5",
        "signalText": "Matrix management is a survival skill. 'Managed competing priorities from [X] stakeholders, proactively surfaced [Y] conflicts with trade-off analysis, maintained trust across all reporting lines.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Matrix Stakeholder Navigation"
      },
      {
        "id": "SIG-154",
        "observableId": "C5-O14",
        "capabilityId": "C5",
        "signalText": "Peer feedback signal: 'Acts as honest broker between competing priorities. I always know where I stand and why trade-offs were made.'",
        "signalType": "peer_feedback",
        "sourceSubTopic": "Managing Up to Multiple Leaders"
      }
    ]
  },
  {
    "id": "C5-O15",
    "capabilityId": "C5",
    "shortText": "Positions engineering as strategic asset at board level, securing investment through business-outcome narrative",
    "slug": "prepares-board-level-materials-that-frame-engineering-investment-as-strategic-business-narrative",
    "fullExample": "Authored engineering section of quarterly board deck: 3 slides covering platform reliability as competitive moat, R&D velocity benchmarked against industry, and talent pipeline as strategic risk. Board approved $2M infrastructure investment based on narrative.",
    "evidenceTypes": [
      "doc",
      "decision_doc"
    ],
    "defaultWeight": 0.025,
    "requiredFrequency": "quarterly",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Directors prepare content; VPs present. EMs rarely interact at board level but should understand the framing.",
    "why": "Board members have 5 minutes of attention for engineering. Technical depth loses them; strategic narrative earns investment and trust.",
    "how": "Frame engineering in board language: competitive advantage, risk mitigation, talent market position. Prepare quarterly board materials (2-3 slides max) using benchmarks and industry comparisons (DORA, Gartner) rather than absolute metrics. Tell a narrative arc: where we are, where we are going, what investment we need and why. Anticipate board questions and prepare 3-5 backup slides with data. Rehearse with VP/CTO one week before the board meeting. Target: board approves engineering investment requests based on your narrative; CTO cites your materials as board-ready. Validate with metric 8.4 (Engineering Investment Mix) and 8.2 (Revenue / Business Impact Attribution).",
    "expectedResult": "Board understands engineering as strategic asset, not cost center; investment requests approved; CTO/VP cites your materials as board-ready.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-155",
        "observableId": "C5-O15",
        "capabilityId": "C5",
        "signalText": "Director-to-VP bridge skill. 'Authored engineering board narrative that secured $[X]M investment. Framed reliability as competitive moat and R&D velocity as market advantage.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Board-Level Communication"
      },
      {
        "id": "SIG-156",
        "observableId": "C5-O15",
        "capabilityId": "C5",
        "signalText": "'Prepared quarterly board materials translating engineering metrics into strategic narrative. Board approved [infrastructure/platform] investment based on competitive positioning analysis.'",
        "signalType": "promo_packet",
        "sourceSubTopic": "Executive Communication at Board Level"
      }
    ]
  },
  {
    "id": "C6-O11",
    "capabilityId": "C6",
    "shortText": "Coaches struggling EMs through skill gaps with structured support and clear intervention criteria",
    "slug": "coaches-struggling-ems-through-skill-gaps-with-structured-support-and-clear-intervention-criteria",
    "fullExample": "Identified EM struggling with conflict resolution: team had 3 unresolved interpersonal issues. Created 90-day coaching plan with weekly role-play sessions, assigned mentor from senior EM pool, monitored via skip-level sentiment. EM resolved all issues by month 3 and cited coaching as career-defining.",
    "evidenceTypes": [
      "meeting_note",
      "doc",
      "peer_feedback"
    ],
    "defaultWeight": 0.03,
    "requiredFrequency": "continuous",
    "emRelevance": "Director-Only",
    "directorRelevance": "High",
    "levelNotes": "Director-level capability. Directors must diagnose EM skill gaps and provide structured coaching, not just delegate to HR.",
    "why": "Struggling EMs create cascading team problems. Without coaching, they either fail visibly or plateau invisibly, dragging team outcomes for quarters.",
    "how": "Use skip-levels (monthly, 20 minutes per IC) and team health signals (engagement surveys, attrition data, delivery metrics) to diagnose EM gaps early — review signals in a private dashboard weekly. Distinguish between skill gaps (coachable) and will gaps (not coachable) using a structured diagnostic framework: is the EM aware of the gap? Have they been given clear feedback and resources? Are they actively trying? Create a 90-day coaching plan in a shared doc with 3-5 observable milestones (e.g., 'resolve interpersonal conflict independently by day 45'), weekly 45-minute coaching sessions, and specific skill-building assignments. Pair with an experienced EM mentor from the senior EM pool for bi-weekly peer coaching. Set clear intervention criteria documented upfront: if no measurable improvement by day 60, initiate role transition conversation with HR partnership. Track coaching outcomes — target >60% of coached EMs showing measurable improvement within 90 days. Validate with metric 5.1 (Developer Satisfaction Score) and 7.8 (eNPS / Engagement Score).",
    "expectedResult": "EMs improve measurably or exit role with dignity. Teams don't suffer prolonged poor management. Director builds reputation as manager-of-managers.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-157",
        "observableId": "C6-O11",
        "capabilityId": "C6",
        "signalText": "Director's highest-leverage people work. 'Coached [X] struggling EM through [skill gap], implemented 90-day plan with measurable milestones, EM improved [metric] and team health score rose [Y] points.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Coaching Struggling EMs"
      },
      {
        "id": "SIG-158",
        "observableId": "C6-O11",
        "capabilityId": "C6",
        "signalText": "Knowing when to coach vs when to act: 'Identified EM skill gap via skip-level signals, distinguished from will gap, provided structured support. [X] of [Y] coached EMs showed measurable improvement within 90 days.'",
        "signalType": "metric",
        "sourceSubTopic": "EM Skill Gap Diagnosis"
      }
    ]
  },
  {
    "id": "C6-O12",
    "capabilityId": "C6",
    "shortText": "Maps succession for critical roles with primary/secondary successors assessed on a 3-tier readiness framework updated quarterly",
    "slug": "builds-succession-plans-ensuring-leadership-continuity-for-all-critical-roles",
    "fullExample": "Maintained succession matrix for all EM and TL roles: identified primary and secondary successors, documented readiness gaps, created development assignments to close gaps. When EM departed unexpectedly, successor stepped in with zero team disruption.",
    "evidenceTypes": [
      "doc",
      "meeting_note"
    ],
    "defaultWeight": 0.03,
    "requiredFrequency": "quarterly",
    "emRelevance": "Low",
    "directorRelevance": "High",
    "levelNotes": "Directors own succession planning for EM and critical TL roles; EMs identify succession candidates within their team for key IC positions.",
    "why": "Without succession planning, every departure is a crisis. Bus factor of 1 on leadership roles creates existential risk for teams.",
    "how": "Map all critical roles (EM, TL, key IC positions) in a succession matrix spreadsheet or Notion database updated quarterly. Identify primary and secondary successors for each role. Assess readiness honestly using a 3-tier framework: ready now, ready in 6 months (with specific gap-closing assignments), ready in 12 months (with development plan). Create stretch assignments that close the top 1-2 readiness gaps per successor per quarter — documented with clear success criteria. Review the succession matrix quarterly in a dedicated 60-minute planning session with HR business partner. Test succession readiness by delegating full authority during PTO periods (minimum 1 week) and measuring team continuity. Target: every critical role has at least 1 ready-now and 1 ready-in-6-months successor; zero leadership vacuums lasting >2 weeks. Validate with metric 7.1 (Regrettable Attrition Rate) and 7.6 (Span of Control).",
    "expectedResult": "No leadership vacuum when people leave; smooth transitions; successors are genuinely prepared, not just named.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-159",
        "observableId": "C6-O12",
        "capabilityId": "C6",
        "signalText": "Org resilience evidence. 'Maintained succession plan for [X] critical roles, with identified successors and readiness timelines. When [EM/TL] departed, successor transitioned with zero team disruption.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Succession Planning"
      },
      {
        "id": "SIG-160",
        "observableId": "C6-O12",
        "capabilityId": "C6",
        "signalText": "'Built bench strength through deliberate development: [X] succession candidates given stretch assignments, [Y] ready-now successors identified across [Z] critical roles. Quarterly review cadence.'",
        "signalType": "metric",
        "sourceSubTopic": "Bench Strength Development"
      }
    ]
  },
  {
    "id": "C8-O5",
    "capabilityId": "C8",
    "shortText": "Manages regulatory compliance risk with proactive frameworks and audit-ready engineering practices",
    "slug": "manages-regulatory-compliance-risk-with-proactive-frameworks-and-audit-ready-engineering-practices",
    "fullExample": "Built GDPR compliance framework for engineering: automated PII detection in data pipelines, consent management API used by 6 teams, data retention policies enforced programmatically. Passed SOC2 audit with zero engineering findings for 3 consecutive years.",
    "evidenceTypes": [
      "doc",
      "checklist",
      "metric"
    ],
    "defaultWeight": 0.04,
    "requiredFrequency": "continuous",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs ensure team compliance with established frameworks; Directors build compliance architecture and own audit readiness across the org.",
    "why": "Regulatory violations carry existential risk — fines, lawsuits, lost customer trust. Reactive compliance is always more expensive and disruptive than proactive frameworks.",
    "how": "Map applicable regulations (GDPR, SOX, HIPAA, PCI) to engineering practices using a compliance requirements matrix. Automate compliance checks in CI/CD where possible (PII detection, data retention enforcement, audit trail). Review matrix quarterly and after any regulatory change. Validate with metric 3.5 (SLO Compliance Rate).",
    "expectedResult": "Audits pass cleanly; compliance is embedded in engineering workflow, not bolted on; teams ship without regulatory surprises.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-161",
        "observableId": "C8-O5",
        "capabilityId": "C8",
        "signalText": "Directors own regulatory risk architecture. 'Built [GDPR/HIPAA/SOX] compliance framework adopted by [X] teams, automated [Y]% of compliance checks, passed [Z] consecutive audits with zero engineering findings.'",
        "signalType": "metric",
        "sourceSubTopic": "Regulatory Compliance Framework"
      },
      {
        "id": "SIG-162",
        "observableId": "C8-O5",
        "capabilityId": "C8",
        "signalText": "Compliance as engineering culture: 'Embedded regulatory requirements into CI/CD pipeline. PII detection automated, data retention enforced programmatically, audit trail complete for all data access.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Audit-Ready Engineering Practices"
      }
    ]
  },
  {
    "id": "C8-O6",
    "capabilityId": "C8",
    "shortText": "Evaluates and manages vendor risk with structured assessment, dependency tracking, and contingency planning",
    "slug": "evaluates-and-manages-vendor-risk-with-structured-assessment-dependency-tracking-and-contingency-planning",
    "fullExample": "Maintained vendor risk registry for 12 critical dependencies. Identified single-vendor lock-in for payment processing; negotiated multi-provider contract and built abstraction layer. When primary vendor had 8-hour outage, failover activated automatically with zero customer impact.",
    "evidenceTypes": [
      "doc",
      "metric",
      "decision_doc"
    ],
    "defaultWeight": 0.04,
    "requiredFrequency": "quarterly",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs evaluate vendors for team-level tools and manage integration risks; Directors own org-level vendor strategy, contract negotiation leverage, and dependency risk portfolio.",
    "why": "Critical vendor outages, pricing changes, or acquisitions can cripple your product. Single-vendor dependencies are hidden risks that only surface during crises.",
    "how": "Catalog all vendor dependencies with criticality rating. Assess: what happens if this vendor is unavailable for 24 hours? For each critical vendor: build abstraction layer, identify alternative, negotiate SLAs with teeth. Review vendor health quarterly. Validate with metric 9.2 (Dependency Currency).",
    "expectedResult": "No single vendor can take down your product; vendor costs are predictable and negotiated from strength; team can switch vendors without rewriting core systems.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-163",
        "observableId": "C8-O6",
        "capabilityId": "C8",
        "signalText": "Operational resilience through vendor management. 'Maintained vendor risk registry for [X] critical dependencies, built abstraction layers for [Y] single-vendor risks, survived [vendor] outage with zero customer impact via automated failover.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Vendor Risk Assessment"
      },
      {
        "id": "SIG-164",
        "observableId": "C8-O6",
        "capabilityId": "C8",
        "signalText": "'Identified [X] single-vendor lock-in risks, negotiated multi-provider contracts saving $[Y]/year, built vendor abstraction reducing switch cost from [Z] months to [W] weeks.'",
        "signalType": "metric",
        "sourceSubTopic": "Vendor Dependency Management"
      }
    ]
  },
  {
    "id": "C9-O7",
    "capabilityId": "C9",
    "shortText": "Governs feature flag lifecycle with creation standards, ownership tracking, and mandatory cleanup",
    "slug": "governs-feature-flag-lifecycle-with-creation-standards-ownership-tracking-and-mandatory-cleanup",
    "fullExample": "Established feature flag governance: every flag has owner, expiry date, and cleanup ticket created at birth. Automated stale flag detection alerts at 30 days. Reduced active flags from 340 to 45. Eliminated 2 production incidents caused by forgotten flag interactions.",
    "evidenceTypes": [
      "doc",
      "metric",
      "dashboard"
    ],
    "defaultWeight": 0.039,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own team-level flag hygiene and experimentation frameworks; Directors set governance standards and track compliance across teams.",
    "why": "Feature flags without lifecycle management become permanent technical debt. Forgotten flags create exponential complexity — 10 flags means 1024 possible code paths that nobody tests. Stale flags slow down every engineer who touches that code.",
    "how": "Every flag gets: owner, purpose, expiry date, cleanup ticket at creation time. Automated alerts when flags exceed expiry. Dashboard showing active flags per team with age distribution. Quarterly cleanup sprints. Flag interaction testing for long-lived flags. Validate with metric 1.4 (Change Failure Rate).",
    "expectedResult": "Within 1 quarter: active flag count stays manageable (target: <20 active flags per service); no production incidents from forgotten flags; experimentation velocity maintained without accumulating flag debt.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-165",
        "observableId": "C9-O7",
        "capabilityId": "C9",
        "signalText": "Process maturity signal. 'Established feature flag governance, reduced active flags from [X] to [Y], eliminated [Z] incidents caused by stale flag interactions. Every flag has owner and expiry.'",
        "signalType": "metric",
        "sourceSubTopic": "Feature Flag Lifecycle Governance"
      },
      {
        "id": "SIG-166",
        "observableId": "C9-O7",
        "capabilityId": "C9",
        "signalText": "Experimentation infrastructure maturity: 'Built experimentation framework with automated flag cleanup, ownership tracking, and interaction testing. Teams run [X] experiments/quarter with zero flag-related incidents.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Experimentation Framework"
      }
    ]
  },
  {
    "id": "C9-O8",
    "capabilityId": "C9",
    "shortText": "Enforces statistical rigor in A/B testing with proper design, power analysis, and result interpretation",
    "slug": "enforces-statistical-rigor-in-ab-testing-with-proper-design-power-analysis-and-result-interpretation",
    "fullExample": "Established A/B testing standards: mandatory power analysis before launch, minimum sample size gates, sequential testing for early stopping. Caught 3 experiments that would have shipped with underpowered results. Team stopped making decisions on p=0.08 results.",
    "evidenceTypes": [
      "doc",
      "metric",
      "decision_doc"
    ],
    "defaultWeight": 0.039,
    "requiredFrequency": "continuous",
    "emRelevance": "Medium",
    "directorRelevance": "Medium",
    "levelNotes": "EMs ensure team follows testing standards and interprets results correctly; Directors set org-level experimentation governance and partner with data science.",
    "why": "Bad A/B testing is worse than no testing — it gives false confidence. Underpowered tests, peeking at results, and cherry-picked metrics lead to shipping features that don't actually work.",
    "how": "Require power analysis before experiment launch using a standardized calculator (e.g., Evan Miller's tool or internal equivalent) — document minimum sample size, expected effect size, and required duration in the experiment brief. Set minimum run duration based on traffic volume (typically 1-2 weeks for high-traffic, 4+ weeks for low-traffic) with automated safeguards that prevent early termination. Ban peeking at results mid-experiment — use sequential testing frameworks (e.g., CUPED or always-valid p-values) if early stopping is genuinely needed. Pre-register the primary metric and 2-3 secondary metrics in the experiment doc before launch; reject post-hoc metric mining. Require practical significance threshold (e.g., >1% conversion lift) in addition to statistical significance (p<0.05). Review results in a structured experiment review meeting (30 minutes) with data science partner using a standardized results template. Track experiment rigor metrics quarterly: % of experiments with proper power analysis (target >95%), % terminated early without sequential testing justification (target <5%). Validate with metric 1.4 (Change Failure Rate) and 4.4 (Escaped Defect Rate).",
    "expectedResult": "Product decisions backed by valid experimental evidence; no shipped features based on noise; team develops experimentation intuition.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-167",
        "observableId": "C9-O8",
        "capabilityId": "C9",
        "signalText": "Data-driven culture signal. 'Established A/B testing governance with mandatory power analysis. Caught [X] underpowered experiments before shipping. Pre-registered metrics for all experiments.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "A/B Testing Standards"
      },
      {
        "id": "SIG-168",
        "observableId": "C9-O8",
        "capabilityId": "C9",
        "signalText": "'Implemented sequential testing framework, reducing average experiment duration [X]% while maintaining statistical validity. [Y] experiments completed per quarter with proper sample sizes.'",
        "signalType": "metric",
        "sourceSubTopic": "Experiment Design Rigor"
      }
    ]
  },
  {
    "id": "C11-O7",
    "capabilityId": "C11",
    "shortText": "Optimizes diversity hiring pipeline with structured sourcing, bias reduction, and inclusive practices",
    "slug": "optimizes-diversity-hiring-pipeline-with-structured-sourcing-bias-reduction-and-inclusive-practices",
    "fullExample": "Audited hiring funnel: identified 40% drop-off of underrepresented candidates at phone screen. Revised job descriptions removing exclusionary language, added structured scoring rubrics, partnered with 3 diversity-focused sourcing organizations. Underrepresented candidate pass-through rate improved 25% within 2 quarters.",
    "evidenceTypes": [
      "metric",
      "doc",
      "hiring_guide"
    ],
    "defaultWeight": 0.037,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs own team-level inclusive hiring practices and funnel analysis; Directors build org-level diversity sourcing partnerships and systemic bias reduction programs.",
    "why": "Homogeneous teams produce homogeneous solutions. Without intentional pipeline optimization, structural biases in sourcing and screening perpetuate underrepresentation regardless of stated values. At Google, structured behavioral interviews with rubrics were adopted specifically because research showed they reduce demographic bias while improving prediction accuracy.",
    "how": "Audit hiring funnel by demographic at each stage. Remove exclusionary language from job descriptions. Require diverse candidate slates. Partner with diversity-focused organizations for sourcing. Use structured interviews with rubrics. Track and report pipeline diversity metrics quarterly. Validate with metric 7.7 (Diversity Metrics).",
    "expectedResult": "Diverse candidate pipeline with underrepresented candidate pass-through rate improving 20%+ within 2 quarters; reduced bias in screening and evaluation; team composition reflects intentional effort; hiring quality maintained or improved.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-169",
        "observableId": "C11-O7",
        "capabilityId": "C11",
        "signalText": "Intentional pipeline building. 'Audited hiring funnel, identified [X]% drop-off at [stage], implemented [structured rubrics / diverse slates / sourcing partnerships], improved underrepresented candidate pass-through [Y]%.'",
        "signalType": "metric",
        "sourceSubTopic": "Diversity Pipeline Optimization"
      },
      {
        "id": "SIG-170",
        "observableId": "C11-O7",
        "capabilityId": "C11",
        "signalText": "Systemic bias reduction: 'Revised job descriptions removing [X] exclusionary patterns, partnered with [Y] diversity-focused orgs, achieved diverse candidate slate for [Z]% of open roles.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Inclusive Hiring Practices"
      }
    ]
  },
  {
    "id": "C11-O8",
    "capabilityId": "C11",
    "shortText": "Runs interviewer calibration program ensuring consistent signal quality and reduced evaluation bias",
    "slug": "runs-interviewer-calibration-program-ensuring-consistent-signal-quality-and-reduced-evaluation-bias",
    "fullExample": "Built interviewer calibration program: monthly sessions reviewing anonymized write-ups, scoring alignment exercises, feedback quality rubric. Interviewer inter-rater reliability improved from 0.4 to 0.75. Reduced 'weak hire' reversals in debrief by 60%.",
    "evidenceTypes": [
      "doc",
      "interview_scorecard",
      "metric"
    ],
    "defaultWeight": 0.037,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs own interviewer training and calibration for their team's interviewers; Directors standardize calibration programs across org and track signal quality metrics.",
    "why": "Uncalibrated interviewers produce noisy, biased signal. Two interviewers evaluating the same candidate can reach opposite conclusions based on personal standards rather than objective criteria.",
    "how": "Run monthly 45-minute calibration sessions: review 2-3 anonymized write-ups as a group, score the same mock interview independently then compare and discuss divergence. Track inter-rater reliability using Cohen's kappa or simple agreement percentage (target >0.7). Provide written feedback on write-up quality, flagging vague impressions vs. specific evidence. Require a shadow cycle (2 shadows, 1 reverse-shadow) for all new interviewers before solo loops. Remove consistently miscalibrated interviewers (agreement rate below 50% for 2+ quarters) from active loops and retrain. Maintain an interviewer quality dashboard reviewed quarterly. Validate with metric 7.3 (Time to Hire) and 7.4 (Offer Acceptance Rate).",
    "expectedResult": "Consistent hiring bar across interviewers; write-ups provide actionable signal; debrief decisions are evidence-based, not personality-driven.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-171",
        "observableId": "C11-O8",
        "capabilityId": "C11",
        "signalText": "Hiring signal quality. 'Built interviewer calibration program with monthly sessions, improved inter-rater reliability from [X] to [Y], reduced weak-hire reversals in debrief [Z]%.'",
        "signalType": "metric",
        "sourceSubTopic": "Interviewer Calibration Program"
      },
      {
        "id": "SIG-172",
        "observableId": "C11-O8",
        "capabilityId": "C11",
        "signalText": "Interview quality culture: 'All interviewers complete shadow cycle before solo interviews. Monthly calibration reviews anonymized write-ups. Feedback rubric ensures evidence-based evaluations, not gut feel.'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Interview Feedback Quality"
      }
    ]
  },
  {
    "id": "C12-O6",
    "capabilityId": "C12",
    "shortText": "Maintains cultural continuity through leadership transitions and team growth",
    "slug": "maintains-cultural-continuity-through-leadership-transitions-and-team-growth",
    "fullExample": "When onboarding a new tech lead, pairs them with culture carriers and ensures they absorb existing norms before introducing changes. During rapid growth, scales culture through structured mentoring rather than hoping new hires absorb it organically.",
    "evidenceTypes": [
      "1_1_notes",
      "retrospective_output"
    ],
    "defaultWeight": 0.065,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs maintain team-level culture during growth; Directors ensure culture survives cross-team leadership transitions",
    "why": "Culture degrades fastest during rapid growth and leadership changes — without intentional continuity, toxic norms can emerge in weeks. At Netflix, the Freedom and Responsibility culture doc serves as the immune system during growth — it gives every new hire and leader a shared operating model that persists through transitions.",
    "how": "Pair new leaders and senior hires with designated culture carriers for their first 60 days (weekly 30-minute syncs on 'how we work here'). Document team norms explicitly in the onboarding checklist and discuss them in the new hire's first week. Run quarterly culture retros using a structured framework — modeled after Spotify's Health Checks where teams self-assess cultural health on specific dimensions (psychological safety, decision speed, technical quality, fun). Track culture survey scores through growth periods, flagging any dimension that drops >10% as an action item. Make values observable in daily decisions: reference principles in design reviews, retros, and hiring debriefs, not just posters. Validate with metric 7.8 (eNPS / Engagement Score) and 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "New team members describe team culture consistently within 60 days; culture surveys stable through growth periods; no 'culture shock' complaints in skip-levels.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-199",
        "observableId": "C12-O6",
        "capabilityId": "C12",
        "signalText": "Key signal for cultural stewardship: 'Onboarded 5 new engineers in Q3 while maintaining team health scores — structured culture buddy program and explicit norms documentation'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Cultural Continuity"
      }
    ]
  },
  {
    "id": "C12-O7",
    "capabilityId": "C12",
    "shortText": "Creates explicit feedback channels ensuring every voice is heard, not just the loudest",
    "slug": "creates-explicit-feedback-channels-ensuring-every-voice-is-heard",
    "fullExample": "Implements anonymous team health surveys, rotating meeting facilitators, and written-first discussion formats that ensure introverts and underrepresented voices have equal opportunity to contribute.",
    "evidenceTypes": [
      "team_survey",
      "retrospective_output"
    ],
    "defaultWeight": 0.065,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs design and maintain feedback channels; Directors audit equitable participation across teams",
    "why": "Without deliberate inclusive structures, decisions reflect the preferences of the most senior or vocal team members, creating blind spots and marginalizing quieter contributors. At Stripe, written-first culture means decisions begin as documents, not meetings — this structurally levels the playing field between extroverts and introverts.",
    "how": "Rotate meeting facilitation weekly so no single voice dominates. Use written-first formats for all significant decisions: circulate proposals as docs 24 hours before meetings — modeled after Stripe's writing culture where proposals are circulated before any meeting. Track meeting participation patterns quarterly (who speaks, who doesn't) using a simple tally during team meetings. Create an anonymous feedback channel (Google Form or Donut bot) for concerns that people may not raise publicly. Solicit input from quieter members directly in 1:1s ('I noticed you didn't weigh in on X — what's your take?'). Run quarterly anonymous team health surveys with specific questions on feeling heard (target >4.0/5.0). Validate with metric 5.1 (Developer Satisfaction Score) and 7.8 (eNPS / Engagement Score).",
    "expectedResult": "Meeting participation is distributed; engagement surveys show all demographics feel heard; decisions reflect diverse perspectives; fewer surprise escalations from unheard concerns.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-200",
        "observableId": "C12-O7",
        "capabilityId": "C12",
        "signalText": "Observable inclusive practice: 'Implemented written-first RFC process and rotating facilitation — meeting participation equity improved from 40% to 80% of team contributing in design discussions'",
        "signalType": "metric",
        "sourceSubTopic": "Inclusive Feedback Channels"
      }
    ]
  },
  {
    "id": "C12-O8",
    "capabilityId": "C12",
    "shortText": "Proactively addresses toxic behaviors before they become normalized team patterns",
    "slug": "proactively-addresses-toxic-behaviors-before-they-become-normalized",
    "fullExample": "When observing dismissive code review comments from a senior engineer, addresses it in 1:1 within 24 hours, resets expectations with the team publicly, and monitors for recurrence rather than waiting for an HR complaint.",
    "evidenceTypes": [
      "1_1_notes",
      "peer_feedback"
    ],
    "defaultWeight": 0.065,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs address individual behaviors; Directors identify systemic toxicity patterns across teams",
    "why": "Toxic behaviors compound quickly — one unchecked bad actor can destroy psychological safety for the entire team within weeks, causing attrition among the best performers first",
    "how": "Name the specific behavior (not the person's character) in a private 1:1 within 24-48 hours of observation, using the SBI framework (Situation, Behavior, Impact): 'In yesterday's code review, when you wrote [specific comment], it made [person] feel dismissed, which reduces the team's willingness to submit code for review.' Set explicit expectations for change with a measurable check-in timeline (2-4 weeks). Reset expectations publicly when appropriate (e.g., in retro: 'We agreed that code review comments should be constructive — here's what that looks like'). Track behavior change over 4-6 weeks using 1:1 notes and peer feedback. Escalate to HR if the pattern continues after two direct conversations. Document all interventions. Validate with metric 7.1 (Regrettable Attrition Rate) and 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Team perceives EM as culture guardian; toxic patterns caught early; psychological safety surveys trend positive; no 'missing stair' dynamics tolerated.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-201",
        "observableId": "C12-O8",
        "capabilityId": "C12",
        "signalText": "Culture protection signal: 'Addressed pattern of dismissive code review comments within 48 hours — reset team norms in retro, followed up in 1:1s, tracked improvement over 4 weeks'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Toxic Behavior Intervention"
      }
    ]
  },
  {
    "id": "C10-O6",
    "capabilityId": "C10",
    "shortText": "Presents resource requests as tiered options with quantified trade-offs for each level",
    "slug": "presents-resource-requests-as-tiered-options-with-quantified-trade-offs",
    "fullExample": "Instead of requesting 5 headcount, presents three options: (A) 2 heads — deliver core features only, delay platform work 6 months; (B) 4 heads — core + platform, accept tech debt in monitoring; (C) 6 heads — full roadmap with reliability investment. Each option includes timeline, risk, and ROI.",
    "evidenceTypes": [
      "decision_doc",
      "planning_artifact"
    ],
    "defaultWeight": 0.065,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs present team-level options; Directors present org-level portfolio allocation options",
    "why": "Binary resource requests force leadership into yes/no decisions without understanding trade-offs; tiered options enable informed prioritization across the org. At Google, resource request tiers are standard practice — teams present bronze/silver/gold options so leadership can optimize across the portfolio.",
    "how": "For every resource request, prepare a 1-page tiered proposal with minimum/target/stretch options (e.g., bronze/silver/gold). Each tier includes: named deliverables, timeline, risks if under-resourced, and ROI projection with payback period — modeled after Google's resource request tiers where each level has a named scope commitment and quantified trade-off. Use a standard template shared with finance partners. Present tiers during quarterly planning or ad-hoc resource reviews, and track which tier was approved and whether actual outcomes matched projections. Validate with metric 8.4 (Engineering Investment Mix) and 8.3 (Cost of Delay).",
    "expectedResult": "Resource discussions are productive, not adversarial; leadership trusts your judgment; approval cycle shortened by 40%+ because trade-offs are pre-computed and transparent.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-202",
        "observableId": "C10-O6",
        "capabilityId": "C10",
        "signalText": "Resource planning maturity: 'Presented 3-tier headcount proposal with quantified ROI per tier — leadership approved Option B, allocating 4 engineers to cover core + platform workstream'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Tiered Resource Planning"
      }
    ]
  },
  {
    "id": "C10-O7",
    "capabilityId": "C10",
    "shortText": "Tracks and communicates cost-per-outcome to demonstrate engineering ROI",
    "slug": "tracks-cost-per-outcome-to-demonstrate-engineering-roi",
    "fullExample": "Maintains a running dashboard showing cost-per-feature, cost-per-customer-served, and engineering investment by workstream category (growth, reliability, platform, debt). Uses this to justify investments and identify efficiency opportunities.",
    "evidenceTypes": [
      "dashboard",
      "planning_artifact"
    ],
    "defaultWeight": 0.065,
    "requiredFrequency": "continuous",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs track team-level cost efficiency; Directors build org-level cost models connecting engineering spend to business outcomes",
    "why": "Without cost-per-outcome tracking, engineering is perceived as a cost center; with it, engineering becomes a strategic investment with measurable returns",
    "how": "Categorize engineering time by investment type (feature, reliability, platform, debt) using Jira labels or a time-tracking tool, reviewed monthly. Track cloud costs per service using AWS Cost Explorer, GCP Billing, or Datadog Cost Management with per-service tagging. Calculate cost per key business outcome (cost-per-feature, cost-per-customer-served, cost-per-transaction) and present as a monthly dashboard to leadership. Set targets for cost-per-outcome improvement of 5-10% QoQ and flag workstreams where cost-per-outcome is rising. Validate with metric 10.1 (Cloud Cost per Transaction/User) and 8.4 (Engineering Investment Mix).",
    "expectedResult": "Engineering seen as strategic investment; budget conversations grounded in data; efficiency improvements visible; trust with finance partners increases.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-203",
        "observableId": "C10-O7",
        "capabilityId": "C10",
        "signalText": "Business partnership signal: 'Built cost-per-customer-served dashboard — identified 30% efficiency gain by consolidating two redundant services, saving $200K/year in infra costs'",
        "signalType": "metric",
        "sourceSubTopic": "Engineering ROI Tracking"
      }
    ]
  },
  {
    "id": "C10-O8",
    "capabilityId": "C10",
    "shortText": "Proactively reallocates capacity to highest-impact work without waiting for top-down direction",
    "slug": "proactively-reallocates-capacity-to-highest-impact-work",
    "fullExample": "When a planned project's expected impact decreases due to market changes, proposes reallocating two engineers to a higher-ROI reliability initiative without waiting for quarterly planning. Communicates the shift with data and gets alignment within a week.",
    "evidenceTypes": [
      "decision_doc",
      "1_1_notes"
    ],
    "defaultWeight": 0.065,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs reallocate within their team; Directors reallocate across teams in their org",
    "why": "Static resource allocation wastes engineering capacity on diminishing-return work; proactive reallocation maximizes impact per engineer-month",
    "how": "Review resource allocation against expected impact monthly using a lightweight impact-vs-effort scorecard per workstream. Identify diminishing-return workstreams by comparing planned vs. actual outcomes at mid-quarter checkpoints. Propose reallocation with a data-backed 1-pager showing: current workstream trajectory, projected impact of reallocation, and stakeholder impact. Communicate explicitly what stops and why within 48 hours of the decision. Get stakeholder alignment through direct conversation, not email, and confirm in writing. Track whether reallocations delivered the projected improvement in subsequent quarters. Validate with metric 8.4 (Engineering Investment Mix) and 8.1 (OKR Achievement Rate).",
    "expectedResult": "Team always working on highest-impact projects; stakeholders trust your judgment; no 'why are we still building this?' complaints; impact per engineer-month improves.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-204",
        "observableId": "C10-O8",
        "capabilityId": "C10",
        "signalText": "Proactive allocation signal: 'Identified diminishing returns on Feature X mid-quarter — reallocated 2 engineers to reliability workstream, improving SLA from 99.5% to 99.9% with zero feature delay'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Dynamic Resource Reallocation"
      }
    ]
  },
  {
    "id": "C7-O8",
    "capabilityId": "C7",
    "shortText": "Structures difficult conversations with preparation, empathy, and clear follow-through",
    "slug": "structures-difficult-conversations-with-preparation-empathy-and-follow-through",
    "fullExample": "Before delivering a promotion rejection, documents specific gaps with examples, prepares a development plan, schedules a private 1:1 with sufficient time, delivers the message with empathy, and follows up with written summary and action items within 24 hours.",
    "evidenceTypes": [
      "1_1_notes",
      "peer_feedback"
    ],
    "defaultWeight": 0.062,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs deliver individual difficult conversations; Directors also handle org-level difficult messaging",
    "why": "Difficult conversations avoided or botched destroy trust, create surprises, and let problems compound; handled well, they build credibility and accelerate resolution",
    "how": "Prepare with specific examples and data using a conversation prep template (situation, behavior observed, impact, desired change) at least 24 hours before the conversation. Schedule adequate private time (minimum 45 minutes in a 1:1 setting, never in an open space or group meeting). Deliver with empathy but clarity — state the issue directly within the first 2 minutes, then provide supporting examples. Allow space for response (plan for 50% listening time). Follow up in writing within 24 hours with a shared summary of what was discussed, agreed actions, and timeline. Track outcomes in a private log — review at next 1:1 (within 1 week). Target: all difficult conversations initiated within 48 hours of the triggering event; zero surprise escalations from delayed feedback. Validate with metric 5.1 (Developer Satisfaction Score) and 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "Difficult messages delivered within 48 hours of trigger; no surprise escalations; team trusts EM to be honest; problems resolved early rather than festering.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-205",
        "observableId": "C7-O8",
        "capabilityId": "C7",
        "signalText": "Communication maturity signal: 'Delivered promotion rejection with specific gap analysis and 6-month development plan — engineer expressed appreciation for clarity and achieved promotion next cycle'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Difficult Conversations"
      }
    ]
  },
  {
    "id": "C7-O9",
    "capabilityId": "C7",
    "shortText": "Scales communication systems as org grows — replaces hallway decisions with documented processes",
    "slug": "scales-communication-systems-as-org-grows",
    "fullExample": "As team grows from 8 to 25 engineers, replaces informal decision-making with an RFC process, introduces weekly written status updates replacing daily syncs, and creates tiered communication channels (urgent/informational/discussion).",
    "evidenceTypes": [
      "decision_doc",
      "process_artifact"
    ],
    "defaultWeight": 0.046,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs adapt team communication as team grows; Directors design org-level communication architecture",
    "why": "Communication patterns that work for 5 people break at 15 and collapse at 50 — scaling communication is a leadership design problem, not something that happens naturally",
    "how": "Audit communication effectiveness quarterly using a team communication survey (5 questions, 1-5 scale) targeting >4.0 average. At 10+ people, introduce written-first decision processes: RFC templates in Confluence or Notion, async comment periods (48-72 hours), and decision logs searchable by topic. Create tiered Slack channels: #team-urgent (P0 only, immediate response), #team-announcements (read-only, informational), #team-discussion (async, threaded). Establish communication cadences that scale: weekly written status replacing daily syncs, bi-weekly all-hands for >15 people, monthly retrospectives. Document all decisions for async consumption in a searchable decision log with required fields (context, options, decision, rationale, date). Track meeting hours per engineer weekly — target <25% of work time in meetings. Target: new team members can find historical context without asking within first 2 weeks. Validate with metric 5.5 (Cognitive Load Index) and 5.2 (Developer Toil Hours).",
    "expectedResult": "Team operates effectively despite growth; decisions aren't bottlenecked by meetings; new members can find context without asking; communication overhead grows sub-linearly with team size.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-206",
        "observableId": "C7-O9",
        "capabilityId": "C7",
        "signalText": "Scaling communication: 'Introduced RFC process as team grew past 15 — reduced decision revisitation by 60% and cut average meeting hours per engineer from 12/week to 7/week'",
        "signalType": "metric",
        "sourceSubTopic": "Communication Scaling"
      }
    ]
  },
  {
    "id": "C7-O10",
    "capabilityId": "C7",
    "shortText": "Distributes decision authority using clear frameworks — knows which decisions need consensus vs. a driver",
    "slug": "distributes-decision-authority-using-clear-frameworks",
    "fullExample": "Maintains a decision authority matrix: team norms require consensus, technical design decisions use DACI with tech lead as Driver, tooling choices are delegated to individual engineers, and architecture bets require EM + TL alignment. Team knows which decisions they can make autonomously.",
    "evidenceTypes": [
      "decision_doc",
      "team_charter"
    ],
    "defaultWeight": 0.062,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs distribute decisions within their team; Directors distribute decision authority across their EM reports",
    "why": "Without clear decision authority, teams either wait for permission (slow) or make conflicting decisions (chaotic); clarity enables both speed and alignment",
    "how": "Map common decision types to authority levels using a Decision Authority Matrix (spreadsheet or Notion table) with columns: decision type, authority level (individual/team/consensus/escalation), driver role, approver, and examples. Categories: individual autonomy (tooling choices, implementation approach), team consensus (team norms, sprint commitments), DACI-driven (technical design, API contracts), and escalation-required (architecture bets, cross-team dependencies). Communicate the framework in team onboarding docs and reference it in sprint planning. Review the matrix quarterly — adjust based on outcomes (if delegated decisions consistently need reversal, tighten; if escalated decisions are rubber-stamped, loosen). Target: >80% of decisions made without EM involvement; track decision reversal rate (target <10%). Resist the urge to centralize after a bad outcome — instead, improve the decision framework. Validate with metric 5.1 (Developer Satisfaction Score) and 5.4 (Code Review Turnaround Time).",
    "expectedResult": "Team makes most decisions without EM involvement; decisions are faster; fewer reversals due to misalignment; team feels empowered.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-207",
        "observableId": "C7-O10",
        "capabilityId": "C7",
        "signalText": "Decision distribution signal: 'Implemented decision authority matrix — 80% of technical decisions now made without my involvement, freeing me to focus on strategic alignment and people development'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Decision Authority Distribution"
      }
    ]
  },
  {
    "id": "C14-O7",
    "capabilityId": "C14",
    "shortText": "Keeps weekly running performance notes per report and surfaces feedback themes monthly so reviews contain zero surprises",
    "slug": "maintains-continuous-performance-signal-through-ongoing-documentation",
    "fullExample": "Keeps running performance notes for each report updated weekly. When review cycle arrives, has 6 months of specific examples ready — not scrambling to remember what happened. Shares feedback themes in 1:1s monthly so reviews contain zero surprises.",
    "evidenceTypes": [
      "1_1_notes",
      "review_doc"
    ],
    "defaultWeight": 0.065,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs maintain individual performance records; Directors ensure their EMs practice continuous documentation",
    "why": "Reconstructing performance from memory creates recency bias, misses critical examples, and leads to inaccurate reviews; continuous documentation produces fair, evidence-rich assessments. The promotion narrative process demands sustained evidence over multiple cycles — managers who don't keep running logs find themselves writing fiction at review time.",
    "how": "Keep running doc per report with dated examples updated weekly; flag themes monthly in 1:1s; use brag docs or shared achievement logs; review trends quarterly before formal review cycles — this is the discipline that makes calibration committees and promotion narratives actually work. Validate with metric 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "Performance reviews are evidence-rich with specific examples; zero surprises for reports; calibration cases are strong; review cycle takes days not weeks to prepare.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-208",
        "observableId": "C14-O7",
        "capabilityId": "C14",
        "signalText": "Review quality signal: 'Maintained running performance logs for all 8 reports — review cycle preparation completed in 3 days with 15+ specific examples per person, zero surprises reported in engagement survey'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Continuous Performance Documentation"
      }
    ]
  },
  {
    "id": "C14-O8",
    "capabilityId": "C14",
    "shortText": "Differentiates high-performer ratings with evidence portfolios and documents retention risk for leadership",
    "slug": "differentiates-high-performer-ratings-with-evidence-and-retention-risk",
    "fullExample": "Prepares calibration cases for top performers with quantified impact evidence (not just 'they're great'), pre-wires calibration committee to prevent default 'meets expectations' ratings, and maintains a written retention risk register shared with skip-level quarterly.",
    "evidenceTypes": [
      "1_1_notes",
      "career_plan"
    ],
    "defaultWeight": 0.065,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs develop individual high performers; Directors create org-level programs for top talent retention and development",
    "why": "High performers whose calibration ratings don't reflect their actual contribution feel invisible and leave. The evaluation system must differentiate — if your top performer gets the same rating as a solid-but-unremarkable contributor, the system has failed. Promotion narratives must include quantified impact evidence and scope comparisons to next-level peers; managers who don't document this continuously can't write a credible case.",
    "how": "Compile a differentiated evidence portfolio for each high performer: quantified impact (revenue, cost savings, velocity improvement), scope comparison to next-level peers, and 360 feedback from cross-functional partners. Pre-wire calibration committee members 2 weeks before sessions by sharing evidence summaries. Maintain a retention risk register with flight risk indicators (compensation gaps, growth stagnation, external interest signals) shared with skip-level quarterly. Time promotion nominations against organizational capacity — don't nominate when the committee is overloaded. Validate with metric 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "High performer ratings accurately reflect contribution; zero instances of top talent rated 'meets expectations' without justification; retention risks documented before departures; calibration committee recognizes evidence quality.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-209",
        "observableId": "C14-O8",
        "capabilityId": "C14",
        "signalText": "High performer management signal: 'Created differentiated development plans for top 3 performers — all received stretch assignments aligned to promotion criteria, one promoted this cycle, zero regrettable departures'",
        "signalType": "calibration_language",
        "sourceSubTopic": "High Performer Development"
      }
    ]
  },
  {
    "id": "C2-O5",
    "capabilityId": "C2",
    "shortText": "Maintains an explicit 'not doing' list that communicates strategic trade-offs to stakeholders",
    "slug": "maintains-explicit-not-doing-list-communicating-strategic-trade-offs",
    "fullExample": "Quarterly planning output includes not just the roadmap but a named list of requests deliberately declined with rationale. Stakeholders understand what was deprioritized and why, reducing re-litigation.",
    "evidenceTypes": [
      "planning_artifact",
      "decision_doc"
    ],
    "defaultWeight": 0.083,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs maintain team-level trade-off visibility; Directors communicate portfolio-level strategic trade-offs",
    "why": "Without an explicit 'not doing' list, stakeholders assume their requests are still in queue; the team gets re-asked and re-litigates decisions; focus is eroded by invisible expectations. Sharing the full prioritization context means stakeholders understand the trade-offs without needing to be told no repeatedly.",
    "how": "For every quarterly planning cycle, publish a 'not-doing' list alongside the roadmap in a shared planning artifact (e.g., Notion page or wiki). For each declined item, include: the request, who asked, estimated cost to do it, what would be dropped, and the named decision-maker. Share with all stakeholders within one week of planning completion. Revisit when conditions change — review the not-doing list at mid-quarter to check if any trade-offs should be reversed. Target a measurable reduction in re-litigation requests quarter over quarter. Validate with metric 8.4 (Engineering Investment Mix) and 8.3 (Cost of Delay).",
    "expectedResult": "Stakeholders don't re-ask for deprioritized items; team focus is protected; trade-offs are transparent; re-prioritization decisions are explicit, not silent.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-210",
        "observableId": "C2-O5",
        "capabilityId": "C2",
        "signalText": "Strategic clarity signal: 'Published explicit not-doing list alongside Q3 roadmap — stakeholder re-litigation requests dropped 70%, freeing team focus for committed deliverables'",
        "signalType": "metric",
        "sourceSubTopic": "Strategic Trade-off Transparency"
      }
    ]
  },
  {
    "id": "C2-O6",
    "capabilityId": "C2",
    "shortText": "Connects engineering investment to business outcomes with measurable success criteria",
    "slug": "connects-engineering-investment-to-business-outcomes",
    "fullExample": "Every major engineering initiative has defined success metrics tied to business outcomes before work begins. Team can articulate why they're building what they're building in business terms, not just technical terms.",
    "evidenceTypes": [
      "planning_artifact",
      "decision_doc"
    ],
    "defaultWeight": 0.083,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs connect team projects to business outcomes; Directors connect org portfolio to company strategy",
    "why": "Engineering without business outcome connection becomes a feature factory; connecting work to outcomes ensures the team is building the right things, not just building things right. Starting every initiative with the customer outcome, not the technical approach, forces clarity — if you can't articulate the business value, you don't understand it well enough to build it.",
    "how": "Require business success metrics for every initiative before work begins; write the expected outcome in customer or business terms first. Ask 'how will we know this worked?' and 'what would make us kill this mid-quarter?'; review outcomes quarterly; kill initiatives that aren't delivering expected business impact. Review metrics against targets weekly to catch drift early. Validate with metric 8.1 (OKR Achievement Rate).",
    "expectedResult": "Every engineer can explain why their work matters in business terms; initiatives without clear outcomes are questioned; engineering seen as strategic partner, not cost center.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-211",
        "observableId": "C2-O6",
        "capabilityId": "C2",
        "signalText": "Business alignment signal: 'Required business success metrics for all Q2 initiatives — killed 2 projects mid-quarter when metrics showed no business impact, reallocated team to higher-ROI work'",
        "signalType": "metric",
        "sourceSubTopic": "Outcome-Connected Planning"
      }
    ]
  },
  {
    "id": "C8-O7",
    "capabilityId": "C8",
    "shortText": "Runs quarterly game days with heroes excluded, testing runbook accuracy and failover procedures against a rotating failure catalog",
    "slug": "builds-system-resilience-through-proactive-chaos-engineering",
    "fullExample": "Schedules quarterly game days that simulate production failures with heroes explicitly excluded. Tests runbook accuracy, on-call readiness, and failover procedures before real incidents occur.",
    "evidenceTypes": [
      "decision_doc",
      "incident_report"
    ],
    "defaultWeight": 0.054,
    "requiredFrequency": "quarterly",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs participate in and organize team game days; Directors establish org-wide resilience testing programs",
    "why": "Resilience untested is resilience assumed; game days reveal gaps in runbooks, tooling, and team readiness before real incidents force discovery under pressure",
    "how": "Schedule quarterly game days (half-day, 4 hours) on the team calendar with at least 2 weeks notice. Simulate realistic failure modes: database failover, upstream service degradation, network partition, certificate expiry — rotate scenarios each quarter from a failure catalog maintained in Confluence. Exclude known heroes explicitly (they observe but cannot intervene) to test team depth and bus factor. Use a structured game day template: scenario description, expected behavior, actual behavior, gaps discovered, runbook accuracy assessment. Review and update runbooks within 1 week of each game day. Track metrics: gaps discovered per game day (target trend downward), time-to-mitigate for simulated incidents (target <30 minutes), runbook accuracy rate (target >90% of steps correct). Require post-game-day retrospective (30 minutes) with documented action items assigned and tracked to completion. Validate with metric 1.3 (Mean Time to Restore) and 3.6 (Incident Rate by Severity).",
    "expectedResult": "Team confident in incident handling; runbooks tested and current; no single point of failure in incident response; MTTR improves from proactive preparation.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-212",
        "observableId": "C8-O7",
        "capabilityId": "C8",
        "signalText": "Proactive resilience signal: 'Ran quarterly game days with heroes excluded — discovered 3 runbook gaps and 2 tooling issues before they caused real incidents. On-call readiness improved from 60% to 90%'",
        "signalType": "metric",
        "sourceSubTopic": "Proactive Resilience Testing"
      }
    ]
  },
  {
    "id": "C8-O8",
    "capabilityId": "C8",
    "shortText": "Manages error budgets to balance reliability investment against feature velocity",
    "slug": "manages-error-budgets-to-balance-reliability-against-velocity",
    "fullExample": "Defines SLOs with product team and maintains error budgets. When budget is consumed, automatically shifts team focus to reliability work. When budget is healthy, uses it to justify faster feature shipping with acceptable risk.",
    "evidenceTypes": [
      "dashboard",
      "decision_doc"
    ],
    "defaultWeight": 0.054,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs implement error budgets for their services; Directors set SLO policy and error budget governance across the org",
    "why": "Without error budgets, reliability vs. velocity is a perpetual argument; error budgets make the trade-off objective and data-driven",
    "how": "Define SLOs collaboratively with product and business stakeholders using a structured SLO workshop (2-hour session) covering user journeys, acceptable failure rates, and business impact thresholds — document in an SLO registry (Confluence or Notion). Calculate error budget from SLO (e.g., 99.9% SLO = 43.2 minutes/month error budget) and display on a real-time dashboard (Datadog, Grafana, or equivalent). Monitor budget consumption weekly in team standup and daily via automated Slack alerts at 50%, 75%, and 90% consumption thresholds. Trigger automatic reliability focus when budget consumption exceeds 80% in a rolling window: freeze non-critical feature work, prioritize reliability backlog, and notify product stakeholders within 24 hours. Use healthy budget (<50% consumed) to justify faster feature shipping with documented risk acceptance. Review SLOs quarterly with stakeholders — adjust based on changing business needs. Validate with metric 3.3 (Error Budget Remaining + Burn Rate) and 3.5 (SLO Compliance Rate).",
    "expectedResult": "Reliability investment is data-driven, not argument-driven; feature teams and SREs aligned on acceptable risk; no surprise outages from over-shipping.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-213",
        "observableId": "C8-O8",
        "capabilityId": "C8",
        "signalText": "Error budget maturity: 'Implemented SLO-based error budgets for 3 critical services — triggered automatic reliability sprint when checkout service consumed 80% of monthly budget, preventing customer-facing degradation'",
        "signalType": "metric",
        "sourceSubTopic": "Error Budget Management"
      }
    ]
  },
  {
    "id": "C13-O5",
    "capabilityId": "C13",
    "shortText": "Embeds security champions within the team who own security awareness and review practices",
    "slug": "embeds-security-champions-within-the-team",
    "fullExample": "Rotates a security champion role quarterly among senior engineers, who attend security guild meetings, review threat models for new features, and run quarterly security training for the team.",
    "evidenceTypes": [
      "process_artifact",
      "peer_feedback"
    ],
    "defaultWeight": 0.065,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs establish team-level security champion rotation; Directors ensure coverage across all teams in their org",
    "why": "Security can't be solely the security team's job — distributed champions create security muscle memory in every team and catch issues at development time, not at audit time. At Google, security engineering is embedded into product teams rather than siloed — the BeyondCorp model works because security ownership is distributed, not centralized.",
    "how": "Rotate security champion role quarterly; have champion attend security guild; integrate threat modeling into design reviews; run quarterly security awareness sessions; track vulnerability SLA compliance. Validate with metric 9.1 (Vulnerability Remediation Time).",
    "expectedResult": "Vulnerabilities caught earlier in development cycle; security review turnaround faster; team has baseline security literacy; audit findings decrease over time.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-229",
        "observableId": "C13-O5",
        "capabilityId": "C13",
        "signalText": "Security champion program: 'Established rotating security champion role — vulnerability detection shifted 60% leftward (caught in code review vs. production scanning)'",
        "signalType": "metric",
        "sourceSubTopic": "Security Champions"
      }
    ]
  },
  {
    "id": "C13-O6",
    "capabilityId": "C13",
    "shortText": "Integrates automated security scanning into CI/CD pipeline with severity-based deployment gates",
    "slug": "integrates-automated-security-scanning-into-cicd-pipeline",
    "fullExample": "Pipeline includes SAST, dependency scanning, and container image scanning. Critical/High vulnerabilities block deployment automatically. Medium vulnerabilities create tracked tickets with 30-day SLA.",
    "evidenceTypes": [
      "dashboard",
      "process_artifact"
    ],
    "defaultWeight": 0.065,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs ensure their team's pipelines have security gates; Directors set org-wide security scanning standards",
    "why": "Manual security reviews don't scale and create deployment bottlenecks; automated scanning catches known vulnerability patterns consistently without human toil. At Netflix, automated compliance scanning runs continuously — security posture is a dashboard metric, not a quarterly audit exercise.",
    "how": "Wire SAST/DAST tools into CI/CD as blocking gates for critical/high findings; configure severity-based deployment gates; track false positive rates and tune quarterly; maintain live vulnerability SLA dashboard — modeled after Netflix's automated compliance where security posture is continuously measured, not periodically audited. Validate with metric 9.1 (Vulnerability Remediation Time).",
    "expectedResult": "Zero critical/high vulnerabilities reach production; vulnerability remediation time predictable; security doesn't bottleneck deployments; compliance evidence generated automatically.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-230",
        "observableId": "C13-O6",
        "capabilityId": "C13",
        "signalText": "Automated security gates: 'Implemented automated security scanning in CI/CD — blocked 15 critical vulnerabilities from reaching production in Q3, zero manual security review bottlenecks'",
        "signalType": "metric",
        "sourceSubTopic": "Automated Security Scanning"
      }
    ]
  },
  {
    "id": "C13-O7",
    "capabilityId": "C13",
    "shortText": "Conducts threat modeling for features touching sensitive data, authentication, or external interfaces",
    "slug": "conducts-threat-modeling-for-sensitive-features",
    "fullExample": "Before launching a new payment integration, leads a STRIDE-based threat model session with the team, identifies 4 attack vectors, implements mitigations, and documents accepted risks with business justification.",
    "evidenceTypes": [
      "design_doc",
      "decision_doc"
    ],
    "defaultWeight": 0.065,
    "requiredFrequency": "episodic",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs ensure threat modeling happens for their team's sensitive features; Directors establish org-wide threat modeling requirements",
    "why": "Threat modeling catches design-level security flaws that no amount of code scanning can find — it's cheaper to fix security issues in design than in production. At Amazon, threat model review is a mandatory gate before any service touching customer data can launch — it's not a suggestion, it's a deployment prerequisite.",
    "how": "Require threat models for all features touching auth, payments, PII, or external APIs — modeled after Amazon's mandatory security review gates where threat model review is a deployment prerequisite. Use the STRIDE framework (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege) in a structured 60-minute workshop with the feature team before design is finalized. Document threats in a standard template: attack vector, severity (CVSS score), proposed mitigation, residual risk, and business justification for any accepted risks. Review the threat model with the security team before implementation begins and archive as part of the design doc for audit trail. Run threat model reviews per-feature for sensitive changes, and audit coverage quarterly (target: 100% of features touching sensitive data have a completed threat model). Validate with metric 9.1 (Vulnerability Remediation Time) and 9.3 (Compliance Audit Readiness).",
    "expectedResult": "Design-level security flaws caught before code is written; security team engaged as partners, not gatekeepers; compliance evidence for audit readiness; no 'we didn't think about that' security incidents.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-231",
        "observableId": "C13-O7",
        "capabilityId": "C13",
        "signalText": "Proactive threat modeling: 'Required threat models for all features touching PII — identified and mitigated 8 design-level vulnerabilities before code was written, zero security incidents from new features'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Threat Modeling Practice"
      }
    ]
  },
  {
    "id": "C11-O9",
    "capabilityId": "C11",
    "shortText": "Designs structured interview processes with rubrics that evaluate demonstrated competencies over pedigree",
    "slug": "designs-structured-interviews-evaluating-competencies-over-pedigree",
    "fullExample": "Replaces 'culture fit' assessment with structured behavioral questions mapped to role competencies. Every interviewer uses the same rubric. Scoring happens independently before debrief to prevent anchoring.",
    "evidenceTypes": [
      "process_artifact",
      "decision_doc"
    ],
    "defaultWeight": 0.067,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs design and maintain team interview processes; Directors ensure consistency and fairness across teams",
    "why": "Unstructured interviews are poor predictors of job performance and amplify bias — structured rubrics improve both prediction accuracy and fairness. At Amazon, structured behavioral interviews (the STAR method mapped to Leadership Principles) are mandatory, not optional — every question has a rubric, every answer is scored independently.",
    "how": "Map role requirements to interview questions with behavioral anchors; create scoring rubrics with concrete examples at each level — modeled after Amazon's structured behavioral interview format; train interviewers on rubric use through shadow/reverse-shadow cycle; collect scores independently before debrief to prevent anchoring; audit for demographic patterns quarterly. Validate with metric 7.3 (Time to Hire) and 7.4 (Offer Acceptance Rate).",
    "expectedResult": "Hiring decisions more predictable; bias reduced measurably; interviewer calibration improved; quality-of-hire metrics trend positive; candidate experience is consistent.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-232",
        "observableId": "C11-O9",
        "capabilityId": "C11",
        "signalText": "Structured hiring process: 'Replaced unstructured interviews with competency-mapped rubrics — interviewer agreement rate improved from 55% to 85%, regrettable hires dropped 40%'",
        "signalType": "metric",
        "sourceSubTopic": "Structured Interview Design"
      }
    ]
  },
  {
    "id": "C11-O10",
    "capabilityId": "C11",
    "shortText": "Wins talent through differentiated value proposition (impact, growth velocity, autonomy) backed by employer brand presence",
    "slug": "builds-competitive-hiring-strategy-beyond-compensation",
    "fullExample": "When competing against FAANG offers, articulates differentiated value proposition: faster career growth, higher impact per engineer, meaningful product ownership. Pairs with competitive (if not top-of-market) compensation and strong employer branding.",
    "evidenceTypes": [
      "planning_artifact",
      "decision_doc"
    ],
    "defaultWeight": 0.05,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs craft team-level employer value proposition; Directors design org-level talent acquisition strategy",
    "why": "Most companies can't match top-of-market compensation — sustainable hiring requires differentiated value propositions that attract candidates whose priorities align with what you can offer. Even Google competes on mission and technical challenge, not just comp — the best candidates choose problems, not packages.",
    "how": "Articulate what your team uniquely offers (impact, growth, autonomy, mission); build employer brand through tech blog, open source, conference talks; target candidates whose priorities match your strengths; train hiring managers on value proposition delivery; close candidates by connecting role specifics to their stated career goals. Validate with metric 7.4 (Offer Acceptance Rate).",
    "expectedResult": "Offer acceptance rate above 70% despite below-top-of-market compensation; hires aligned to team values with 90%+ one-year retention; reduced recruiter dependency through 40%+ referral pipeline; stronger employer brand.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-233",
        "observableId": "C11-O10",
        "capabilityId": "C11",
        "signalText": "Hiring strategy maturity: 'Built differentiated hiring pitch emphasizing career velocity and impact — offer acceptance rate 75% despite 15% below FAANG compensation, 90% retention at 1 year'",
        "signalType": "metric",
        "sourceSubTopic": "Competitive Hiring Strategy"
      }
    ]
  },
  {
    "id": "C11-O11",
    "capabilityId": "C11",
    "shortText": "Measures and optimizes hiring funnel conversion rates with data-driven pipeline management",
    "slug": "measures-and-optimizes-hiring-funnel-conversion-rates",
    "fullExample": "Tracks conversion rates at each hiring stage (sourcing → screen → onsite → offer → accept). Identifies bottlenecks, A/B tests improvements, and shares pipeline health with leadership weekly.",
    "evidenceTypes": [
      "dashboard",
      "planning_artifact"
    ],
    "defaultWeight": 0.05,
    "requiredFrequency": "continuous",
    "emRelevance": "Medium",
    "directorRelevance": "High",
    "levelNotes": "EMs track team hiring pipeline health; Directors optimize org-wide funnel and identify systemic bottlenecks",
    "why": "Without funnel data, hiring feels like black magic — conversion rate tracking reveals specific bottlenecks and enables targeted improvements instead of throwing more candidates at a broken process",
    "how": "Instrument hiring funnel stages (sourcing, recruiter screen, technical screen, onsite, offer, accept) in your ATS (Greenhouse, Lever, or equivalent). Track conversion rates weekly in a shared dashboard, flagging any stage with >40% drop-off as a bottleneck. A/B test improvements at the bottleneck stage (e.g., redesign technical screen format, shorten onsite from 5 to 4 rounds). Track time-to-fill alongside quality-of-hire (90-day manager satisfaction) to ensure speed improvements do not sacrifice quality. Audit conversion rates by demographic group quarterly to identify and address disparities at specific stages. Share pipeline health summary with leadership bi-weekly. Validate with metric 7.3 (Time to Hire), 7.4 (Offer Acceptance Rate), and 7.7 (Diversity Metrics).",
    "expectedResult": "Hiring timeline predictable; bottlenecks identified and addressed; leadership has visibility into pipeline health; time-to-fill decreasing; no demographic drop-off patterns at specific stages.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-234",
        "observableId": "C11-O11",
        "capabilityId": "C11",
        "signalText": "Pipeline optimization: 'Instrumented hiring funnel — identified 60% drop-off at technical screen, redesigned assessment format, improved conversion to 45%, reduced time-to-fill from 65 to 40 days'",
        "signalType": "metric",
        "sourceSubTopic": "Hiring Pipeline Analytics"
      }
    ]
  },
  {
    "id": "C4-O12",
    "capabilityId": "C4",
    "shortText": "Diagnoses and resolves engineering velocity bottlenecks using systemic analysis, not just process changes",
    "slug": "diagnoses-velocity-bottlenecks-using-systemic-analysis",
    "fullExample": "When team velocity drops, maps the entire value stream from idea to production, identifies specific bottleneck (code review turnaround was 3 days), implements targeted fix (review SLA + pairing rotation), and measures improvement.",
    "evidenceTypes": [
      "dashboard",
      "retrospective_output"
    ],
    "defaultWeight": 0.069,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs diagnose team-level velocity issues; Directors identify org-level systemic bottlenecks across teams",
    "why": "Velocity problems rarely have a single cause — systemic analysis prevents whack-a-mole process changes that shift bottlenecks instead of resolving them. Teams who measure their flow end-to-end and target the actual constraint improve 2-3x faster than teams who guess.",
    "how": "Map the value stream end-to-end (idea to production) using a tool like Jellyfish, LinearB, or a manual Miro board. Measure time at each stage using DORA-aligned metrics; identify largest wait times and distinguish between flow time and work time. Target the constraint, not the symptom — run a focused improvement cycle per quarter and measure post-intervention impact within 2 sprints. Target: cycle time trending down quarter-over-quarter with stable or improving quality. Validate with metric 1.2 (Lead Time for Changes), 2.1 (Flow Time), 2.3 (Cycle Time), and 1.1 (Deployment Frequency).",
    "expectedResult": "Velocity improvements sustained (not temporary); team understands their own flow; bottlenecks resolved rather than shifted; cycle time trending down with stable quality.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-217",
        "observableId": "C4-O12",
        "capabilityId": "C4",
        "signalText": "Velocity diagnosis: 'Value stream mapped end-to-end — identified code review turnaround (3 day avg) as primary bottleneck, implemented review SLA and pairing rotation, reduced to 6 hours, overall cycle time improved 40%'",
        "signalType": "metric",
        "sourceSubTopic": "Velocity Bottleneck Analysis"
      }
    ]
  },
  {
    "id": "C4-O13",
    "capabilityId": "C4",
    "shortText": "Protects maker time with structural calendar policies ensuring engineers have 4+ hours of uninterrupted focus daily",
    "slug": "protects-maker-time-with-structural-calendar-policies",
    "fullExample": "Establishes meeting-free mornings, consolidates ceremonies into 2 days per week, defaults to async for status updates, and tracks focus time as a team health metric. Engineers report 5+ hours of focus time daily.",
    "evidenceTypes": [
      "team_survey",
      "process_artifact"
    ],
    "defaultWeight": 0.069,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs protect team-level focus time; Directors set org-wide meeting norms and audit calendar health across teams",
    "why": "Context-switching is the biggest hidden tax on engineering productivity — a 30-minute meeting in the middle of a focus block costs 2+ hours of deep work productivity. Protecting maker schedules is a leadership responsibility, not an individual one.",
    "how": "Audit team calendars monthly for focus block availability; establish meeting-free mornings (or equivalent 4-hour protected windows). Consolidate ceremonies into 2 days per week; default to async for status updates using Slack/Loom/recorded standups. Track focus time as a team health metric in quarterly pulse surveys — target >=4 hours of uninterrupted focus time daily, with >80% of engineers reporting satisfaction with focus time. Validate with metric 5.1 (Developer Satisfaction Score) and 2.4 (Throughput).",
    "expectedResult": "Engineers report 4+ hours of daily uninterrupted focus time; deep work improves; complex problems get solved; team satisfaction with working conditions increases.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-218",
        "observableId": "C4-O13",
        "capabilityId": "C4",
        "signalText": "Focus time protection: 'Established meeting-free mornings and async-first status updates — engineer focus time increased from 2.5 to 5 hours/day, complex feature delivery improved 35%'",
        "signalType": "metric",
        "sourceSubTopic": "Focus Time Protection"
      }
    ]
  },
  {
    "id": "C9-O9",
    "capabilityId": "C9",
    "shortText": "Uses metric pairings to prevent single-metric optimization — always balances speed with quality",
    "slug": "uses-metric-pairings-to-prevent-single-metric-optimization",
    "fullExample": "Pairs deployment frequency with change failure rate; pairs cycle time with customer-reported bugs; pairs velocity with tech debt ratio. Never reports a speed metric without its quality counterpart.",
    "evidenceTypes": [
      "dashboard",
      "planning_artifact"
    ],
    "defaultWeight": 0.07,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs implement paired metrics for their team; Directors establish org-wide metric pairing standards",
    "why": "Single metrics are gameable and misleading — metric pairs create natural guardrails that prevent optimizing one dimension at the cost of another. Every speed metric needs a quality counterweight.",
    "how": "For every speed metric, pair with a quality metric; for every throughput metric, pair with a sustainability metric. Maintain a documented metric pairing registry (Confluence or Notion table) listing: speed metric, quality counterpart, acceptable divergence threshold, and investigation trigger. Standard pairings: deployment frequency + change failure rate, cycle time + escaped defect rate, velocity + tech debt ratio, feature throughput + developer satisfaction. Present paired metrics together on the same dashboard panel (Datadog, Grafana, or LinearB) — never show a speed metric in isolation. Set automated alerts when pairs diverge beyond threshold (e.g., deployment frequency up >20% while change failure rate also rises >10%). Investigate divergence within 1 sprint and present findings in the next retrospective. Review metric pairings quarterly to ensure they remain relevant and add new pairings as team adopts new metrics. Validate with metric 1.2 (Lead Time for Changes) and 1.4 (Change Failure Rate).",
    "expectedResult": "Within 1 quarter: no gaming of individual metrics; balanced delivery; quality doesn't degrade when speed improves; leadership gets accurate picture of team health.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-237",
        "observableId": "C9-O9",
        "capabilityId": "C9",
        "signalText": "Balanced measurement: 'Implemented paired metrics dashboard — deployment frequency improved 40% while change failure rate remained stable, preventing the velocity-quality tradeoff'",
        "signalType": "metric",
        "sourceSubTopic": "Metric Pairings"
      }
    ]
  },
  {
    "id": "C9-O10",
    "capabilityId": "C9",
    "shortText": "Adapts metric sophistication to organizational maturity — starts simple and evolves based on decisions driven",
    "slug": "adapts-metric-sophistication-to-organizational-maturity",
    "fullExample": "For a new team, starts with 3 core metrics (cycle time, deployment frequency, change failure rate). Adds developer satisfaction surveys after 1 quarter. Introduces SPACE framework dimensions only after basic metrics drive consistent decisions.",
    "evidenceTypes": [
      "dashboard",
      "planning_artifact"
    ],
    "defaultWeight": 0.053,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs calibrate team metric sophistication; Directors establish org-wide metrics maturity roadmap",
    "why": "Teams that jump to complex metrics frameworks before building basic measurement discipline end up with death-by-metrics. Starting simple and evolving based on decisions driven ensures metrics remain useful, not burdensome.",
    "how": "Start with 3-5 core metrics appropriate to team maturity: Level 1 (new team) — cycle time, deployment frequency, change failure rate; Level 2 (established) — add developer satisfaction survey (quarterly) and code review turnaround; Level 3 (mature) — introduce SPACE framework dimensions and custom business impact metrics. Conduct a monthly 15-minute 'metric usefulness' check in retrospective: ask 'what decision did this metric inform this month?' for each active metric. Retire any metric that has not informed a decision in 2 consecutive months — archive it, do not just ignore it. Add new metrics only when the team demonstrates consistent decision-making from existing ones (minimum 1 decision per metric per quarter). Document the team's metrics maturity level and evolution roadmap in the team handbook, reviewed quarterly. Target: every active metric informs at least 1 decision per quarter; total active metrics <10 to prevent dashboard fatigue. Validate with metric 8.1 (OKR Achievement Rate) and 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Within 2 quarters: every active metric has informed at least one decision within 90 days; team engaged with measurement rather than resenting it; metric overhead proportional to value delivered.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-238",
        "observableId": "C9-O10",
        "capabilityId": "C9",
        "signalText": "Metrics maturity: 'Started with 3 core metrics, retired 2 that weren't driving decisions, added developer satisfaction survey after Q2 — every active metric informed at least one decision per quarter'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Metrics Maturity Evolution"
      }
    ]
  },
  {
    "id": "C3-O11",
    "capabilityId": "C3",
    "shortText": "Quantifies tech debt in business terms with ROI-justified remediation backlog",
    "slug": "quantifies-tech-debt-in-business-terms-with-roi-justification",
    "fullExample": "Maintains a tech debt registry where each item has estimated cost-of-delay, risk rating, and remediation cost. Presents quarterly tech debt report to leadership in business terms: 'This tech debt item costs us 2 engineer-days per sprint in workarounds and has caused 3 incidents.'",
    "evidenceTypes": [
      "planning_artifact",
      "dashboard"
    ],
    "defaultWeight": 0.057,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs maintain team-level tech debt visibility; Directors advocate for tech debt investment at org level with business justification",
    "why": "Tech debt is invisible until it causes incidents or blocks features — making it visible in business terms enables informed investment decisions rather than emergency firefighting.",
    "how": "Maintain tech debt registry with business impact estimates (cost-of-delay, incident risk, velocity drag). Categorize by risk (security, reliability, velocity). Present in business terms using the same format as feature ROI analysis. Include tech debt in quarterly planning alongside features. Track remediation ROI. Validate with metric 8.6 (Tech Debt Ratio).",
    "expectedResult": "Tech debt investment is deliberate, not reactive; leadership understands tech debt impact in business terms; tech debt ratio stable or improving; fewer 'surprise' incidents from accumulated debt.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-221",
        "observableId": "C3-O11",
        "capabilityId": "C3",
        "signalText": "Tech debt management: 'Maintained quantified tech debt registry — business case for top 3 items showed $500K/year in engineering productivity savings, secured 20% capacity allocation for remediation'",
        "signalType": "metric",
        "sourceSubTopic": "Tech Debt Quantification"
      }
    ]
  },
  {
    "id": "C5-O16",
    "capabilityId": "C5",
    "shortText": "Runs weekly triad syncs with PM and Design using shared OKRs and joint quarterly retrospectives",
    "slug": "maintains-triad-alignment-through-regular-syncs-and-shared-metrics",
    "fullExample": "Weekly 30-minute triad sync with PM and Design lead. Shared OKRs (not separate Eng/PM OKRs). Joint quarterly retrospective. When disagreements arise, resolves within the triad before escalating.",
    "evidenceTypes": [
      "1_1_notes",
      "planning_artifact"
    ],
    "defaultWeight": 0.057,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "Medium",
    "levelNotes": "EMs maintain team-level triad health; Directors ensure triad model adopted across their org",
    "why": "Most cross-functional friction stems from misaligned incentives and infrequent communication — regular triad alignment prevents the adversarial dynamic that derails delivery. The Eng/PM/Design triad should operate as a single leadership unit with shared OKRs, not three functions reporting to a meeting.",
    "how": "Establish a weekly 30-minute triad sync (Eng, PM, Design) with rotating facilitation. Create shared success metrics and OKRs — not separate Eng/PM/Design OKRs. Resolve disagreements within the triad first; escalate only after 1 week of unresolved discussion with a written options doc. Run joint quarterly retrospectives to assess triad health. Track cross-functional escalation frequency — target <2 escalations per quarter. Alignment is a continuous practice, not a quarterly planning event. Validate with metric 8.1 (OKR Achievement Rate) and 2.5 (Sprint Commitment Accuracy).",
    "expectedResult": "No 'us vs them' dynamic between Eng, PM, and Design; shared ownership of outcomes; disagreements resolved quickly; team sees unified leadership.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-240",
        "observableId": "C5-O16",
        "capabilityId": "C5",
        "signalText": "Triad health signal: 'Established shared OKRs and weekly triad syncs with PM and Design — cross-functional escalations dropped from 5/quarter to 0, joint retro satisfaction 4.5/5'",
        "signalType": "metric",
        "sourceSubTopic": "Product Triad Alignment"
      }
    ]
  },
  {
    "id": "C6-O13",
    "capabilityId": "C6",
    "shortText": "Creates clear career pathways for engineers at all levels, including IC and management tracks",
    "slug": "creates-clear-career-pathways-including-ic-and-management-tracks",
    "fullExample": "Maintains documented career ladder with specific expectations per level. Helps engineers choose between IC and management tracks based on strengths and interests. Creates explicit pathways for Staff+ ICs alongside management progression.",
    "evidenceTypes": [
      "career_plan",
      "decision_doc"
    ],
    "defaultWeight": 0.057,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs guide individual career conversations; Directors ensure career framework exists and is applied consistently",
    "why": "Without clear pathways, engineers guess at what's needed for promotion, high performers stall, and the management track becomes the only perceived path to growth — losing your best ICs to management roles they don't want",
    "how": "Maintain a career ladder document with behavioral expectations per level for both IC track (mid → senior → staff → principal) and management track (TL → EM → senior EM → Director). Each level specifies: scope expectations, technical depth, leadership behaviors, and example artifacts that demonstrate readiness. Review and update framework annually with input from Staff+ engineers and HR. In monthly career 1:1s (see C6-O10), help engineers self-assess against the ladder and choose the right track based on strengths and interests. For promotion candidates, map current evidence against next-level expectations using a gap analysis template. Create stretch assignments (see C6-O5) aligned to the top 1-2 gaps per engineer per half. Validate with metric 5.1 (Developer Satisfaction Score).",
    "expectedResult": "Engineers understand what's needed for advancement; both IC and management tracks valued equally; promotions feel predictable and fair; fewer 'accidental managers' (ICs who manage because it was the only path to growth).",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-241",
        "observableId": "C6-O13",
        "capabilityId": "C6",
        "signalText": "Career pathway clarity: 'Documented explicit IC and management career tracks — 100% of reports have active career plans, Staff IC track adopted by 3 engineers who would have otherwise become reluctant managers'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Career Pathway Design"
      }
    ]
  },
  {
    "id": "C1-O16",
    "capabilityId": "C1",
    "shortText": "Translates business strategy into team-level goals with explicit success criteria and cross-team dependency mapping",
    "slug": "translates-business-strategy-into-team-level-goals-with-cross-team-dependencies",
    "fullExample": "Takes company-level strategic priorities and decomposes them into team OKRs with measurable success criteria. Maps cross-team dependencies explicitly so no team is blocked by an invisible handoff. Quarterly reviews tie team outcomes back to business impact.",
    "evidenceTypes": [
      "planning_artifact",
      "decision_doc",
      "okr_doc"
    ],
    "defaultWeight": 0.061,
    "requiredFrequency": "quarterly",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs translate org strategy to team goals; Directors ensure cross-team alignment and resolve conflicting priorities",
    "why": "Without explicit translation, teams execute locally-optimal work that doesn't compound into business outcomes; dependencies become surprise blockers mid-quarter. A cascading OKR structure provides the translation layer — company-level objectives decompose into team-level key results, making the connection from individual work to business strategy structurally explicit.",
    "how": "Start with business priorities each quarter, decompose to team-level outcomes with measurable success criteria using a cascading OKR structure (company objective to team key results). Map cross-team dependencies in a shared dependency board (e.g., Miro or spreadsheet) updated at the start of each quarter and reviewed bi-weekly. Review progress against business outcomes, not just task completion, in a monthly business review — write the press release first and work backward to team deliverables. Target >85% of team OKRs traceable to a company-level objective. Validate with metric 8.1 (OKR Achievement Rate), 8.2 (Revenue / Business Impact Attribution), and 2.6 (Blocked Work Rate).",
    "expectedResult": "Every team can articulate how their work connects to business strategy; cross-team dependencies are known and tracked; quarterly reviews show business outcome progress not just feature delivery; >85% of team OKRs trace to company-level objectives.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-255",
        "observableId": "C1-O16",
        "capabilityId": "C1",
        "signalText": "Strategic alignment: 'Translated company H2 strategy into 4 team OKRs with measurable business outcomes — 3 of 4 hit target, cross-team dependency map prevented 2 potential mid-quarter blockers'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Business Strategy to Team Goals"
      },
      {
        "id": "SIG-256",
        "observableId": "C1-O16",
        "capabilityId": "C1",
        "signalText": "Director-level: 'Established quarterly strategy decomposition process across 5 teams — 85% of team goals now have explicit business outcome metrics, up from 30% before'",
        "signalType": "metric",
        "sourceSubTopic": "Cross-Team Goal Alignment"
      }
    ]
  },
  {
    "id": "C7-O11",
    "capabilityId": "C7",
    "shortText": "Communicates trade-offs explicitly — names what's being sacrificed, for whom, and why it's the right call",
    "slug": "communicates-trade-offs-explicitly-names-what-is-sacrificed",
    "fullExample": "When presenting a technical investment decision, doesn't just advocate for the chosen path — explicitly names the alternative that was rejected, who it disadvantages, what risk it introduces, and why the trade-off is still correct. Stakeholders leave with a clear understanding of the cost, not just the benefit.",
    "evidenceTypes": [
      "decision_doc",
      "meeting_note",
      "planning_artifact"
    ],
    "defaultWeight": 0.062,
    "requiredFrequency": "continuous",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs communicate trade-offs within their team scope; Directors communicate portfolio-level trade-offs to leadership and across orgs",
    "why": "Hiding trade-offs creates false consensus — stakeholders feel blindsided when costs materialize; explicit trade-off communication builds trust and prevents re-litigation",
    "how": "For every significant decision (>1 engineer-week impact), include a mandatory 'Trade-offs & Costs' section in the decision doc using a standard template: rejected alternatives with rationale, quantified costs (engineer-weeks, latency impact, user experience degradation), who bears the cost (specific teams named), why this trade-off is acceptable given current priorities, and conditions under which you would revisit. Present trade-offs in planning reviews and stakeholder meetings with explicit visual framing (e.g., a 2x2 matrix of benefit vs. cost per option). Document in a searchable decision log for future reference. Track decision durability — target >85% stick rate for decisions with explicit trade-off framing (vs. typical ~55% without). Review trade-off accuracy retrospectively each quarter: were the predicted costs accurate? Validate with metric 8.1 (OKR Achievement Rate).",
    "expectedResult": "Stakeholders trust decisions because they understand what was traded away; fewer surprises when costs materialize; decisions stick because trade-offs were explicit upfront.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-257",
        "observableId": "C7-O11",
        "capabilityId": "C7",
        "signalText": "Trade-off clarity: 'Presented platform migration proposal with explicit cost section — Team B loses 2 weeks of roadmap, Team C takes on temporary operational burden — leadership approved with full awareness of costs'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Trade-off Communication"
      },
      {
        "id": "SIG-258",
        "observableId": "C7-O11",
        "capabilityId": "C7",
        "signalText": "Decision durability metric: 'Decisions presented with explicit trade-off framing had 90% stick rate vs 55% for decisions where costs were implicit — reduced re-litigation meetings by 40%'",
        "signalType": "metric",
        "sourceSubTopic": "Explicit Trade-off Framing"
      }
    ]
  },
  {
    "id": "C14-O9",
    "capabilityId": "C14",
    "shortText": "Executes managed exits with dignity — clear timeline, support, and no surprises for the departing employee",
    "slug": "executes-managed-exits-with-dignity-and-clear-process",
    "fullExample": "When a PIP doesn't result in improvement, transitions to a managed exit with transparency: communicates the decision directly, provides a clear timeline, offers references and job search support, handles team communication thoughtfully, and ensures clean knowledge transfer. The departing employee and remaining team both feel the process was fair.",
    "evidenceTypes": [
      "hr_doc",
      "comms_plan",
      "meeting_note"
    ],
    "defaultWeight": 0.065,
    "requiredFrequency": "episodic",
    "emRelevance": "High",
    "directorRelevance": "High",
    "levelNotes": "EMs execute individual managed exits; Directors ensure consistent, fair processes across their org and coach EMs through difficult exits",
    "why": "Botched exits destroy team trust — remaining employees watch how departures are handled to judge if they're safe; poorly managed exits also create legal risk and damage employer brand. The Keeper Test normalizes honest performance conversations so exits are rarely a surprise — when someone leaves, the team understands why because the culture of candor precedes it.",
    "how": "Decide with conviction after a fair PIP process has concluded. Deliver the exit decision directly and clearly in a private 1:1 (never via email or Slack), using prepared talking points reviewed with HR. Provide a clear timeline (typically 2-4 weeks for transition), covering last day, benefits continuation, and knowledge transfer expectations. Communicate to the team within 24 hours without violating the departing employee's privacy — focus on transition plan, not reasons. Create a structured knowledge transfer checklist: documentation updates, code ownership handoffs, access revocations, and ongoing project status. Offer references, job search advocacy, and outplacement support where appropriate. Debrief with HR within 1 week on process quality and lessons learned. For Directors: maintain an exit process playbook with templates for each stage, reviewed annually. Validate with metric 7.2 (Non-Regrettable Attrition Rate) and 7.1 (Regrettable Attrition Rate).",
    "expectedResult": "Departing employee feels treated fairly even if disappointed; remaining team trusts the process; zero legal escalations; knowledge transfer complete; team morale preserved.",
    "status": "NEW",
    "calibrationSignals": [
      {
        "id": "SIG-259",
        "observableId": "C14-O9",
        "capabilityId": "C14",
        "signalText": "Exit management signal: 'Managed 3 exits this year — all completed with full knowledge transfer, zero legal escalations, and post-departure survey showed remaining team trusted the process was fair'",
        "signalType": "calibration_language",
        "sourceSubTopic": "Managed Exit Execution"
      },
      {
        "id": "SIG-260",
        "observableId": "C14-O9",
        "capabilityId": "C14",
        "signalText": "Director-level: 'Established managed exit playbook across org — standardized timeline, communication templates, and knowledge transfer checklist; reduced average exit timeline from 6 weeks of ambiguity to 3 weeks of structured transition'",
        "signalType": "metric",
        "sourceSubTopic": "Exit Process Standardization"
      }
    ]
  }
]
